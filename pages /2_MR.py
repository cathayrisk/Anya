import streamlit as st
import os
import tempfile
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langgraph.constants import Send
from langgraph.graph import END, START, StateGraph
from langchain_core.documents import Document
from langchain.chains.combine_documents.reduce import (
    acollapse_docs,
    split_list_of_docs,
)
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore
from typing import List, TypedDict, Literal, Annotated
import operator
import asyncio
import pymupdf4llm
import faiss
import time
from uuid import uuid4

# åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_KEY"]

# åˆå§‹åŒ– LangChain çš„ ChatOpenAI æ¨¡å‹ï¼Œè¨­ç½®è¶…æ™‚
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.0, streaming=True)

# è¨­ç½®ç¶²é æ¨™é¡Œå’Œåœ–æ¨™
st.set_page_config(page_title="Generate_Summary", layout="wide", page_icon="ğŸ“")

# ä½¿ç”¨ expander æ§åˆ¶æ–‡ä»¶ä¸Šå‚³éƒ¨åˆ†
with st.expander("Upload your PDF files", expanded=True):
    uploaded_files = st.file_uploader("", type=["pdf"], accept_multiple_files=True, label_visibility="hidden")

# åˆ†å‰²æ–‡ä»¶
text_splitter = RecursiveCharacterTextSplitter(
    separators=[
        "\n\n",
        "\n",
        " ",
        ".",
        ",",
        "\u200b",  # é›¶å¯¬ç©ºæ ¼
        "\uff0c",  # å…¨è§’é€—è™Ÿ
        "\u3001",  # å¥è®€é»
        "\uff0e",  # å…¨è§’å¥è™Ÿ
        "\u3002",  # å¥è™Ÿ
        ""
    ],
    chunk_size=500,  # æ ¹æ“šéœ€è¦èª¿æ•´
    chunk_overlap=20  # æ ¹æ“šéœ€è¦èª¿æ•´
)

token_max = 100000

# å®šç¾©ç‹€æ…‹é¡å‹
class SummaryState(TypedDict):
    content: str

class OverallState(TypedDict):
    contents: List[str]
    summaries: Annotated[List[str], operator.add]  # ä½¿ç”¨ Annotated ä¾†å…è¨±å¤šå€‹å€¼
    collapsed_summaries: List[Document]
    final_summary: str

# ç”Ÿæˆæ‘˜è¦
async def generate_summary(state: SummaryState):
    try:
        #st.write(f"Generating summary for content: {state['content'][:100]}...")  # æ‰“å°å‰100å€‹å­—ç¬¦
        response = await map_chain.ainvoke(state["content"])
        if not response:  # æª¢æŸ¥è¿”å›çš„æ‘˜è¦æ˜¯å¦ç‚ºç©º
            st.write("Warning: Empty summary generated for content.")
        return {"summaries": [response]}
    except Exception as e:
        st.write(f"Error generating summary: {e}")
        return {"summaries": []}

# æ˜ å°„æ‘˜è¦
def map_summaries(state: OverallState):
    return [Send("generate_summary", {"content": content}) for content in state["contents"]]

# è¨ˆç®—æ–‡ä»¶é•·åº¦
def length_function(documents: List[Document]) -> int:
    return sum(llm.get_num_tokens(doc.page_content) for doc in documents)

# æ”¶é›†æ‘˜è¦
def collect_summaries(state: OverallState):
    return {"collapsed_summaries": [Document(summary) for summary in state["summaries"]]}

# ç”Ÿæˆæœ€çµ‚æ‘˜è¦
async def generate_final_summary(state: OverallState):
    if not state['summaries']:
        return {"final_summary": "ç•¶å‰æ²’æœ‰æä¾›å…·é«”çš„ä¸»é¡Œæ‘˜è¦ï¼Œå› æ­¤ç„¡æ³•é€²è¡Œæ•´åˆå’Œè©³ç´°ç¸½çµã€‚"}

    # æº–å‚™è¼¸å…¥çµ¦ reduce_chain
    collapsed_summaries = [Document(page_content=summary) for summary in state['summaries']]
    state['collapsed_summaries'] = collapsed_summaries

    #st.write(f"Generating final summary for {len(collapsed_summaries)} summaries.")

    try:
        response = await reduce_chain.ainvoke(collapsed_summaries)
        return {"final_summary": response}
    except Exception as e:
        st.write(f"Error generating final summary: {e}")
        return {"final_summary": "ç„¡æ³•ç”Ÿæˆæœ€çµ‚æ‘˜è¦ï¼Œè«‹æª¢æŸ¥è¼¸å…¥å…§å®¹ã€‚"}
    
# æ‘ºç–Šæ‘˜è¦
async def collapse_summaries(state: OverallState):
    doc_lists = split_list_of_docs(state["collapsed_summaries"], length_function, token_max)
    results = []
    for doc_list in doc_lists:
        results.append(await acollapse_docs(doc_list, reduce_chain.ainvoke))
    return {"collapsed_summaries": results}

# åˆ¤æ–·æ˜¯å¦éœ€è¦æ‘ºç–Š
def should_collapse(state: OverallState) -> Literal["collapse_summaries", "generate_final_summary"]:
    num_tokens = length_function(state["collapsed_summaries"])
    if num_tokens > token_max:
        return "collapse_summaries"
    else:
        return "generate_final_summary"

# å»ºç«‹ç‹€æ…‹åœ–
graph = StateGraph(OverallState)
graph.add_node("generate_summary", generate_summary)
graph.add_node("collect_summaries", collect_summaries)
graph.add_node("generate_final_summary", generate_final_summary)
graph.add_node("collapse_summaries", collapse_summaries)

# æ·»åŠ é‚Š
graph.add_conditional_edges(START, map_summaries, ["generate_summary"])
graph.add_edge("generate_summary", "collect_summaries")
graph.add_conditional_edges("collect_summaries", should_collapse)
graph.add_conditional_edges("collapse_summaries", should_collapse)
graph.add_edge("generate_final_summary", END)

# ç·¨è­¯æ‡‰ç”¨ç¨‹å¼
app = graph.compile()

# å®šç¾©æç¤ºæ¨¡æ¿
map_template = """
Please read the following content and identify the main themes and key insights especially in risk related. Focus on extracting the unique insights and specific details that are central to the document's purpose.
- Use bullet points to list key insights and specific details for each theme.
- Include specific data, examples, and references to support each theme, ensuring a rich level of detail.
- Explain the cause-and-effect relationships for each key point, highlighting how different factors interact.
- Pay attention to the tone and style to ensure consistency with the conversational nature of the recording.
- Identify and highlight any important action items or decisions, and explain the rationale behind them.
- Summarize the key points that the document most wants to convey.
Please respond in Traditional Chinese, not Simplified Chinese.
Content: {context}
"""

reduce_template = """
The following are summaries of key themes extracted from the document:
{docs}
Please consolidate these into broader categories, focusing on grouping related themes under major categories. For each major category, provide a detailed summary using bullet points to highlight key insights, specific details, and explain the cause-and-effect relationships.
- Ensure all important details and examples are included to support each category's summary, enhancing the richness of the content.
- Maintain consistency in tone and style, reflecting the conversational nature of the recording.
- Ensure identification and emphasis on any important action items or decisions, and provide explanations for these recommendations.
- Summarize the key points that the document most wants to convey, without assuming additional strategic implications unless explicitly mentioned.
Please respond in Traditional Chinese, not Simplified Chinese.
"""

map_prompt = ChatPromptTemplate([("human", map_template)])
reduce_prompt = ChatPromptTemplate([("human", reduce_template)])

map_chain = map_prompt | llm | StrOutputParser()
reduce_chain = reduce_prompt | llm | StrOutputParser()

# å®šç¾©é‹è¡Œæ‡‰ç”¨ç¨‹å¼çš„ç•°æ­¥å‡½æ•¸
async def run_app(split_docs):
    contents = [doc.page_content for doc in split_docs]
    #st.write(f"Processing {len(contents)} chunks.")
    tasks = [generate_summary({"content": content}) for content in contents]
    summaries = await asyncio.gather(*tasks)

    # æª¢æŸ¥ summaries æ˜¯å¦åŒ…å«æœ‰æ•ˆçš„æ‘˜è¦
    valid_summaries = [summary['summaries'][0] for summary in summaries if summary['summaries']]
    if not valid_summaries:
        st.write("No valid summaries generated.")
        return "ç•¶å‰æ²’æœ‰æä¾›å…·é«”çš„ä¸»é¡Œæ‘˜è¦ï¼Œå› æ­¤ç„¡æ³•é€²è¡Œæ•´åˆå’Œè©³ç´°ç¸½çµã€‚"

    #st.write(f"Valid summaries: {valid_summaries}")
    overall_state = OverallState(contents=contents, summaries=valid_summaries, collapsed_summaries=[], final_summary="")
    final_summary = await generate_final_summary(overall_state)
    return final_summary['final_summary']

# è™•ç†ä¸Šå‚³çš„æ–‡ä»¶
if uploaded_files:
    # æŒ‰éˆ•ä¾†è§¸ç™¼æ‘˜è¦ç”Ÿæˆ
    if st.button("ç”Ÿæˆæ‘˜è¦"):
        try:
            all_transcriptions = []
            for uploaded_file in uploaded_files:
                # Save each uploaded file to a temporary location
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_pdf_file:
                    temp_pdf_file.write(uploaded_file.read())
                    temp_pdf_path = temp_pdf_file.name

                # æå– PDF å…§å®¹ç‚º Markdown
                md_text_list = pymupdf4llm.to_markdown(
                    temp_pdf_path,
                    write_images=False,  # ä¸æå–åœ–åƒ
                    page_chunks=True  # æŒ‰é é¢åˆ†å¡Šè¼¸å‡º
                )

                # å°‡æå–çš„æ–‡æœ¬åˆä½µ
                full_transcription = "\n".join(page.get('text', '') for page in md_text_list if isinstance(page, dict))
                all_transcriptions.append(full_transcription)

            # åˆä½µæ‰€æœ‰æ–‡ä»¶çš„è½‰éŒ„æ–‡æœ¬
            combined_transcription = "\n".join(all_transcriptions)

            # é¡¯ç¤º formatted_transcription
            tab1, tab2 = st.tabs(["é‡é»æ‘˜è¦", "åŸå§‹å…§å®¹"])
            with tab1:
                # ç•°æ­¥è¨ˆç®— summarize_transcription ä¸¦åœ¨ Tab2 ä¸­é¡¯ç¤º spinner
                async def calculate_summary():
                    # åˆ†å‰²è½‰éŒ„æ–‡æœ¬ä¸¦åŒ…è£æˆ Document å°è±¡
                    start_time = time.time()  # é–‹å§‹æ™‚é–“
                    split_docs = [Document(page_content=content) for content in text_splitter.split_text(combined_transcription)]
                    summarize_transcription = await run_app(split_docs)
                    end_time = time.time()  # çµæŸæ™‚é–“
                    st.write(f"calculate_summary took {end_time - start_time:.2f} seconds")
                    st.session_state['summarize_transcription'] = summarize_transcription
                    return summarize_transcription

                with st.spinner('Generating summary...'):
                    summarize_transcription = asyncio.run(calculate_summary())
                    st.markdown(summarize_transcription)

            with tab2:
                with st.container():
                    st.markdown(combined_transcription)

        except Exception as e:
            st.error(f"An error occurred: {e}")

else:
    st.info("Please upload PDF files to start the analysis.")
