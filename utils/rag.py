import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
from langgraph.graph import StateGraph
from typing import List, Annotated, Literal, Sequence, TypedDict
from langgraph.graph import END, StateGraph, START
import asyncio
import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.schema import Document
from tavily import TavilyClient
import nltk
for pkg in ['punkt', 'averaged_perceptron_tagger']:
    try:
        path = f'tokenizers/{pkg}' if pkg == 'punkt' else f'taggers/{pkg}'
        nltk.data.find(path)
    except LookupError:
        nltk.download(pkg)


class RouteQuery(BaseModel):
    """å°‡ä½¿ç”¨è€…çš„å•é¡Œå°å‘æœ€ç›¸é—œçš„è³‡æ–™ä¾†æºã€‚"""

    datasource: Literal["vectorstore", "web_search"] = Field(
        ...,
        description="ä¾æ“šä½¿ç”¨è€…çš„å•é¡Œï¼Œå°å‘è‡³vectorstoreæˆ–web_searchã€‚",
    )


class GradeDocuments(BaseModel):
    """ç”¨æ–¼æª¢æŸ¥æ‰€å–å¾—æ–‡ä»¶ç›¸é—œæ€§çš„äºŒå…ƒï¼ˆã€Œyesã€æˆ–ã€Œnoã€ï¼‰è©•åˆ†ã€‚"""

    binary_score: str = Field(
        description="åˆ¤æ–·æ–‡ä»¶å’Œå•é¡Œæœ‰æ²’æœ‰ç›¸é—œï¼Œç”¨ã€Œyesã€æˆ–ã€Œnoã€è¡¨ç¤ºã€‚"
    )


class GradeHallucinations(BaseModel):
    """ç”¨æ–¼è¡¨ç¤ºç”Ÿæˆå›ç­”ä¸­æ˜¯å¦å­˜åœ¨å¹»è¦ºçš„äºŒå…ƒè©•åˆ†ã€‚"""

    binary_score: str = Field(
        description="åˆ¤æ–·å›ç­”æ˜¯å¦æ ¹æ“šäº‹å¯¦ï¼Œç”¨ã€Œyesã€æˆ–ã€Œnoã€è¡¨ç¤ºã€‚"
    )

class GradeAnswer(BaseModel):
    """ç”¨æ–¼è©•ä¼°å›ç­”æ˜¯å¦æœ‰å›æ‡‰å•é¡Œçš„äºŒå…ƒè©•åˆ†ã€‚"""

    binary_score: str = Field(
        description="åˆ¤æ–·å›ç­”æ˜¯å¦æœ‰å›æ‡‰å•é¡Œï¼Œç”¨ã€Œyesã€æˆ–ã€Œnoã€è¡¨ç¤º"
    )

class GraphState(TypedDict):
    """
    ç”¨ä¾†è¡¨ç¤ºGraphStateçš„é¡åˆ¥ã€‚

    å±æ€§:
        question: å•é¡Œ
        generation: LLMç”Ÿæˆå…§å®¹
        documents: æ–‡ä»¶åˆ—è¡¨
    """

    question: str
    generation: str
    documents: List[str]

import tempfile
from langchain_community.document_loaders import (
    PyMuPDFLoader,  # PDF
    UnstructuredWordDocumentLoader,  # Word
    UnstructuredPowerPointLoader,    # PPT
    UnstructuredExcelLoader,         # Excel
    TextLoader                       # TXT
)

# ----------- æ–°å¢ï¼šå…¨åŸŸ vectorstore åˆå§‹åŒ– -----------
if "vectorstore" not in st.session_state:
    st.session_state["vectorstore"] = None

uploaded_files = st.file_uploader(
    "ä¸Šå‚³çŸ¥è­˜æ–‡ä»¶ï¼ˆPDF, Word, PPT, Excel, TXTï¼‰",
    type=["pdf", "docx", "doc", "pptx", "xlsx", "xls", "txt"],
    accept_multiple_files=True
)

if uploaded_files:
    for uploaded_file in uploaded_files:
        file_ext = os.path.splitext(uploaded_file.name)[1].lower()
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp:
            tmp.write(uploaded_file.read())
            tmp_path = tmp.name
        # Loader åˆ¤æ–·
        if file_ext == ".pdf":
            loader = PyMuPDFLoader(tmp_path)
        elif file_ext in [".docx", ".doc"]:
            loader = UnstructuredWordDocumentLoader(tmp_path, mode="single")
        elif file_ext == ".pptx":
            loader = UnstructuredPowerPointLoader(tmp_path, mode="single")
        elif file_ext in [".xlsx", ".xls"]:
            loader = UnstructuredExcelLoader(tmp_path, mode="single")
        elif file_ext == ".txt":
            loader = TextLoader(tmp_path)
        else:
            st.warning(f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ï¼š{uploaded_file.name}")
            continue
        docs = loader.load()
        if not docs:   # â† æª¢æŸ¥æ–‡ä»¶å…§å®¹
            st.warning(f"æª”æ¡ˆ {uploaded_file.name} æ²’æœ‰ä»»ä½•å¯è®€å…§å®¹ï¼Œè«‹æ›å€‹æª”æ¡ˆï¼")
            continue  # è·³éé€™å€‹æª”æ¡ˆ

        # åˆ†æ®µ
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=30)
        splits = splitter.split_documents(docs)

        # åµŒå…¥
        embd = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=st.secrets["OPENAI_KEY"])

        if st.session_state["vectorstore"] is None:
            st.session_state["vectorstore"] = FAISS.from_documents(splits, embd)
        else:
            st.session_state["vectorstore"].add_documents(splits)

        st.success(f"æª”æ¡ˆ {uploaded_file.name} å·²åµŒå…¥çŸ¥è­˜åº«ï¼")

async def route_question(state):
    st.session_state.status.update(label=f"**---ROUTE QUESTION---**", state="running", expanded=True)
    st.session_state.log += "---ROUTE QUESTION---" + "\n\n"
    llm = ChatOpenAI(openai_api_key=st.secrets["OPENAI_KEY"], model="gpt-4.1-mini", temperature=0)
    structured_llm_router = llm.with_structured_output(RouteQuery)

    system = """ä½ æ˜¯å°‡ä½¿ç”¨è€…çš„å•é¡Œå°å‘vectorstoreæˆ–web_searchçš„å°ˆå®¶ã€‚
    å¦‚æœä½¿ç”¨è€…èƒ½å¾è³‡æ–™åº«ä¸­å–å¾—è³‡è¨Šè«‹ä½¿ç”¨vectorstoreï¼›å…¶ä»–æƒ…æ³å‰‡è«‹ä½¿ç”¨ç¶²è·¯æœå°‹"""
    route_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            ("human", "{question}"),
        ]
    )

    question_router = route_prompt | structured_llm_router

    question = state["question"]
    source = question_router.invoke({"question": question})
    if source.datasource == "web_search":
        st.session_state.log += "---ROUTE QUESTION TO WEB SEARCH---" + "\n\n"
        st.session_state.placeholder.markdown("---ROUTE QUESTION TO WEB SEARCH---")
        return "web_search"
    elif source.datasource == "vectorstore":
        st.session_state.placeholder.markdown("ROUTE QUESTION TO RAG")
        st.session_state.log += "ROUTE QUESTION TO RAG" + "\n\n"
        return "vectorstore"


async def retrieve(state):
    st.session_state.status.update(label=f"**---RETRIEVE---**", state="running", expanded=True)
    st.session_state.placeholder.markdown(f"RETRIEVINGâ€¦\n\nKEY WORD:{state['question']}")
    st.session_state.log += f"RETRIEVINGâ€¦\n\nKEY WORD:{state['question']}" + "\n\n"
    embd = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=st.secrets["OPENAI_KEY"])
    question = state["question"]

    # ğŸŸ¢ æ–°å¢ï¼šå„ªå…ˆæŸ¥è©¢æœ¬åœ° vectorstore
    if st.session_state.get("vectorstore", None) is not None:
        retriever = st.session_state["vectorstore"].as_retriever()
        documents = retriever.invoke(question)
        st.session_state.placeholder.markdown("RETRIEVE FROM USER UPLOADED VECTORSTORE SUCCESS!!")
        return {"documents": documents, "question": question}

async def web_search(state):
    st.session_state.status.update(label=f"**---WEB SEARCH---**", state="running", expanded=True)
    st.session_state.placeholder.markdown(f"WEB SEARCHâ€¦\n\nKEY WORD:{state['question']}")
    st.session_state.log += f"WEB SEARCHâ€¦\n\nKEY WORD:{state['question']}" + "\n\n"
    question = state["question"]

    # ä½¿ç”¨ TavilyClient åˆå§‹åŒ–
    client = TavilyClient(api_key=st.secrets["Tavily_key"])

    # åŸ·è¡Œæœå°‹ï¼Œä¸¦ä½¿ç”¨ä¸€äº›å¯é¸åƒæ•¸
    response = client.search(
        question,
        search_depth="advanced",
        max_results=5,
    )
    # å‡è¨­ response["results"] æ˜¯ä¸€å€‹åˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ æœ‰ "content" æ¬„ä½
    web_results = "\n".join([item["content"] for item in response["results"]])
    web_results = Document(page_content=web_results)

    return {"documents": web_results, "question": question}

async def grade_documents(state):
    st.session_state.number_trial += 1
    llm = ChatOpenAI(openai_api_key=st.secrets["OPENAI_KEY"],model="gpt-4.1-mini", temperature=0)
    structured_llm_grader = llm.with_structured_output(GradeDocuments)

    system = """ä½ æ˜¯è² è²¬è©•ä¼°æ‰€å–å¾—æ–‡ä»¶èˆ‡ä½¿ç”¨è€…å•é¡Œç›¸é—œæ€§çš„è©•åˆ†è€…ã€‚
å¦‚æœæ–‡ä»¶ä¸­åŒ…å«èˆ‡ä½¿ç”¨è€…å•é¡Œç›¸é—œçš„é—œéµå­—æˆ–æ„ç¾©ï¼Œè«‹è©•ç‚ºæœ‰ç›¸é—œæ€§ã€‚
ç›®çš„æ˜¯æ’é™¤æ˜é¡¯éŒ¯èª¤çš„å–å¾—çµæœï¼Œä¸éœ€è¦é€²è¡Œåš´æ ¼çš„æ¸¬è©¦ã€‚
è«‹ç”¨äºŒå…ƒè©•åˆ†ã€Œyesã€æˆ–ã€Œnoã€ä¾†è¡¨ç¤ºæ–‡ä»¶æ˜¯å¦èˆ‡å•é¡Œç›¸é—œã€‚"""
    grade_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            ("human", "Retrieved document: \n\n {document} \n\n User question: {question}"),
        ]
    )

    retrieval_grader = grade_prompt | structured_llm_grader
    st.session_state.status.update(label=f"**---CHECK DOCUMENT RELEVANCE TO QUESTION---**", state="running", expanded=False)
    st.session_state.log += "**---CHECK DOCUMENT RELEVANCE TO QUESTION---**" + "\n\n"
    question = state["question"]
    documents = state["documents"]
    filtered_docs = []
    i = 0
    for d in documents:
        if st.session_state.number_trial <= 2:
            file_name = d.metadata["source"]
            file_name = os.path.basename(file_name.replace("\\","/"))
            i += 1
            score = retrieval_grader.invoke(
                {"question": question, "document": d.page_content}
            )
            grade = score.binary_score
            if grade == "yes":
                st.session_state.status.update(label=f"**---GRADE: DOCUMENT RELEVANT---**", state="running", expanded=True)
                st.session_state.placeholder.markdown(f"DOC {i}/{len(documents)} : **RELEVANT**\n\n")
                st.session_state.log += "---GRADE: DOCUMENT RELEVANT---" + "\n\n"
                st.session_state.log += f"doc {i}/{len(documents)} : RELEVANT\n\n"
                filtered_docs.append(d)
            else:
                st.session_state.status.update(label=f"**---GRADE: DOCUMENT NOT RELEVANT---**", state="error", expanded=True)
                st.session_state.placeholder.markdown(f"DOC {i}/{len(documents)} : **NOT RELEVANT**\n\n")
                st.session_state.log += "---GRADE: DOCUMENT NOT RELEVANT---" + "\n\n"
                st.session_state.log += f"DOC {i}/{len(documents)} : NOT RELEVANT\n\n"
        else:

            filtered_docs.append(d)

    if not st.session_state.number_trial <= 2:
        st.session_state.status.update(label=f"**---NO NEED TO CHECK---**", state="running", expanded=True)
        st.session_state.placeholder.markdown("QUERY TRANSFORMATION HAS BEEN COMPLETED")
        st.session_state.log += "QUERY TRANSFORMATION HAS BEEN COMPLETED" + "\n\n"

    return {"documents": filtered_docs, "question": question}

async def generate(state):
    st.session_state.status.update(label=f"**---GENERATE---**", state="running", expanded=False)
    st.session_state.log += "---GENERATE---" + "\n\n"
    prompt = ChatPromptTemplate.from_messages(
            [
                ("system", """è«‹åƒè€ƒä½¿ç”¨è€…æä¾›çš„èƒŒæ™¯è³‡è¨Šä¾†å›ç­”å•é¡Œã€‚"""),
                ("human", """Question: {question} 
Context: {context}"""),
            ]
        )
        
    llm = ChatOpenAI(openai_api_key=st.secrets["OPENAI_KEY"], model_name="gpt-4.1-mini", temperature=0)

    rag_chain = prompt | llm | StrOutputParser()
    question = state["question"]
    documents = state["documents"]
    generation = rag_chain.invoke({"context": documents, "question": question})
    return {"documents": documents, "question": question, "generation": generation}


async def transform_query(state):
    st.session_state.status.update(label=f"**---TRANSFORM QUERY---**", state="running", expanded=True)
    st.session_state.placeholder.empty()
    st.session_state.log += "---TRANSFORM QUERY---" + "\n\n"
    llm = ChatOpenAI(openai_api_key=st.secrets["OPENAI_KEY"], model="gpt-4.1-mini", temperature=0)

    system = """ä½ æ˜¯ä¸€ä½å°‡è¼¸å…¥å•é¡Œè½‰æ›æˆæ›´é©åˆvectorstoreæœå°‹çš„å„ªåŒ–ç‰ˆæœ¬çš„å•é¡Œé‡å¯«è€…ã€‚
è«‹é–±è®€å•é¡Œï¼Œæ¨è«–æå•è€…çš„æ„åœ–æˆ–å«ç¾©ï¼Œä¸¦ç”¢ç”Ÿæ›´é©åˆvectorstoreæœå°‹çš„å•é¡Œã€‚"""
    re_write_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            (
                "human",
                "Here is the initial question: \n\n {question} \n Formulate an improved question.",
            ),
        ]
    )

    question_rewriter = re_write_prompt | llm | StrOutputParser()
    question = state["question"]
    documents = state["documents"]
    better_question = question_rewriter.invoke({"question": question})
    st.session_state.log += f"better_question : {better_question}\n\n"
    st.session_state.placeholder.markdown(f"better_question : {better_question}")
    return {"documents": documents, "question": better_question}


async def decide_to_generate(state):
    filtered_documents = state["documents"]
    if not filtered_documents:
        st.session_state.status.update(label=f"**---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---**", state="error", expanded=False)
        st.session_state.log += "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---" + "\n\n"
        return "transform_query"                                     
    else:
        st.session_state.status.update(label=f"**---DECISION: GENERATE---**", state="running", expanded=False)
        st.session_state.log += "---DECISION: GENERATE---" + "\n\n"
        return "generate"

async def grade_generation_v_documents_and_question(state):
    st.session_state.number_trial += 1
    st.session_state.status.update(label=f"**---CHECK HALLUCINATIONS---**", state="running", expanded=False)
    st.session_state.log += "---CHECK HALLUCINATIONS---" + "\n\n"
    llm = ChatOpenAI(openai_api_key=st.secrets["OPENAI_KEY"], model="gpt-4.1-mini", temperature=0)
    structured_llm_grader = llm.with_structured_output(GradeHallucinations)

    system = """ä½ æ˜¯è² è²¬è©•ä¼°å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆå…§å®¹æ˜¯å¦æ ¹æ“šï¼å—åˆ°æ‰€å–å¾—äº‹å¯¦é›†åˆæ”¯æŒçš„è©•åˆ†è€…ã€‚
è«‹çµ¦äºˆäºŒå…ƒè©•åˆ†ã€Œyesã€æˆ–ã€Œnoã€ã€‚ã€Œyesã€è¡¨ç¤ºå›ç­”æ˜¯æ ¹æ“šäº‹å¯¦é›†åˆç”¢ç”Ÿæˆ–å—åˆ°å…¶æ”¯æŒã€‚"""
    hallucination_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            ("human", "Set of facts: \n\n {documents} \n\n LLM generation: {generation}"),
        ]
    )
    llm = ChatOpenAI(openai_api_key=st.secrets["OPENAI_KEY"],model="gpt-4.1-mini", temperature=0)
    structured_llm_grader = llm.with_structured_output(GradeAnswer)

    system = """ä½ æ˜¯è² è²¬è©•ä¼°å›ç­”æ˜¯å¦æœ‰å›æ‡‰ï¼è§£æ±ºå•é¡Œçš„è©•åˆ†è€…ã€‚
è«‹çµ¦äºˆäºŒå…ƒè©•åˆ†ã€Œyesã€æˆ–ã€Œnoã€ã€‚ã€Œyesã€è¡¨ç¤ºå›ç­”æœ‰è§£æ±ºå•é¡Œã€‚"""
    answer_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            ("human", "User question: \n\n {question} \n\n LLM generation: {generation}"),
        ]
    )

    answer_grader = answer_prompt | structured_llm_grader
    hallucination_grader = hallucination_prompt | structured_llm_grader
    question = state["question"]
    documents = state["documents"]
    generation = state["generation"]
    score = hallucination_grader.invoke(
        {"documents": documents, "generation": generation}
    )
    grade = score.binary_score
    if st.session_state.number_trial <= 3:
        if grade == "yes":
            st.session_state.placeholder.markdown("DECISION: ANSWER IS BASED ON A SET OF FACTS")
            st.session_state.log += "---DECISION: ANSWER IS BASED ON A SET OF FACTS---" + "\n\n"
            st.session_state.log += "---GRADE GENERATION vs QUESTION---" + "\n\n"
            score = answer_grader.invoke({"question": question, "generation": generation})
            grade = score.binary_score
            st.session_state.status.update(label=f"**---GRADE GENERATION vs QUESTION---**", state="running", expanded=True)
            if grade == "yes":
                st.session_state.status.update(label=f"**---DECISION: GENERATION ADDRESSES QUESTION---**", state="running", expanded=True)
                with st.session_state.placeholder:
                    st.markdown("**USEFUL!!**")
                    st.markdown(f"question : {question}")
                    st.markdown(f"generation : {generation}")                   
                    st.session_state.log += "---DECISION: GENERATION ADDRESSES QUESTION---" + "\n\n"
                    st.session_state.log += f"USEFUL!!\n\n"
                    st.session_state.log += f"question:{question}\n\n"
                    st.session_state.log += f"generation:{generation}\n\n"
                return "useful"
            else:
                st.session_state.number_trial -= 1
                st.session_state.status.update(label=f"**---DECISION: GENERATION DOES NOT ADDRESS QUESTION---**", state="error", expanded=True)
                with st.session_state.placeholder:
                    st.markdown("**NOT USEFUL**")
                    st.markdown(f"question:{question}")
                    st.markdown(f"generation:{generation}")
                    st.session_state.log += "---DECISION: GENERATION DOES NOT ADDRESS QUESTION---" + "\n\n"
                    st.session_state.log += f"NOT USEFUL\n\n"
                    st.session_state.log += f"question:{question}\n\n"
                    st.session_state.log += f"generation:{generation}\n\n"
                return "not useful"
        else:
            st.session_state.status.update(label=f"**---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---**", state="error", expanded=True)
            with st.session_state.placeholder:
                st.markdown("not grounded")
                st.markdown(f"question:{question}")
                st.markdown(f"generation:{generation}")
                st.session_state.log += "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---" + "\n\n"
                st.session_state.log += f"not grounded\n\n"
                st.session_state.log += f"question:{question}\n\n"
                st.session_state.log += f"generation:{generation}\n\n"
            return "not supported"
    else:
        st.session_state.status.update(label=f"**---NO NEED TO CHECK---**", state="running", expanded=False)
        st.session_state.placeholder.markdown("TRIAL LIMIT EXCEEDED")
        st.session_state.log += "---NO NEED TO CHECK---" + "\n\n"
        st.session_state.log += "TRIAL LIMIT EXCEEDED" + "\n\n"
        return "useful"

async def run_workflow(inputs):
    st.session_state.number_trial = 0
    with st.status(label="**GO!!**", expanded=True,state="running") as st.session_state.status:
        st.session_state.placeholder = st.empty()
        value = await st.session_state.workflow.ainvoke(inputs)

    st.session_state.placeholder.empty()
    st.session_state.message_placeholder = st.empty()
    st.session_state.status.update(label="**FINISH!!**", state="complete", expanded=False)
    #st.session_state.message_placeholder.markdown(value["generation"])
    #with st.popover("log"):
    #    st.markdown(st.session_state.log)
    return value   # <== åŠ é€™è¡Œï¼ï¼

def st_rag_langgraph():

    if 'log' not in st.session_state:
        st.session_state.log = ""

    if 'chat_history' not in st.session_state:
        st.session_state['chat_history'] = []

    if 'status_container' not in st.session_state:
        st.session_state.status_container = st.empty()

    if not hasattr(st.session_state, "workflow"):

        workflow = StateGraph(GraphState)

        workflow.add_node("web_search", web_search)
        workflow.add_node("retrieve", retrieve)
        workflow.add_node("grade_documents", grade_documents)
        workflow.add_node("generate", generate)
        workflow.add_node("transform_query", transform_query)

        workflow.add_conditional_edges(
            START,
            route_question,
            {
                "vectorstore": "retrieve",
                "web_search": "web_search",
            },
        )
        workflow.add_edge("web_search", "generate")
        workflow.add_edge("retrieve", "grade_documents")
        workflow.add_conditional_edges(
            "grade_documents",
            decide_to_generate,
            {
                "transform_query": "transform_query",
                "generate": "generate",
            },
        )
        workflow.add_edge("transform_query", "retrieve")
        workflow.add_conditional_edges(
            "generate",
            grade_generation_v_documents_and_question,
            {
                "not supported": "generate",
                "useful": END,
                "not useful": "transform_query",
            },
        )

        app = workflow.compile()
        app = app.with_config(recursion_limit=10,run_name="Agent",tags=["Agent"])
        app.name = "Agent"
        st.session_state.workflow = app


    st.title("Adaptive RAG by LangGraph")

    # é¡¯ç¤ºå…¨éƒ¨æ­·å²
    for entry in st.session_state['chat_history']:
        with st.chat_message(entry["role"]):
            st.markdown(entry["content"])

    if prompt := st.chat_input("è«‹è¼¸å…¥å•é¡Œ"):
        st.session_state.log = ""
        # æ–°ç•™è¨€å­˜é€²æ­·å²
        st.session_state['chat_history'].append({"role": "user", "content": prompt})

        with st.chat_message("user"):
            st.markdown(prompt)

        # å»ºç«‹å°è©±ä¸Šä¸‹æ–‡ä¸²
        conversation = []
        for turn in st.session_state['chat_history']:
            conversation.append({"role": turn["role"], "content": turn["content"]})

        inputs = {"question": prompt, "chat_history": conversation}
        value = asyncio.run(run_workflow(inputs))   # <<<< æ‹¿ value["generation"]

        # <==== é€™é‚Šæ˜¯é—œéµï¼ŒæŠŠAIå›ç­”åŠ å…¥chat_history
        st.session_state['chat_history'].append({"role": "assistant", "content": value["generation"]})

        # ä¹Ÿå»ºè­°ç•¶ä¸‹ç›´æ¥å±•ç¤º
        with st.chat_message("assistant"):
            st.markdown(value["generation"])

if __name__ == "__main__":
    st_rag_langgraph()
