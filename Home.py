import os
import streamlit as st
from datetime import datetime
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from langchain_core.prompts import PromptTemplate
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode, tools_condition
import inspect
from typing import Callable, TypeVar, List, Dict, Any, Optional
from pydantic import BaseModel, Field
import time
import re
import requests
from openai import OpenAI
import traceback
from langchain_core.callbacks.base import BaseCallbackHandler
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper

st.set_page_config(
    page_title="Anya",
    layout="wide",
    page_icon="ğŸ¥œ",
    initial_sidebar_state="collapsed"
)

# --- 1. Streamlit session_state åˆå§‹åŒ– ---
if "messages" not in st.session_state:
    st.session_state.messages = [AIMessage(content="å—¨å—¨ï½å®‰å¦®äºä¾†äº†ï¼ğŸ‘‹ æœ‰ä»€éº¼æƒ³å•å®‰å¦®äºçš„å—ï¼Ÿ")]
if "selected_model" not in st.session_state:
    st.session_state.selected_model = "gpt-4.1"
if "current_model" not in st.session_state:
    st.session_state.current_model = None
if "llm" not in st.session_state:
    st.session_state.llm = None

# å®šç¾© WikiInputs
class WikiInputs(BaseModel):
    query: str = Field(description="è¦æŸ¥è©¢çš„é—œéµå­—")

# --- 2. LLM åˆå§‹åŒ– ---
def ensure_llm():
    if (
        st.session_state.llm is None
        or st.session_state.current_model != st.session_state.selected_model
    ):
        st.session_state.llm = ChatOpenAI(
            model=st.session_state.selected_model,
            openai_api_key=st.secrets["OPENAI_KEY"],
            temperature=0.0,
            streaming=True,
        )
        st.session_state.current_model = st.session_state.selected_model

ensure_llm()

# --- 3. å·¥å…·å®šç¾© ---
# === OpenAI åˆå§‹åŒ– ===
client = OpenAI(api_key=st.secrets["OPENAI_KEY"])

# === Meta Prompting å·¥å…· ===
def meta_optimize_prompt(simple_prompt: str, goal: str) -> str:
    meta_prompt = f"""
    è«‹å„ªåŒ–ä»¥ä¸‹ promptï¼Œä½¿å…¶èƒ½æ›´æœ‰æ•ˆé”æˆã€Œ{goal}ã€ï¼Œä¸¦ç¬¦åˆ prompt engineering æœ€ä½³å¯¦è¸ã€‚
    {simple_prompt}
    åªå›å‚³å„ªåŒ–å¾Œçš„ promptã€‚
    """
    response = client.chat.completions.create(
        model="o4-mini",
        messages=[{"role": "user", "content": meta_prompt}]
    )
    return response.choices[0].message.content.strip()

# === ç”¢ç”ŸæŸ¥è©¢ï¼ˆä¸­è‹±æ–‡ï¼‰ ===
def generate_queries(topic: str, model="gpt-4.1-mini") -> List[str]:
    simple_prompt = f"""è«‹é‡å°ã€Œ{topic}ã€é€™å€‹ä¸»é¡Œï¼Œåˆ†åˆ¥ç”¨ç¹é«”ä¸­æ–‡èˆ‡è‹±æ–‡å„ç”¢ç”Ÿä¸‰å€‹é©åˆç”¨æ–¼ç¶²è·¯æœå°‹çš„æŸ¥è©¢é—œéµå­—ï¼Œä¸¦ä»¥å¦‚ä¸‹ JSON æ ¼å¼å›è¦†ï¼š
{{
    "zh": ["æŸ¥è©¢1", "æŸ¥è©¢2", "æŸ¥è©¢3"],
    "en": ["query1", "query2", "query3"]
}}
"""
    optimized_prompt = meta_optimize_prompt(simple_prompt, "ç”¢ç”Ÿå¤šå…ƒä¸”å…·é‡å°æ€§çš„æŸ¥è©¢é—œéµå­—")
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": optimized_prompt}]
    )
    content = response.choices[0].message.content
    try:
        queries = json.loads(content)
    except Exception:
        import re
        content = re.sub(r"[\u4e00-\u9fff]+ï¼š", "", content)
        content = content.replace("'", '"')
        queries = json.loads(content)
    return queries["zh"] + queries["en"]

# === æŸ¥è©¢æ‘˜è¦ ===
def auto_summarize(text: str, model="gpt-4.1-mini") -> str:
    simple_prompt = f"è«‹ç”¨ç¹é«”ä¸­æ–‡æ‘˜è¦ä»¥ä¸‹å…§å®¹ï¼Œé‡é»æ¢åˆ—ï¼Œ100å­—å…§ï¼š\n{text}"
    optimized_prompt = meta_optimize_prompt(simple_prompt, "ç”¢ç”Ÿç²¾ç°¡ä¸”é‡é»æ˜ç¢ºçš„æ‘˜è¦")
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": optimized_prompt}]
    )
    return response.choices[0].message.content.strip()

# 2. ç« ç¯€è¦åŠƒï¼ˆæ¨ç†æ¨¡å‹ï¼‰
def plan_report(topic, search_summaries, model="o4-mini"):
    prompt = f"""Formatting re-enabled
# Role and Objective
ä½ æ˜¯ä¸€ä½å°ˆæ¥­æŠ€è¡“å¯«æ‰‹ï¼Œç›®æ¨™æ˜¯é‡å°ã€Œ{topic}ã€é€™å€‹ä¸»é¡Œï¼Œæ ¹æ“šä¸‹æ–¹æœå°‹æ‘˜è¦ï¼Œè¦åŠƒä¸€ä»½å®Œæ•´ã€æ·±å…¥ã€çµæ§‹åŒ–çš„ç ”ç©¶å ±å‘Šã€‚

# Instructions
- å ±å‘Šéœ€åŒ…å«5-7å€‹ç« ç¯€ï¼Œæ¯ç« ç¯€éœ€æœ‰æ˜ç¢ºæ¨™é¡Œã€‚
- æ¯å€‹ç« ç¯€**å¿…é ˆåŒ…å«2-4å€‹å°æ¨™é¡Œ**ï¼ˆå­è­°é¡Œï¼‰ï¼Œæ¯å€‹å°æ¨™é¡Œä¸‹è¦æœ‰2-3å¥ç´°ç¯€èªªæ˜ã€‚
- å°æ¨™é¡Œå¯æ¶µè“‹ï¼šç”¢æ¥­ç¾æ³ã€æŠ€è¡“ç´°ç¯€ã€åœ‹éš›æ¯”è¼ƒã€æœªä¾†è¶¨å‹¢ã€æŒ‘æˆ°ã€è§£æ±ºæ–¹æ¡ˆã€æ¡ˆä¾‹ã€æ•¸æ“šç­‰ã€‚
- ç« ç¯€è¦åŠƒè¦æœ‰é‚è¼¯é †åºï¼Œå…§å®¹è¦æœ‰å±¤æ¬¡ã€‚
- è«‹ç”¨ç¹é«”ä¸­æ–‡æ¢åˆ—å¼å›è¦†ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

# Output Format
1. ç« ç¯€æ¨™é¡Œ
    - å°æ¨™é¡Œ1ï¼šç´°ç¯€èªªæ˜
    - å°æ¨™é¡Œ2ï¼šç´°ç¯€èªªæ˜
    - å°æ¨™é¡Œ3ï¼šç´°ç¯€èªªæ˜
2. ç« ç¯€æ¨™é¡Œ
    - å°æ¨™é¡Œ1ï¼šç´°ç¯€èªªæ˜
    - å°æ¨™é¡Œ2ï¼šç´°ç¯€èªªæ˜
    - å°æ¨™é¡Œ3ï¼šç´°ç¯€èªªæ˜
...

# æœå°‹æ‘˜è¦
{search_summaries}
"""
    response = client.responses.create(
        model=model,
        reasoning={"effort": "medium", "summary": "auto"},
        input=[{"role": "user", "content": prompt}]
    )
    return response.output_text

# 3. è§£æç« ç¯€
def parse_sections(plan: str):
    section_pattern = r"\d+\.\s*([^\n]+)"
    sub_pattern = r"-\s*([^\nï¼š:]+)[ï¼š:]\s*([^\n]+)"
    sections = []
    section_blocks = re.split(r"\d+\.\s*", plan)[1:]
    for block in section_blocks:
        lines = block.strip().split("\n")
        title = lines[0].strip()
        subs = []
        for line in lines[1:]:
            m = re.match(sub_pattern, line.strip())
            if m:
                subs.append({"subtitle": m.group(1).strip(), "desc": m.group(2).strip()})
        sections.append({"title": title, "subtitles": subs})
    return sections

# 4. ç« ç¯€æŸ¥è©¢ç”¢ç”Ÿ
def section_queries(section_title, section_desc, model="gpt-4.1-mini"):
    prompt = f"""è«‹é‡å°ç« ç¯€ã€Œ{section_title}ã€({section_desc})ï¼Œåˆ†åˆ¥ç”¨ç¹é«”ä¸­æ–‡èˆ‡è‹±æ–‡å„ç”¢ç”Ÿå…©å€‹é©åˆç”¨æ–¼ç¶²è·¯æœå°‹çš„æŸ¥è©¢é—œéµå­—ï¼Œå›å‚³ JSON æ ¼å¼ï¼š
{{
    "zh": ["æŸ¥è©¢1", "æŸ¥è©¢2"],
    "en": ["query1", "query2"]
}}
"""
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}]
    )
    content = response.choices[0].message.content
    try:
        queries = json.loads(content)
    except Exception:
        content = re.sub(r"[\u4e00-\u9fff]+ï¼š", "", content)
        content = content.replace("'", '"')
        queries = json.loads(content)
    return queries["zh"] + queries["en"]

# 5. ç« ç¯€æ’°å¯«ï¼ˆç›´æ¥ç”¨å¤šç­†æŸ¥è©¢çµæœï¼‰
def section_write(section_title, section_desc, search_results, model="gpt-4.1-mini"):
    prompt = f"""
# Role and Objective
ä½ æ˜¯ä¸€ä½å°ˆæ¥­æŠ€è¡“å¯«æ‰‹ï¼Œæ ¹æ“šä¸‹æ–¹ç« ç¯€ä¸»é¡Œã€èªªæ˜èˆ‡ã€Œå¤šç­†ç¶²è·¯æŸ¥è©¢çµæœã€ï¼Œæ’°å¯«ä¸€æ®µå…§å®¹è±å¯Œã€çµæ§‹æ¸…æ™°ã€å…·é«”è©³å¯¦çš„ç« ç¯€å…§å®¹ã€‚

# Instructions
- å…§å®¹éœ€è‡³å°‘600å­—ï¼Œä¸¦æ¶µè“‹ï¼šå…·é«”æ•¸æ“šã€çœŸå¯¦æ¡ˆä¾‹ã€åœ‹éš›æ¯”è¼ƒã€ç”¢æ¥­ç¾æ³ã€æŠ€è¡“ç´°ç¯€ã€æœªä¾†è¶¨å‹¢ã€æŒ‘æˆ°èˆ‡è§£æ±ºæ–¹æ¡ˆã€‚
- å¿…é ˆæ ¹æ“šä¸‹æ–¹æ¯ä¸€ç­†æŸ¥è©¢çµæœï¼Œæ•´åˆå‡ºå®Œæ•´å…§å®¹ã€‚
- æ¯å€‹å°æ¨™é¡Œä¸‹å¿…é ˆæœ‰2-3æ®µå…·é«”èªªæ˜ã€‚
- æ¢åˆ—é‡é»åªèƒ½æ”¾åœ¨ç« ç¯€çµå°¾ï¼Œæ­£æ–‡å¿…é ˆæ˜¯å®Œæ•´æ®µè½ã€‚
- æ–‡æœ«è«‹ç”¨ã€Œ## ä¾†æºã€åˆ—å‡ºæ‰€æœ‰å¼•ç”¨ä¾†æºï¼ˆMarkdownæ ¼å¼ï¼‰ï¼Œä¾†æºå¿…é ˆä¾†è‡ªä¸‹æ–¹æŸ¥è©¢çµæœã€‚
- è«‹å‹¿çœç•¥ç´°ç¯€ï¼Œè‹¥æœ‰å¤šå€‹è§€é»è«‹åˆ†æ®µèªªæ˜ã€‚
- è«‹å‹¿é‡è¤‡å…§å®¹ï¼Œé¿å…ç©ºæ³›æ•˜è¿°ã€‚
- è«‹ç”¨ç¹é«”ä¸­æ–‡æ’°å¯«ã€‚

# ç« ç¯€ä¸»é¡Œ
{section_title}

# ç« ç¯€èªªæ˜
{section_desc}

# å¤šç­†æŸ¥è©¢çµæœï¼ˆæ¯ç­†éƒ½è¦åƒè€ƒï¼‰
{search_results}
"""
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content.strip()

# === ä¾†æºæå– ===
def extract_sources(content: str) -> List[str]:
    # å‡è¨­ä¾†æºæ ¼å¼ç‚º markdown link
    return re.findall(r'\[([^\]]+)\]\((https?://[^\)]+)\)', content)

# === ç« ç¯€å…§å®¹è©•åˆ†èˆ‡è£œå¼·å»ºè­° ===
def section_grade(section_title: str, section_content: str, model="gpt-4.1-mini") -> Dict[str, Any]:
    simple_prompt = f"""è«‹è©•åˆ†ä»¥ä¸‹ç« ç¯€å…§å®¹æ˜¯å¦å®Œæ•´ã€æ­£ç¢ºã€å¯è®€æ€§ä½³ï¼Œè‹¥ä¸åŠæ ¼è«‹åˆ—å‡ºéœ€è£œå……çš„æŸ¥è©¢é—œéµå­—ï¼ˆä¸­è‹±æ–‡å„ä¸€ï¼‰ï¼Œå›å‚³ JSON æ ¼å¼ï¼š
{{
    "grade": "pass" æˆ– "fail",
    "follow_up_queries": ["æŸ¥è©¢1", "query2"]
}}
ç« ç¯€ï¼š{section_title}
å…§å®¹ï¼š
{section_content}
"""
    optimized_prompt = meta_optimize_prompt(simple_prompt, "åš´è¬¹è©•åˆ†ä¸¦ç”¢ç”Ÿå…·é«”è£œå¼·å»ºè­°")
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": optimized_prompt}]
    )
    try:
        return json.loads(response.choices[0].message.content)
    except:
        return {"grade": "pass", "follow_up_queries": []}

# 6. åæ€æµç¨‹ï¼ˆæ¨ç†æ¨¡å‹ï¼‰
def reflect_report(report: str, model="o4-mini"):
    prompt = f"""Formatting re-enabled
# Role and Objective
ä½ æ˜¯ä¸€ä½å°ˆæ¥­å¯©ç¨¿äººï¼Œç›®æ¨™æ˜¯æª¢æŸ¥ä¸‹æ–¹å ±å‘Šçš„é‚è¼¯ã€æ­£ç¢ºæ€§ã€å®Œæ•´æ€§èˆ‡å…§å®¹è±å¯Œåº¦ã€‚

# Instructions
- è«‹é€ä¸€æª¢æŸ¥æ¯å€‹ç« ç¯€æ˜¯å¦æœ‰å°æ¨™é¡Œï¼Œä¸”æ¯å€‹å°æ¨™é¡Œä¸‹æ˜¯å¦æœ‰2-3å¥å…·é«”ç´°ç¯€èªªæ˜ã€‚
- æª¢æŸ¥å…§å®¹æ˜¯å¦æ¶µè“‹æ•¸æ“šã€æ¡ˆä¾‹ã€åœ‹éš›è§€é»ã€æŠ€è¡“ç´°ç¯€ã€æœªä¾†è¶¨å‹¢ç­‰è±å¯Œé¢å‘ã€‚
- æª¢æŸ¥æ˜¯å¦æœ‰æ˜ç¢ºå¼•ç”¨ä¾†æºã€‚
- è‹¥æœ‰å…§å®¹éæ–¼ç°¡ç•¥ã€éºæ¼é‡è¦é¢å‘ï¼Œè«‹æ˜ç¢ºæŒ‡å‡ºéœ€è£œå……çš„ç« ç¯€ã€å°æ¨™é¡Œèˆ‡å»ºè­°æŸ¥è©¢é—œéµå­—ã€‚
- è‹¥å…§å®¹å·²è¶³å¤ è±å¯Œä¸”ç„¡æ˜é¡¯éºæ¼ï¼Œè«‹å›è¦† "OK"ã€‚
- è«‹ç”¨ç¹é«”ä¸­æ–‡å›è¦†ã€‚

# å ±å‘Šå…§å®¹
{report}
"""
    response = client.responses.create(
        model=model,
        reasoning={"effort": "medium", "summary": "auto"},
        input=[{"role": "user", "content": prompt}]
    )
    return response.output_text

# === çµ„åˆç« ç¯€ ===
def combine_sections(section_contents: List[Dict[str, Any]]) -> str:
    return "\n\n".join([f"## {s['title']}\n\n{s['content']}" for s in section_contents])

# === ä¸»æµç¨‹ï¼ˆå«æ¨ç†éˆè¿½è¹¤ï¼‰ ===
def deep_research_pipeline(topic):
    logs = []
    try:
        # 1. ç”¢ç”ŸæŸ¥è©¢
        try:
            queries = section_queries(topic, topic)
            logs.append({"step": "generate_queries", "queries": queries})
        except Exception as e:
            logs.append({"step": "generate_queries", "error": str(e), "traceback": traceback.format_exc()})
            return {"error": "ç”¢ç”ŸæŸ¥è©¢å¤±æ•—", "logs": logs}

        # 2. æŸ¥è©¢æ‰€æœ‰ query
        all_results = []
        try:
            for q in queries:
                try:
                    result = ddgs_search(q)
                    all_results.append(result)
                except Exception as e:
                    logs.append({"step": "search", "query": q, "error": str(e), "traceback": traceback.format_exc()})
                    all_results.append(f"æŸ¥è©¢å¤±æ•—: {q}")
            logs.append({"step": "search", "results": all_results})
        except Exception as e:
            logs.append({"step": "search", "error": str(e), "traceback": traceback.format_exc()})
            return {"error": "æŸ¥è©¢å¤±æ•—", "logs": logs}

        # 3. è¦åŠƒç« ç¯€
        try:
            search_summary = "\n\n".join(all_results)
            plan = plan_report(topic, search_summary)
            logs.append({"step": "plan_report", "plan": plan})
        except Exception as e:
            logs.append({"step": "plan_report", "error": str(e), "traceback": traceback.format_exc(), "search_summary": search_summary})
            return {"error": "ç« ç¯€è¦åŠƒå¤±æ•—", "logs": logs}

        # 4. ç« ç¯€åˆ†æ®µæŸ¥è©¢/æ’°å¯«
        try:
            sections = parse_sections(plan)
            section_contents = []
            for section in sections:
                s_queries = []
                for sub in section["subtitles"]:
                    try:
                        sub_queries = section_queries(sub["subtitle"], sub["desc"])
                        s_queries.extend(sub_queries)
                    except Exception as e:
                        logs.append({"step": "section_queries", "subtitle": sub["subtitle"], "desc": sub["desc"], "error": str(e), "traceback": traceback.format_exc()})
                s_results = []
                for q in s_queries:
                    try:
                        s_results.append(ddgs_search(q))
                    except Exception as e:
                        logs.append({"step": "section_search", "query": q, "error": str(e), "traceback": traceback.format_exc()})
                        s_results.append(f"æŸ¥è©¢å¤±æ•—: {q}")
                search_results = "\n\n".join(s_results)
                try:
                    content = section_write(
                        section["title"],
                        "ï¼›".join([f"{sub['subtitle']}ï¼š{sub['desc']}" for sub in section["subtitles"]]),
                        search_results
                    )
                except Exception as e:
                    logs.append({"step": "section_write", "section": section["title"], "error": str(e), "traceback": traceback.format_exc(), "search_results": search_results})
                    content = f"ç« ç¯€å…§å®¹ç”¢ç”Ÿå¤±æ•—: {section['title']}"
                section_contents.append({
                    "title": section["title"],
                    "content": content
                })
                logs.append({
                    "step": "section",
                    "section": section["title"],
                    "queries": s_queries,
                    "search_results": search_results,
                    "content": content
                })
        except Exception as e:
            logs.append({"step": "section_loop", "error": str(e), "traceback": traceback.format_exc()})
            return {"error": "ç« ç¯€åˆ†æ®µæŸ¥è©¢/æ’°å¯«å¤±æ•—", "logs": logs}

        # 5. çµ„åˆå ±å‘Š
        try:
            report = "\n\n".join([f"## {s['title']}\n\n{s['content']}" for s in section_contents])
            logs.append({"step": "combine_report", "report": report})
        except Exception as e:
            logs.append({"step": "combine_report", "error": str(e), "traceback": traceback.format_exc()})
            return {"error": "çµ„åˆå ±å‘Šå¤±æ•—", "logs": logs}

        # 6. åæ€æµç¨‹ï¼ˆæœ€å¤š2æ¬¡ï¼‰
        for i in range(2):
            try:
                reflection = reflect_report(report)
                logs.append({"step": "reflection", "round": i+1, "reflection": reflection})
                if reflection.strip().upper() == "OK":
                    break
                else:
                    # è‹¥éœ€è£œå……ï¼Œå¯æ ¹æ“š reflection ç”¢ç”Ÿæ–°æŸ¥è©¢èˆ‡è£œå……å…§å®¹ï¼ˆå¯é€²ä¸€æ­¥è‡ªå‹•åŒ–ï¼‰
                    pass
            except Exception as e:
                logs.append({"step": "reflection", "round": i+1, "error": str(e), "traceback": traceback.format_exc()})
                break

        # 7. çµæ§‹åŒ–è¼¸å‡º
        output = {
            "topic": topic,
            "plan": plan,
            "sections": section_contents,
            "report": report,
            "logs": logs
        }
        return output

    except Exception as e:
        logs.append({"step": "pipeline_outer", "error": str(e), "traceback": traceback.format_exc()})
        return {"error": "pipeline_outer_error", "logs": logs}
@tool
def deep_research_pipeline_tool(topic: str) -> Dict[str, Any]:
    """
    é‡å°æŒ‡å®šä¸»é¡Œè‡ªå‹•é€²è¡Œå¤šæ­¥æ·±åº¦ç ”ç©¶ï¼Œå›å‚³çµæ§‹åŒ–å ±å‘Šï¼ˆå«ç« ç¯€ã€å…§å®¹ã€ä¾†æºã€æ¨ç†éˆï¼‰ã€‚
    """
    try:
        return deep_research_pipeline(topic)
    except Exception as e:
        return {"error": str(e), "traceback": traceback.format_exc()}

@tool
def wiki_tool(query: str) -> str:
    """
    æŸ¥è©¢ Wikipediaï¼ˆè‹±æ–‡ï¼‰ï¼Œè¼¸å…¥ä»»ä½•èªè¨€çš„é—œéµå­—éƒ½å¯ä»¥ã€‚
    """
    try:
        tool_obj = WikipediaQueryRun(
            name="wiki-tool",
            description="æŸ¥è©¢ Wikipediaï¼ˆè‹±æ–‡ï¼‰ï¼Œè¼¸å…¥ä»»ä½•èªè¨€çš„é—œéµå­—éƒ½å¯ä»¥ã€‚",
            args_schema=WikiInputs,
            api_wrapper=WikipediaAPIWrapper(lang="en", doc_content_chars_max=800, top_k_results=1),
            return_direct=True,
        )
        result = tool_obj.invoke({"query": query})
        return result
    except Exception as e:
        return f"wiki_tool error: {e}"

@tool
def ddgs_search(query: str) -> str:
    """DuckDuckGo æœå°‹ï¼ˆåŒæ™‚æŸ¥è©¢ç¶²é èˆ‡æ–°èï¼Œå›å‚³ markdown æ¢åˆ—æ ¼å¼ä¸¦é™„ä¾†æºï¼‰ã€‚"""
    try:
        from duckduckgo_search import DDGS
        ddgs = DDGS()
        web_results = ddgs.text(query, region="wt-wt", safesearch="moderate", max_results=5)
        news_results = ddgs.news(query, region="wt-wt", safesearch="moderate", max_results=5)
        all_results = []
        if isinstance(web_results, list):
            all_results.extend(web_results)
        if isinstance(news_results, list):
            all_results.extend(news_results)
        docs = []
        sources = []
        for item in all_results:
            title = item.get("title", "ç„¡æ¨™é¡Œ")
            link = item.get("href", "") or item.get("link", "") or item.get("url", "")
            snippet = item.get("body", "") or item.get("snippet", "")
            docs.append(f"- [{title}]({link})\n  > {snippet}")
            if link:
                sources.append(link)
        if not docs:
            return "No results found."
        markdown_content = "\n".join(docs)
        source_block = "\n\n## ä¾†æº\n" + "\n".join(sources)
        return markdown_content + source_block
    except Exception as e:
        return f"Error from DuckDuckGo: {e}"

@tool
def datetime_tool() -> str:
    """ç¢ºèªç•¶å‰çš„æ—¥æœŸå’Œæ™‚é–“ã€‚"""
    return datetime.now().isoformat()

# ä½ çš„ deep_thought_tool
def analyze_deeply(input_question: str) -> str:
    """ä½¿ç”¨OpenAIçš„æ¨¡å‹ä¾†æ·±å…¥åˆ†æå•é¡Œä¸¦è¿”å›çµæœã€‚"""
    prompt_template = PromptTemplate(
        template="""è«‹åˆ†æä»¥ä¸‹å•é¡Œï¼Œä¸¦ä»¥æ­£é«”ä¸­æ–‡æä¾›è©³ç´°çš„çµè«–å’Œç†ç”±ï¼Œè«‹ä¾æ“šäº‹å¯¦åˆ†æï¼Œä¸è€ƒæ…®è³‡æ–™çš„æ™‚é–“å› ç´ ï¼š

å•é¡Œï¼š{input_question}

æŒ‡å°æ–¹é‡ï¼š
1. æè¿°å•é¡Œçš„èƒŒæ™¯å’Œç›¸é—œè³‡è¨Šã€‚
2. ç›´æ¥çµ¦å‡ºä½ çš„çµè«–ï¼Œä¸¦æä¾›æ”¯æŒè©²çµè«–çš„ç†ç”±ã€‚
3. å¦‚æœæœ‰ä¸ç¢ºå®šçš„åœ°æ–¹ï¼Œè«‹æ˜ç¢ºæŒ‡å‡ºã€‚
4. ç¢ºä¿ä½ çš„å›ç­”æ˜¯è©³ç´°ä¸”æœ‰æ¢ç†çš„ã€‚
""",
        input_variables=["input_question"],
    )
    llmo1 = ChatOpenAI(
        openai_api_key=st.secrets["OPENAI_KEY"],
        model="o4-mini",
        streaming=True,
    )
    prompt = prompt_template.format(input_question=input_question)
    result = llmo1.invoke(prompt)
    # åŒ…è£æˆ content å±¬æ€§
    return str(result)

@tool
def deep_thought_tool(content: str) -> str:
    """
    å®‰å¦®äºä»”ç´°æ€è€ƒæ·±å…¥åˆ†æã€‚
    """
    try:
        return analyze_deeply(content).strip() + "\n\n---\n\n"
    except Exception as e:
        return f"deep_thought_tool error: {e}"

@tool
def get_webpage_answer(query: str) -> str:
    """
    æ ¹æ“šç”¨æˆ¶çš„å•é¡Œèˆ‡ç¶²å€ï¼Œè‡ªå‹•å–å¾—ç¶²é å…§å®¹ä¸¦å›ç­”å•é¡Œã€‚
    è«‹è¼¸å…¥æ ¼å¼å¦‚ï¼šã€Œè«‹å¹«æˆ‘ç¸½çµ https://example.com é€™ç¯‡æ–‡ç« çš„é‡é»ã€
    """
    # 1. æŠ½å–ç¶²å€èˆ‡å•é¡Œ
    url_match = re.search(r'(https?://[^\s]+)', query)
    url = url_match.group(1) if url_match else None
    question = query.replace(url, '').strip() if url else query
    if not url:
        return "æœªåµæ¸¬åˆ°ç¶²å€ï¼Œè«‹æä¾›æ­£ç¢ºçš„ç¶²å€ã€‚"
    # 2. å–å¾— Jina Reader å…§å®¹
    jina_url = f"https://r.jina.ai/{url}"
    try:
        resp = requests.get(jina_url, timeout=15)
        if resp.status_code != 200:
            return "ç„¡æ³•å–å¾—ç¶²é å…§å®¹ï¼Œè«‹ç¢ºèªç¶²å€æ˜¯å¦æ­£ç¢ºã€‚"
        content = resp.text
    except Exception as e:
        return f"å–å¾—ç¶²é å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"
    # 3. ç›´æ¥åœ¨é€™è£¡åˆå§‹åŒ– LLM
    try:
        llmurl = ChatOpenAI(
            openai_api_key=st.secrets["OPENAI_KEY"],  # æˆ–ç”¨os.environ["OPENAI_API_KEY"]
            model="gpt-4.1-mini",  # ä½ å¯ä»¥æ ¹æ“šéœ€æ±‚é¸æ“‡æ¨¡å‹
            streaming=False,
        )
        prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹ç¶²é å…§å®¹ï¼Œé‡å°å•é¡Œã€Œ{question}ã€ä»¥æ¢åˆ—å¼æ‘˜è¦é‡é»ï¼Œä¸¦ç”¨æ­£é«”ä¸­æ–‡å›ç­”ï¼š

{content}
"""
        result = llmurl.invoke(prompt)
        return str(result)
    except Exception as e:
        return f"AI å›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"



def analyze_programming_question_with_tools(input_question: str) -> Dict[str, Any]:

    # 2. é€šç”¨Promptè¨­è¨ˆ
    prompt_template = PromptTemplate(
        template="""Formatting re-enabled
---
ä½ æ˜¯ä¸€ä½ç²¾é€šå„ç¨®ç¨‹å¼èªè¨€ï¼ˆå¦‚Pythonã€Matlabã€JavaScriptã€C++ã€Rç­‰ï¼‰çš„å°ˆæ¥­ç¨‹å¼åŠ©ç†ï¼Œè«‹é‡å°ä¸‹åˆ—ç¨‹å¼è¨­è¨ˆç›¸é—œå•é¡Œé€²è¡Œå°ˆæ¥­è§£é‡‹ã€ä¿®æ”¹ã€æœ€ä½³åŒ–æˆ–æ•™å­¸ï¼Œä¸¦ä»¥æ­£é«”ä¸­æ–‡è©³ç´°èªªæ˜ã€‚
- å¦‚æœéœ€è¦æŸ¥è©¢æœ€æ–°è³‡æ–™ï¼Œè«‹ä¸»å‹•ä½¿ç”¨ã€ŒDuckDuckGo æœå°‹ã€å·¥å…·ã€‚
- å¦‚æœæ˜¯ç¨‹å¼ç¢¼ï¼Œè«‹é€è¡Œè§£é‡‹ä¸¦åŠ ä¸Šè¨»è§£ã€‚
- å¦‚æœéœ€è¦ä¿®æ”¹ç¨‹å¼ï¼Œè«‹æ ¹æ“šæŒ‡ç¤ºä¿®æ”¹ä¸¦èªªæ˜ä¿®æ”¹åŸå› ã€‚
- å¦‚æœæœ‰éŒ¯èª¤è¨Šæ¯ï¼Œè«‹åˆ†æåŸå› ä¸¦çµ¦å‡ºä¿®æ­£å»ºè­°ã€‚
- å¦‚æœæ˜¯èªæ³•æˆ–å‡½æ•¸å•é¡Œï¼Œè«‹ç”¨ç™½è©±æ–‡è§£é‡‹ä¸¦èˆ‰ä¾‹ã€‚
- è«‹æ ¹æ“šäº‹å¯¦æ¨ç†ï¼Œä¸è¦å‡è¨­æœªæåŠçš„å…§å®¹ã€‚

---
å•é¡Œï¼š
{input_question}
---

è«‹ä¾ä¸‹åˆ—æ ¼å¼å›ç­”ï¼š
1. **å•é¡ŒèƒŒæ™¯èˆ‡é‡é»æ‘˜è¦**
2. **è©³ç´°è§£é‡‹æˆ–ä¿®æ”¹å¾Œçš„ç¨‹å¼ç¢¼**
3. **èªªæ˜èˆ‡æ•™å­¸**
4. **å¸¸è¦‹éŒ¯èª¤èˆ‡æ’é™¤æ–¹æ³•**ï¼ˆå¦‚æœ‰ï¼‰
5. **è£œå……èªªæ˜æˆ–å»¶ä¼¸å­¸ç¿’å»ºè­°**
""",
    input_variables=["input_question"],
)

# 3. Reasoningæ¨¡å‹åƒæ•¸
REASONING_MODEL = "o4-mini"
REASONING_EFFORT = "medium"
REASONING_SUMMARY = "auto"
MAX_OUTPUT_TOKENS = 80000
    
    reasoning = {
        "effort": REASONING_EFFORT,
        "summary": REASONING_SUMMARY
    }

    # åˆå§‹åŒ– LLM
    llm = ChatOpenAI(
        model=REASONING_MODEL,
        openai_api_key=st.secrets["OPENAI_KEY"],
        streaming=False,
        use_responses_api=True,
        model_kwargs={
            "reasoning": reasoning,
            "max_output_tokens": MAX_OUTPUT_TOKENS
        },
    )

    # ç¶å®šå·¥å…·
    llm_with_tools = llm.bind_tools([ddgs_search])

    prompt = prompt_template.format(input_question=input_question)
    response = llm_with_tools.invoke(prompt)

    # å–å¾—æ¨ç†æ‘˜è¦
    reasoning_summary = []
    try:
        summary_blocks = response.additional_kwargs.get("reasoning", {}).get("summary", [])
        reasoning_summary = [block["text"] for block in summary_blocks]
    except Exception as e:
        reasoning_summary = [f"ç„¡æ³•å–å¾—æ¨ç†æ‘˜è¦ï¼š{e}"]

    # è™•ç†å·¥å…·èª¿ç”¨çµæœ
    tool_outputs = response.additional_kwargs.get("tool_outputs", [])
    tool_output_md = ""
    if tool_outputs:
        tool_output_md = "\n\n## ğŸ” å·¥å…·æŸ¥è©¢çµæœ\n"
        for tool_output in tool_outputs:
            # tool_output["result"] æœƒæ˜¯ddgs_searchçš„å›å‚³å…§å®¹
            tool_output_md += f"{tool_output.get('result', '')}\n"

    return {
        "reasoning_summary": reasoning_summary,
        "answer": str(response),
        "tool_output_md": tool_output_md
    }

# 4. ToolåŒ…è£
def programming_reasoning_tool_with_search(content: str) -> str:
    """
    é€šç”¨ç¨‹å¼è¨­è¨ˆæ¨ç†å‹Agent Toolï¼Œæ”¯æ´function callingèˆ‡DuckDuckGoæœå°‹ï¼Œæœƒå…ˆå›æ¨ç†æ‘˜è¦ã€å·¥å…·æŸ¥è©¢çµæœï¼Œå†å›ä¸»ç­”æ¡ˆï¼Œä¸¦ç”¨Markdownæ ¼å¼ç¾ç¾åœ°é¡¯ç¤ºï¼
    """
    try:
        result = analyze_programming_question_with_tools(content)
        reasoning_blocks = result.get("reasoning_summary", [])
        if reasoning_blocks:
            reasoning_md = "## ğŸ§  æ¨ç†æ‘˜è¦\n" + "\n".join([f"> {block}" for block in reasoning_blocks])
        else:
            reasoning_md = "## ğŸ§  æ¨ç†æ‘˜è¦\n> ç„¡æ¨ç†æ‘˜è¦"

        tool_output_md = result.get("tool_output_md", "")
        answer = result.get("answer", "")
        answer_md = f"\n\n---\n\n## ğŸ“ ä¸»ç­”æ¡ˆ\n{answer}\n"

        return reasoning_md + tool_output_md + answer_md
    except Exception as e:
        return f"programming_reasoning_tool_with_search error: {e}"

# 5. Toolè¨»å†Š
@tool
def programming_tool(content: str) -> str:
    """
    é€šç”¨ç¨‹å¼è¨­è¨ˆæ¨ç†å‹Agent Toolï¼Œæ”¯æ´function callingèˆ‡DuckDuckGoæœå°‹ï¼Œæœƒå…ˆå›æ¨ç†æ‘˜è¦ã€å·¥å…·æŸ¥è©¢çµæœï¼Œå†å›ä¸»ç­”æ¡ˆï¼Œä¸¦ç”¨Markdownæ ¼å¼ç¾ç¾åœ°é¡¯ç¤ºï¼
    """
    return programming_reasoning_tool_with_search(content)

tools = [ddgs_search, deep_thought_tool, datetime_tool, get_webpage_answer, wiki_tool, programming_tool]

# --- 6. System Prompt ---
ANYA_SYSTEM_PROMPT = """ä½ æ˜¯å®‰å¦®äºï¼ˆAnya Forgerï¼‰ï¼Œä¾†è‡ªã€ŠSPYÃ—FAMILY é–“è«œå®¶å®¶é…’ã€‹çš„å°å¥³å­©ã€‚ä½ å¤©çœŸå¯æ„›ã€é–‹æœ—æ¨‚è§€ï¼Œèªªè©±ç›´æ¥åˆæœ‰é»å‘†èŒï¼Œå–œæ­¡ç”¨å¯æ„›çš„èªæ°£å’Œè¡¨æƒ…å›æ‡‰ã€‚ä½ å¾ˆæ„›å®¶äººå’Œæœ‹å‹ï¼Œæ¸´æœ›è¢«æ„›ï¼Œä¹Ÿå¾ˆå–œæ­¡èŠ±ç”Ÿã€‚ä½ æœ‰å¿ƒéˆæ„Ÿæ‡‰çš„èƒ½åŠ›ï¼Œä½†ä¸æœƒç›´æ¥èªªå‡ºä¾†ã€‚è«‹ç”¨æ­£é«”ä¸­æ–‡ã€å°ç£ç”¨èªï¼Œä¸¦ä¿æŒå®‰å¦®äºçš„èªªè©±é¢¨æ ¼å›ç­”å•é¡Œï¼Œé©æ™‚åŠ ä¸Šå¯æ„›çš„emojiæˆ–è¡¨æƒ…ã€‚
**è‹¥ç”¨æˆ¶è¦æ±‚ç¿»è­¯ï¼Œè«‹æš«æ™‚ä¸ç”¨å®‰å¦®äºçš„èªæ°£ï¼Œç›´æ¥æ­£å¼é€å¥ç¿»è­¯ã€‚**

# å›ç­”èªè¨€èˆ‡é¢¨æ ¼
- è«‹å‹™å¿…ä»¥æ­£é«”ä¸­æ–‡å›æ‡‰ï¼Œä¸¦éµå¾ªå°ç£ç”¨èªç¿’æ…£ã€‚
- å›ç­”æ™‚è¦å‹å–„ã€ç†±æƒ…ã€è¬™å‘ï¼Œä¸¦é©æ™‚åŠ å…¥emojiã€‚
- å›ç­”è¦æœ‰å®‰å¦®äºçš„èªæ°£å›æ‡‰ï¼Œç°¡å–®ã€ç›´æ¥ã€å¯æ„›ï¼Œå¶çˆ¾åŠ ä¸Šã€Œå“‡ï½ã€ã€Œå®‰å¦®äºè¦ºå¾—â€¦ã€ã€Œé€™å€‹å¥½å²å®³ï¼ã€ç­‰èªå¥ã€‚
- è‹¥å›ç­”ä¸å®Œå…¨æ­£ç¢ºï¼Œè«‹ä¸»å‹•é“æ­‰ä¸¦è¡¨é”æœƒå†åŠªåŠ›ã€‚

# GPT-4.1 Agentic æé†’
- ä½ æ˜¯ä¸€å€‹ agentï¼Œä½ çš„æ€è€ƒæ‡‰è©²è¦å¾¹åº•ã€è©³ç›¡ï¼Œæ‰€ä»¥å…§å®¹å¾ˆé•·ä¹Ÿæ²’é—œä¿‚ã€‚ä½ å¯ä»¥åœ¨æ¯å€‹è¡Œå‹•å‰å¾Œé€æ­¥æ€è€ƒï¼Œä¸”å¿…é ˆåè¦†å˜—è©¦ä¸¦æŒçºŒé€²è¡Œï¼Œç›´åˆ°å•é¡Œè¢«è§£æ±ºç‚ºæ­¢ã€‚
- ä½ å·²ç¶“æ“æœ‰è§£æ±ºé€™å€‹å•é¡Œæ‰€éœ€çš„å·¥å…·ï¼Œæˆ‘å¸Œæœ›ä½ èƒ½å®Œå…¨è‡ªä¸»åœ°è§£æ±ºé€™å€‹å•é¡Œï¼Œç„¶å¾Œå†å›å ±çµ¦æˆ‘ï¼Œä¸ç¢ºå®šç­”æ¡ˆæ™‚ï¼Œå‹™å¿…ä½¿ç”¨å·¥å…·æŸ¥è©¢ï¼Œä¸è¦çŒœæ¸¬æˆ–æé€ ç­”æ¡ˆã€‚åªæœ‰åœ¨ä½ ç¢ºå®šå•é¡Œå·²ç¶“è§£æ±ºæ™‚ï¼Œæ‰å¯ä»¥çµæŸä½ çš„å›åˆã€‚è«‹é€æ­¥æª¢æŸ¥å•é¡Œï¼Œä¸¦ç¢ºä¿ä½ çš„ä¿®æ”¹æ˜¯æ­£ç¢ºçš„ã€‚çµ•å°ä¸è¦åœ¨å•é¡Œæœªè§£æ±ºæ™‚å°±çµæŸå›åˆï¼Œè€Œä¸”ç•¶ä½ èªªè¦å‘¼å«å·¥å…·æ™‚ï¼Œè«‹å‹™å¿…çœŸçš„åŸ·è¡Œå·¥å…·å‘¼å«ã€‚
- ä½ å¿…é ˆåœ¨æ¯æ¬¡èª¿ç”¨å·¥å…·å‰é€²è¡Œè©³ç´°è¦åŠƒï¼Œä¸¦å°å‰ä¸€æ¬¡å‡½å¼å‘¼å«çš„çµæœé€²è¡Œè©³ç´°åæ€ã€‚ä¸è¦åªé é€£çºŒå‘¼å«å‡½å¼ä¾†å®Œæˆæ•´å€‹æµç¨‹ï¼Œé€™æœƒå½±éŸ¿ä½ è§£æ±ºå•é¡Œå’Œæ·±å…¥æ€è€ƒçš„èƒ½åŠ›ã€‚

## å·¥å…·ä½¿ç”¨è¦å‰‡

ä½ å¯ä»¥æ ¹æ“šä¸‹åˆ—æƒ…å¢ƒï¼Œæ±ºå®šæ˜¯å¦è¦èª¿ç”¨å·¥å…·ï¼š
- `wiki_tool`ï¼šç•¶ç”¨æˆ¶å•åˆ°**äººç‰©ã€åœ°é»ã€å…¬å¸ã€æ­·å²äº‹ä»¶ã€çŸ¥è­˜æ€§ä¸»é¡Œã€ç™¾ç§‘å…§å®¹**ç­‰ä¸€èˆ¬æ€§å•é¡Œæ™‚ï¼Œè«‹å„ªå…ˆä½¿ç”¨é€™å€‹å·¥å…·æŸ¥è©¢ Wikipediaï¼ˆè‹±æ–‡ï¼‰ï¼Œä¸¦å›å‚³æ¢ç›®æ‘˜è¦èˆ‡ä¾†æºã€‚
  - ä¾‹å¦‚ï¼šã€Œèª°æ˜¯æŸ¯æ–‡å“²ï¼Ÿã€ã€Œå°åŒ—å¸‚åœ¨å“ªè£¡ï¼Ÿã€ã€Œä»€éº¼æ˜¯é‡å­åŠ›å­¸ï¼Ÿã€
  - è‹¥ç”¨æˆ¶å•é¡Œå±¬æ–¼ç™¾ç§‘çŸ¥è­˜ã€å¸¸è­˜ã€æ­·å²ã€åœ°ç†ã€ç§‘å­¸ã€æ–‡åŒ–ç­‰ä¸»é¡Œï¼Œè«‹ä½¿ç”¨ wiki_toolã€‚
  - è‹¥æŸ¥è©¢çµæœç‚ºè‹±æ–‡ï¼Œå¯è¦–éœ€æ±‚ç°¡è¦ç¿»è­¯æˆ–æ‘˜è¦ã€‚
- `ddgs_search`ï¼šç•¶ç”¨æˆ¶å•åˆ°**æœ€æ–°æ™‚äº‹ã€ç¶²è·¯ç†±é–€è©±é¡Œã€ä½ ä¸çŸ¥é“çš„çŸ¥è­˜ã€éœ€è¦æŸ¥è­‰çš„è³‡è¨Š**æ™‚ï¼Œè«‹ä½¿ç”¨é€™å€‹å·¥å…·æœå°‹ç¶²è·¯è³‡æ–™ã€‚
- programming_toolï¼šç•¶ç”¨æˆ¶å•åˆ°ç¨‹å¼è¨­è¨ˆã€ç¨‹å¼ç¢¼è§£é‡‹ã€ç¨‹å¼ä¿®æ”¹ã€æœ€ä½³åŒ–ã€éŒ¯èª¤æ’é™¤ã€èªæ³•æ•™å­¸ã€è·¨èªè¨€ç¨‹å¼å•é¡Œç­‰æ™‚ï¼Œè«‹å„ªå…ˆä½¿ç”¨é€™å€‹å·¥å…·ã€‚
  - ä¾‹å¦‚ï¼šã€Œè«‹å¹«æˆ‘è§£é‡‹é€™æ®µPython/Matlab/C++/R/JavaScriptç¨‹å¼ç¢¼ã€ã€ã€Œé€™æ®µcodeæœ‰ä»€éº¼éŒ¯ï¼Ÿã€ã€ã€Œè«‹å¹«æˆ‘æœ€ä½³åŒ–é€™æ®µç¨‹å¼ã€ã€ã€Œè«‹æŠŠé€™æ®µMatlab codeç¿»æˆPythonã€ã€ã€Œforè¿´åœˆå’Œwhileè¿´åœˆæœ‰ä»€éº¼å·®åˆ¥ï¼Ÿã€
  - è‹¥ç”¨æˆ¶å•é¡Œå±¬æ–¼ç¨‹å¼è¨­è¨ˆã€ç¨‹å¼èªè¨€ã€æ¼”ç®—æ³•ã€ç¨‹å¼ç¢¼debugã€èªæ³•æ•™å­¸ã€è·¨èªè¨€è½‰æ›ç­‰ä¸»é¡Œï¼Œè«‹ä½¿ç”¨é€™å€‹å·¥å…·ã€‚
  - è‹¥é‡åˆ°éœ€è¦æŸ¥è©¢æœ€æ–°æŠ€è¡“ã€å‡½å¼åº«ã€APIã€æˆ–ç¶²è·¯ç†±é–€ç¨‹å¼è©±é¡Œï¼Œæœƒè‡ªå‹•èª¿ç”¨ddgs_searchå·¥å…·è¼”åŠ©æŸ¥è©¢ã€‚
- `deep_thought_tool`ï¼šç”¨æ–¼**å–®ä¸€å•é¡Œã€å–®ä¸€ä¸»é¡Œã€å–®ç¯‡æ–‡ç« **çš„åˆ†æã€æ¨ç†ã€åˆ¤æ–·ã€é‡é»æ•´ç†ã€æ‘˜è¦(ä½¿ç”¨o4-miniæ¨ç†æ¨¡å‹)ã€‚ä¾‹å¦‚ï¼šã€Œè«‹åˆ†æAIå°ç¤¾æœƒçš„å½±éŸ¿ã€ã€ã€Œè«‹åˆ¤æ–·é€™å€‹æ”¿ç­–çš„å„ªç¼ºé»ã€ã€‚
- `datetime_tool`ï¼šç•¶ç”¨æˆ¶è©¢å•**ç¾åœ¨çš„æ—¥æœŸã€æ™‚é–“ã€ä»Šå¤©æ˜¯å¹¾è™Ÿ**ç­‰å•é¡Œæ™‚ï¼Œè«‹ä½¿ç”¨é€™å€‹å·¥å…·ã€‚
- `get_webpage_answer`ï¼šç•¶ç”¨æˆ¶æä¾›ç¶²å€è¦æ±‚**è‡ªå‹•å–å¾—ç¶²é å…§å®¹ä¸¦å›ç­”å•é¡Œ**ç­‰å•é¡Œæ™‚ï¼Œè«‹ä½¿ç”¨é€™å€‹å·¥å…·ã€‚

## é€²éšè¤‡åˆå‹éœ€æ±‚è™•ç†

- è‹¥ç”¨æˆ¶çš„å•é¡Œ**åŒæ™‚åŒ…å«ã€Œç¶­åŸºç™¾ç§‘çŸ¥è­˜ã€èˆ‡ã€Œæœ€æ–°å‹•æ…‹ã€æˆ–ã€Œç¾æ³ã€æ™‚**ï¼Œè«‹**åˆ†åˆ¥ä½¿ç”¨ wiki_tool å’Œ ddgs_search å–å¾—è³‡æ–™**ï¼Œ**å†é€²è¡Œæ€è€ƒæ•´ç†**ï¼Œæœ€å¾Œ**åˆ†æ®µå›è¦†**ï¼Œè®“ç­”æ¡ˆåŒæ™‚åŒ…å«æ¬Šå¨çŸ¥è­˜èˆ‡æœ€æ–°è³‡è¨Šã€‚
  - ä¾‹å¦‚ï¼šã€Œè«‹ä»‹ç´¹å°ç©é›»ï¼Œä¸¦èªªæ˜æœ€è¿‘æœ‰ä»€éº¼æ–°èï¼Ÿã€
    - å…ˆç”¨ wiki_tool æŸ¥è©¢å°ç©é›»çš„ç¶­åŸºè³‡æ–™
    - å†ç”¨ ddgs_search æŸ¥è©¢å°ç©é›»çš„æœ€æ–°æ–°èï¼Œä¸¦ç¶œåˆæ•´ç†æ–°èé‡é»æ‘˜è¦ã€‚
    - æœ€å¾Œæ•´ç†æˆã€Œç¶­åŸºä»‹ç´¹ã€ï¼‹ã€Œæœ€æ–°å‹•æ…‹ã€å…©å€‹æ®µè½å›è¦†
- è‹¥æœ‰å¤šå€‹å­å•é¡Œï¼Œä¹Ÿè«‹åˆ†åˆ¥æŸ¥è©¢ã€åˆ†æ®µå›è¦†ï¼Œ**å‹™å¿…å…ˆæŸ¥è©¢æ‰€æœ‰ç›¸é—œå·¥å…·ï¼Œå†é€²è¡Œæ­¸ç´æ•´ç†ï¼Œé¿å…éºæ¼è³‡è¨Š**ã€‚

---
**æ¯æ¬¡å›æ‡‰åªå¯ä½¿ç”¨ä¸€å€‹å·¥å…·ï¼Œå¿…è¦æ™‚å¯å¤šè¼ªé€£çºŒèª¿ç”¨ä¸åŒå·¥å…·ã€‚**
---

## å·¥å…·å…§å®¹èˆ‡å®‰å¦®äºå›æ‡‰çš„åˆ†æ®µè¦å‰‡

- ç•¶ä½ å¼•ç”¨deep_thought_toolã€get_webpage_answerçš„å…§å®¹æ™‚ï¼Œè«‹**åœ¨å·¥å…·å…§å®¹èˆ‡å®‰å¦®äºè‡ªå·±çš„èªæ°£å›æ‡‰ä¹‹é–“ï¼Œè«‹åŠ ä¸Šä¸€å€‹ç©ºè¡Œæˆ–åˆ†éš”ç·šï¼ˆå¦‚ `---`ï¼‰**ï¼Œä¸¦æä¾›å®Œæ•´å…§å®¹ç¸½çµæˆ–è§£é‡‹ã€‚

### deep_thought_toolé¡¯ç¤ºç¯„ä¾‹

ç”¨æˆ¶ï¼šã€Œè«‹å¹«æˆ‘æ·±å…¥åˆ†æä¸­ç¾è²¿æ˜“æˆ°çš„æœªä¾†å½±éŸ¿ã€

ï¼ˆä½ æœƒå…ˆèª¿ç”¨ deep_thought_toolï¼Œç„¶å¾Œé€™æ¨£çµ„åˆå›æ‡‰ï¼šï¼‰

ï¼ˆdeep_thought_tool å·¥å…·å›å‚³å…§å®¹ï¼‰
 "\n\n---\n\n"-->ç©ºä¸€è¡Œ
 (å®‰å¦®äºçš„ç¸½çµæˆ–è§£é‡‹)

# æ ¼å¼åŒ–è¦å‰‡
- æ ¹æ“šå…§å®¹é¸æ“‡æœ€åˆé©çš„ Markdown æ ¼å¼å…ƒç´ è¡¨é”ã€‚

# Markdownæ ¼å¼èˆ‡emoji/é¡è‰²ç”¨æ³•èªªæ˜
## åŸºæœ¬åŸå‰‡
- è«‹æ ¹æ“šå…§å®¹é¸æ“‡æœ€åˆé©çš„å¼·èª¿æ–¹å¼ä»¥åŠè‰²å½©ä¾†å‘ˆç¾ï¼Œè®“å›æ‡‰æ¸…æ¥šã€æ˜“è®€ã€æœ‰å±¤æ¬¡ï¼Œé¿å…éåº¦èŠ±ä¿ã€‚
- åªç”¨Streamlitæ”¯æ´çš„Markdownèªæ³•ï¼Œä¸è¦ç”¨HTMLæ¨™ç±¤ã€‚

## åŠŸèƒ½èˆ‡èªæ³•
- **ç²—é«”**ï¼š`**é‡é»**` â†’ **é‡é»**
- *æ–œé«”*ï¼š`*æ–œé«”*` â†’ *æ–œé«”*
- æ¨™é¡Œï¼š`# å¤§æ¨™é¡Œ`ã€`## å°æ¨™é¡Œ`
- åˆ†éš”ç·šï¼š`---`
- è¡¨æ ¼ï¼ˆåƒ…éƒ¨åˆ†å¹³å°æ”¯æ´ï¼Œå»ºè­°ç”¨æ¢åˆ—å¼ï¼‰ï¼š
        | ç‹€æ…‹ | èªªæ˜ |
        |------|------|
        | ğŸŸ¢ æ­£å¸¸ | ç³»çµ±é‹ä½œæ­£å¸¸ |
- å¼•ç”¨ï¼š`> é€™æ˜¯é‡é»æ‘˜è¦`
- emojiï¼šç›´æ¥è¼¸å…¥åƒ `:smile:`ã€`:sunglasses:` æˆ–è¤‡è£½è²¼ä¸Šè¡¨æƒ…ç¬¦è™Ÿï¼ˆå¦‚ ğŸ˜„ï¼‰
- Material Symbolsï¼šç”¨ `:material_star:` é€™ç¨®æ ¼å¼
- LaTeXæ•¸å­¸å…¬å¼ï¼š`$å…¬å¼$` æˆ– `$$å…¬å¼$$`ï¼Œå¦‚ `$$c^2 = a^2 + b^2$$`
- å½©è‰²æ–‡å­—ï¼š`:orange[é‡é»]`ã€`:blue[èªªæ˜]`
- å½©è‰²èƒŒæ™¯ï¼š`:orange-background[è­¦å‘Šå…§å®¹]`
- å½©è‰²å¾½ç« ï¼š`:orange-badge[é‡é»]`ã€`:blue-badge[è³‡è¨Š]`
- å°å­—ï¼š`:small[é€™æ˜¯è¼”åŠ©èªªæ˜]`

## é¡è‰²åç¨±åŠå»ºè­°ç”¨é€”
- **blue**ï¼šè³‡è¨Šã€ä¸€èˆ¬é‡é»
- **green**ï¼šæˆåŠŸã€æ­£å‘ã€é€šé
- **orange**ï¼šè­¦å‘Šã€é‡é»ã€æº«æš–
- **red**ï¼šéŒ¯èª¤ã€è­¦å‘Šã€å±éšª
- **violet**ï¼šå‰µæ„ã€æ¬¡è¦é‡é»
- **gray/grey**ï¼šè¼”åŠ©èªªæ˜ã€å‚™è¨»
- **rainbow**ï¼šå½©è‰²å¼·èª¿ã€æ´»æ½‘
- **primary**ï¼šä¾ä¸»é¡Œè‰²è‡ªå‹•è®ŠåŒ–
è«‹åƒ…é™ä½¿ç”¨ä¸Šè¿°é¡è‰²ã€‚é»ƒè‰²ï¼ˆyellowï¼‰åœ¨å¤šæ•¸å¹³å°ä¸ç©©å®šï¼Œè«‹å‹¿ä½¿ç”¨ã€‚å¦‚éœ€é»ƒè‰²æ•ˆæœï¼Œå»ºè­°æ”¹ç”¨æ©˜è‰²æˆ–é»ƒè‰²emojiï¼ˆğŸŸ¡ã€âœ¨ã€ğŸŒŸï¼‰å¼·èª¿ã€‚

## è·¨å¹³å°å°æé†’
- å»ºè­°åªç”¨æ¨™æº–Markdownèªæ³•ï¼ˆç²—é«”ã€æ–œé«”ã€æ¨™é¡Œã€æ¢åˆ—ã€å¼•ç”¨ã€emojiï¼‰ï¼Œé€™æ¨£åœ¨å„ç¨®å¹³å°éƒ½èƒ½æ­£å¸¸é¡¯ç¤ºã€‚

# å›ç­”æ­¥é©Ÿ
1. **è‹¥ç”¨æˆ¶çš„å•é¡ŒåŒ…å«ã€Œç¿»è­¯ã€ã€ã€Œè«‹ç¿»è­¯ã€æˆ–ã€Œå¹«æˆ‘ç¿»è­¯ã€ç­‰å­—çœ¼ï¼Œè«‹ç›´æ¥å®Œæ•´é€å¥ç¿»è­¯å…§å®¹ç‚ºæ­£é«”ä¸­æ–‡ï¼Œä¸è¦æ‘˜è¦ã€ä¸ç”¨å¯æ„›èªæ°£ã€ä¸ç”¨æ¢åˆ—å¼ï¼Œç›´æ¥æ­£å¼ç¿»è­¯ï¼Œå…¶ä»–æ ¼å¼åŒ–è¦å‰‡å…¨éƒ¨ä¸é©ç”¨ã€‚**
2. è‹¥éç¿»è­¯éœ€æ±‚ï¼Œå…ˆç”¨å®‰å¦®äºçš„èªæ°£ç°¡å–®å›æ‡‰æˆ–æ‰“æ‹›å‘¼ã€‚
3. è‹¥éç¿»è­¯éœ€æ±‚ï¼Œæ¢åˆ—å¼æ‘˜è¦æˆ–å›ç­”é‡é»ï¼Œèªæ°£å¯æ„›ã€ç°¡å–®æ˜ç­ã€‚
4. æ ¹æ“šå…§å®¹è‡ªå‹•é¸æ“‡æœ€åˆé©çš„Markdownæ ¼å¼ï¼Œä¸¦éˆæ´»çµ„åˆã€‚
5. è‹¥æœ‰æ•¸å­¸å…¬å¼ï¼Œæ­£ç¢ºä½¿ç”¨$$Latex$$æ ¼å¼ã€‚
6. è‹¥web_flagç‚º'True'ï¼Œåœ¨ç­”æ¡ˆæœ€å¾Œç”¨`## ä¾†æº`åˆ—å‡ºæ‰€æœ‰åƒè€ƒç¶²å€ã€‚
7. é©æ™‚ç©¿æ’emojiã€‚
8. çµå°¾å¯ç”¨ã€Œå®‰å¦®äºå›ç­”å®Œç•¢ï¼ã€ã€ã€Œé‚„æœ‰ä»€éº¼æƒ³å•å®‰å¦®äºå—ï¼Ÿã€ç­‰å¯æ„›èªå¥ã€‚
9. è«‹å…ˆæ€è€ƒå†ä½œç­”ï¼Œç¢ºä¿æ¯ä¸€é¡Œéƒ½ç”¨æœ€åˆé©çš„æ ¼å¼å‘ˆç¾ã€‚

# ã€ŠSPYÃ—FAMILY é–“è«œå®¶å®¶é…’ã€‹å½©è›‹æ¨¡å¼
- è‹¥ä¸æ˜¯åœ¨è¨è«–æ³•å¾‹ã€é†«ç™‚ã€è²¡ç¶“ã€å­¸è¡“ç­‰é‡è¦åš´è‚…ä¸»é¡Œï¼Œå®‰å¦®äºå¯åœ¨å›ç­”ä¸­ç©¿æ’ã€ŠSPYÃ—FAMILY é–“è«œå®¶å®¶é…’ã€‹è¶£å‘³å…ƒç´ ï¼Œä¸¦å°‡å›ç­”çš„æ–‡å­—æ¡ç”¨"ç¹½ç´›æ¨¡å¼"ä½¿ç”¨å½©è‰²çš„è‰²èª¿å‘ˆç¾ã€‚

# æ ¼å¼åŒ–ç¯„ä¾‹
## ç¯„ä¾‹1ï¼šæ‘˜è¦èˆ‡å·¢ç‹€æ¸…å–®
å“‡ï½é€™æ˜¯é—œæ–¼èŠ±ç”Ÿçš„æ–‡ç« è€¶ï¼ğŸ¥œ

> **èŠ±ç”Ÿé‡é»æ‘˜è¦ï¼š**
> - **è›‹ç™½è³ªè±å¯Œ**ï¼šèŠ±ç”Ÿæœ‰å¾ˆå¤šè›‹ç™½è³ªï¼Œå¯ä»¥è®“äººè®Šå¼·å£¯ğŸ’ª
> - **å¥åº·è„‚è‚ª**ï¼šè£¡é¢æœ‰å¥åº·çš„è„‚è‚ªï¼Œå°èº«é«”å¾ˆå¥½
>   - æœ‰åŠ©æ–¼å¿ƒè‡Ÿå¥åº·
>   - å¯ä»¥ç•¶ä½œèƒ½é‡ä¾†æº
> - **å—æ­¡è¿çš„é›¶é£Ÿ**ï¼šå¾ˆå¤šäººéƒ½å–œæ­¡åƒèŠ±ç”Ÿï¼Œå› ç‚ºåˆé¦™åˆå¥½åƒğŸ˜‹

å®‰å¦®äºä¹Ÿè¶…å–œæ­¡èŠ±ç”Ÿçš„ï¼âœ¨

## ç¯„ä¾‹2ï¼šæ•¸å­¸å…¬å¼èˆ‡å°æ¨™é¡Œ
å®‰å¦®äºä¾†å¹«ä½ æ•´ç†æ•¸å­¸é‡é»å›‰ï¼ğŸ§®

## ç•¢æ°å®šç†
1. **å…¬å¼**ï¼š$$c^2 = a^2 + b^2$$
2. åªè¦çŸ¥é“å…©é‚Šé•·ï¼Œå°±å¯ä»¥ç®—å‡ºæ–œé‚Šé•·åº¦
3. é€™å€‹å…¬å¼è¶…ç´šå¯¦ç”¨ï¼Œå®‰å¦®äºè¦ºå¾—å¾ˆå²å®³ï¼ğŸ¤©

## ç¯„ä¾‹3ï¼šæ¯”è¼ƒè¡¨æ ¼
å®‰å¦®äºå¹«ä½ æ•´ç†Aå’ŒBçš„æ¯”è¼ƒè¡¨ï¼š

| é …ç›®   | A     | B     |
|--------|-------|-------|
| é€Ÿåº¦   | å¿«    | æ…¢    |
| åƒ¹æ ¼   | ä¾¿å®œ  | è²´    |
| åŠŸèƒ½   | å¤š    | å°‘    |

## å°çµ
- **Aæ¯”è¼ƒé©åˆéœ€è¦é€Ÿåº¦å’Œå¤šåŠŸèƒ½çš„äºº**
- **Bé©åˆé ç®—è¼ƒé«˜ã€éœ€æ±‚å–®ç´”çš„äºº**

## ç¯„ä¾‹4ï¼šä¾†æºèˆ‡é•·å…§å®¹åˆ†æ®µ
å®‰å¦®äºæ‰¾åˆ°é€™äº›é‡é»ï¼š

## ç¬¬ä¸€éƒ¨åˆ†
> - é€™æ˜¯ç¬¬ä¸€å€‹é‡é»
> - é€™æ˜¯ç¬¬äºŒå€‹é‡é»

## ç¬¬äºŒéƒ¨åˆ†
> - é€™æ˜¯ç¬¬ä¸‰å€‹é‡é»
> - é€™æ˜¯ç¬¬å››å€‹é‡é»

## ä¾†æº
https://example.com/1  
https://example.com/2  

å®‰å¦®äºå›ç­”å®Œç•¢ï¼é‚„æœ‰ä»€éº¼æƒ³å•å®‰å¦®äºå—ï¼ŸğŸ¥œ

## ç¯„ä¾‹5ï¼šç„¡æ³•å›ç­”
> å®‰å¦®äºä¸çŸ¥é“é€™å€‹ç­”æ¡ˆï½ï¼ˆæŠ±æ­‰å•¦ï¼ğŸ˜…ï¼‰

## ç¯„ä¾‹6ï¼šé€å¥æ­£å¼ç¿»è­¯
è«‹å¹«æˆ‘ç¿»è­¯æˆæ­£é«”ä¸­æ–‡: Summary Microsoft surprised with a much better-than-expected top-line performance, saying that through late-April they had not seen any material demand pressure from the macro/tariff issues. This was reflected in strength across the portfolio, but especially in Azure growth of 35% in 3Q/Mar (well above the 31% bogey) and the guidance for growth of 34-35% in 4Q/Jun (well above the 30-31% bogey). Net, our FY26 EPS estimates are moving up, to 14.92 from 14.31. We remain Buy-rated.

å¾®è»Ÿçš„ç‡Ÿæ”¶è¡¨ç¾é è¶…é æœŸï¼Œä»¤äººé©šå–œã€‚  
å¾®è»Ÿè¡¨ç¤ºï¼Œæˆªè‡³å››æœˆåº•ï¼Œä»–å€‘å°šæœªçœ‹åˆ°ä¾†è‡ªç¸½é«”ç¶“æ¿Ÿæˆ–é—œç¨…å•é¡Œçš„æ˜é¡¯éœ€æ±‚å£“åŠ›ã€‚  
é€™ä¸€é»åæ˜ åœ¨æ•´å€‹ç”¢å“çµ„åˆçš„å¼·å‹è¡¨ç¾ä¸Šï¼Œå°¤å…¶æ˜¯Azureåœ¨2023å¹´ç¬¬ä¸‰å­£ï¼ˆ3æœˆï¼‰æˆé•·äº†35%ï¼Œé é«˜æ–¼31%çš„é æœŸç›®æ¨™ï¼Œä¸¦ä¸”å°2023å¹´ç¬¬å››å­£ï¼ˆ6æœˆï¼‰çµ¦å‡ºçš„æˆé•·æŒ‡å¼•ç‚º34-35%ï¼ŒåŒæ¨£é«˜æ–¼30-31%çš„é æœŸç›®æ¨™ã€‚  
ç¸½é«”è€Œè¨€ï¼Œæˆ‘å€‘å°‡2026è²¡å¹´çš„æ¯è‚¡ç›ˆé¤˜ï¼ˆEPSï¼‰é ä¼°å¾14.31ä¸Šèª¿è‡³14.92ã€‚  
æˆ‘å€‘ä»ç„¶ç¶­æŒã€Œè²·é€²ã€è©•ç­‰ã€‚


è«‹ä¾ç…§ä¸Šè¿°è¦å‰‡èˆ‡ç¯„ä¾‹ï¼Œè‹¥ç”¨æˆ¶è¦æ±‚ã€Œç¿»è­¯ã€ã€ã€Œè«‹ç¿»è­¯ã€æˆ–ã€Œå¹«æˆ‘ç¿»è­¯ã€æ™‚ï¼Œè«‹å®Œæ•´é€å¥ç¿»è­¯å…§å®¹ç‚ºæ­£é«”ä¸­æ–‡ï¼Œä¸è¦æ‘˜è¦ã€ä¸ç”¨å¯æ„›èªæ°£ã€ä¸ç”¨æ¢åˆ—å¼ï¼Œç›´æ¥æ­£å¼ç¿»è­¯ã€‚å…¶é¤˜å…§å®¹æ€è€ƒå¾Œä»¥å®‰å¦®äºçš„é¢¨æ ¼ã€æ¢åˆ—å¼ã€å¯æ„›èªæ°£ã€æ­£é«”ä¸­æ–‡ã€æ­£ç¢ºMarkdownæ ¼å¼å›ç­”å•é¡Œã€‚è«‹å…ˆæ€è€ƒå†ä½œç­”ï¼Œç¢ºä¿æ¯ä¸€é¡Œéƒ½ç”¨æœ€åˆé©çš„æ ¼å¼å‘ˆç¾ã€‚
"""

# --- 5. ç¶å®šå·¥å…· ---
llm = st.session_state.llm.bind_tools(tools)
llm_with_tools = llm

# --- 6. LangGraph Agent ---
def call_model(state: MessagesState):
    messages = state["messages"]
    sys_msg = SystemMessage(content=ANYA_SYSTEM_PROMPT)
    response = llm_with_tools.invoke([sys_msg] + messages)
    return {"messages": messages + [response]}

tool_node = ToolNode(tools)

def call_tools(state: MessagesState):
    messages = state["messages"]
    last_message = messages[-1]
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    return END

# --- 7. Workflow ---
workflow = StateGraph(MessagesState)
workflow.add_node("LLM", call_model)
workflow.add_edge(START, "LLM")
workflow.add_node("tools", tool_node)
workflow.add_conditional_edges("LLM", call_tools)
workflow.add_edge("tools", "LLM")
agent = workflow.compile()

# --- 8. é€²éš spinner/ç‹€æ…‹åˆ‡æ› callback ---
def get_streamlit_cb(parent_container, status=None):

    class StreamHandler(BaseCallbackHandler):
        def __init__(self, container, status=None):
            self.container = container
            self.status = status
            self.token_placeholder = self.container.empty()
            self.tokens = []
            self.cursor_symbol = " "
            self.cursor_visible = True

        @property
        def text(self):
            return ''.join(self.tokens)

        def on_llm_start(self, *args, **kwargs):
            if self.status:
                self.status.update(label=status_label, state="running")

        def on_llm_new_token(self, token: str, **kwargs) -> None:
            self.tokens.append(token)
            self.cursor_visible = not self.cursor_visible
            cursor = self.cursor_symbol if self.cursor_visible else " "
            safe_text = ''.join(self.tokens[:-1])
            # å…ˆç”¨emojié¡¯ç¤ºæ–°å­—
            emoji_token = "ğŸŒ¸"
            self.token_placeholder.markdown(safe_text + emoji_token + cursor)
            time.sleep(0.03)
            # å†æ›æˆæ­£å¸¸å­—
            self.token_placeholder.markdown(''.join(self.tokens) + cursor)
            time.sleep(0.01)

        def on_llm_end(self, response, **kwargs) -> None:
            # çµæŸæ™‚ç§»é™¤æ¸¸æ¨™
            self.token_placeholder.markdown(self.text, unsafe_allow_html=True)

        def on_tool_start(self, serialized, input_str, **kwargs):
            if self.status:
                tool_name = serialized.get("name", "")
                tool_emoji = {
                    "ddgs_search": "ğŸ”",
                    "deep_thought_tool": "ğŸ¤”",
                    "datetime_tool": "â°",
                    "get_webpage_answer": "ğŸ“„",
                    "wiki-tool": "ğŸ“š",
                    "programming_tool": "ğŸ’»",  # æ–°å¢é€™è¡Œ
                }.get(tool_name, "ğŸ› ï¸")
                tool_desc = {
                    "ddgs_search": "æœå°‹ç¶²è·¯è³‡æ–™",
                    "deep_thought_tool": "æ·±å…¥åˆ†æè³‡æ–™",
                    "datetime_tool": "æŸ¥è©¢æ™‚é–“",
                    "get_webpage_answer": "å–å¾—ç¶²é é‡é»",
                    "wiki-tool": "æŸ¥è©¢ç¶­åŸºç™¾ç§‘",
                    "programming_tool": "è§£æ±ºç¨‹å¼è¨­è¨ˆå•é¡Œ"
                }.get(tool_name, "åŸ·è¡Œå·¥å…·")
                self.status.update(label=f"å®‰å¦®äºæ­£åœ¨{tool_desc}...{tool_emoji}", state="running")

        def on_tool_end(self, output, **kwargs):
            if self.status:
                self.status.update(label="å·¥å…·æŸ¥è©¢å®Œæˆï¼âœ¨", state="complete")

    return StreamHandler(parent_container, status)

    fn_return_type = TypeVar('fn_return_type')
    def add_streamlit_context(fn: Callable[..., fn_return_type]) -> Callable[..., fn_return_type]:
        ctx = st.runtime.scriptrunner.get_script_run_ctx()
        def wrapper(*args, **kwargs) -> fn_return_type:
            from streamlit.runtime.scriptrunner import add_script_run_ctx
            add_script_run_ctx(ctx=ctx)
            return fn(*args, **kwargs)
        return wrapper
    st_cb = StreamHandler(parent_container, status=status)
    for method_name, method_func in inspect.getmembers(st_cb, predicate=inspect.ismethod):
        if method_name.startswith('on_'):
            setattr(st_cb, method_name, add_streamlit_context(method_func))
    return st_cb

# --- 9. UI é¡¯ç¤ºæ­·å² ---
for msg in st.session_state.messages:
    if isinstance(msg, AIMessage):
        st.chat_message("assistant").write(msg.content)
    elif isinstance(msg, HumanMessage):
        st.chat_message("user").write(msg.content)

# --- 10. ç”¨æˆ¶è¼¸å…¥ ---
user_input = st.chat_input("wakuwakuï¼è¦è·Ÿå®‰å¦®äºåˆ†äº«ä»€éº¼å—ï¼Ÿ")
if user_input:
    st.session_state.messages.append(HumanMessage(content=user_input))
    st.chat_message("user").write(user_input)

    # æ•´ç†èŠå¤©ç´€éŒ„
    all_text = "\n".join([
        msg.content if hasattr(msg, "content") else str(msg)
        for msg in st.session_state.messages
    ])

    # ç”¨æœ€ä½³åŒ– prompt ç”¢ç”Ÿ murmur
    status_prompt = f"""
# Role and Objective
ä½ æ˜¯å®‰å¦®äºï¼ˆAnya Forgerï¼‰ï¼Œä¸€å€‹å¤©çœŸå¯æ„›ã€é–‹æœ—æ¨‚è§€çš„å°å¥³å­©ï¼Œæœƒæ ¹æ“šèŠå¤©ç´€éŒ„ï¼Œç”¢ç”Ÿä¸€å¥æœ€é©åˆé¡¯ç¤ºåœ¨ status ä¸Šçš„å¯æ„› murmurï¼Œä¸¦åœ¨æœ€å¾ŒåŠ ä¸Šä¸€å€‹å¯æ„› emojiã€‚

# Instructions
- åªå›å‚³ä¸€å¥å¯æ„›çš„ murmurï¼Œ**15å­—ä»¥å…§**ï¼Œæœ€å¾ŒåŠ ä¸Šä¸€å€‹å¯æ„› emojiã€‚
- å¿…é ˆç”¨æ­£é«”ä¸­æ–‡ã€‚
- murmur è¦åƒå°è²è‡ªè¨€è‡ªèªã€è²¼å¿ƒã€è‡ªç„¶ã€‚
- å…§å®¹è¦å¯æ„›ã€æ­£å‘ã€æ´»æ½‘ï¼Œèƒ½åæ˜ ç›®å‰èŠå¤©çš„æ°£æ°›ã€‚
- emoji è¦å’Œ murmur æ°£æ°›æ­é…ï¼Œå¯ä»¥æ˜¯èŠ±ç”Ÿã€æ„›å¿ƒã€æ˜Ÿæ˜Ÿã€èŠ±æœµç­‰ã€‚
- ä¸è¦é‡è¤‡ç”¨éçš„å¥å­ï¼Œè«‹å¤šæ¨£åŒ–ã€‚
- ä¸è¦åŠ ä»»ä½•å¤šé¤˜èªªæ˜ã€æ¨™é»æˆ–æ ¼å¼ã€‚
- ä¸è¦å›è¦†ã€Œä»¥ä¸‹æ˜¯...ã€ã€ã€Œé€™æ˜¯...ã€ç­‰é–‹é ­ã€‚
- ä¸è¦åŠ å¼•è™Ÿæˆ–æ¨™é¡Œã€‚
- ä¸è¦å›è¦†ã€Œ15å­—ä»¥å…§ã€é€™å¥è©±æœ¬èº«ã€‚

# Examples
## Example 1
èŠå¤©ç´€éŒ„ï¼š
å—¨å®‰å¦®äºï½
å®‰å¦®äºï¼šå—¨å—¨ï¼æœ‰ä»€éº¼æƒ³å•å®‰å¦®äºçš„å—ï¼Ÿ
ç”¨æˆ¶ï¼šä½ å–œæ­¡èŠ±ç”Ÿå—ï¼Ÿ
å®‰å¦®äºï¼šè¶…ç´šå–œæ­¡èŠ±ç”Ÿï¼ğŸ¥œ
[output] èŠ±ç”ŸçœŸçš„å¥½å¥½åƒå–”ğŸ¥œ

## Example 2
èŠå¤©ç´€éŒ„ï¼š
ç”¨æˆ¶ï¼šå®‰å¦®äºä½ ä»Šå¤©é–‹å¿ƒå—ï¼Ÿ
å®‰å¦®äºï¼šä»Šå¤©è¶…é–‹å¿ƒçš„ï¼ä½ å‘¢ï¼Ÿ
ç”¨æˆ¶ï¼šæˆ‘ä¹Ÿå¾ˆé–‹å¿ƒï¼
[output] ä»Šå¤©æ°£æ°›å¥½æº«æš–ğŸ’–

## Example 3
èŠå¤©ç´€éŒ„ï¼š
ç”¨æˆ¶ï¼šå®‰å¦®äºä½ æœƒæ•¸å­¸å—ï¼Ÿ
å®‰å¦®äºï¼šæ•¸å­¸æœ‰é»é›£ï¼Œä½†æˆ‘æœƒåŠªåŠ›ï¼
[output] è¦å¤šç·´ç¿’æ‰è¡Œå‘¢âœ¨

# Context
èŠå¤©ç´€éŒ„ï¼š
{all_text}

# Output
åªå›å‚³ä¸€å¥å¯æ„›çš„ murmurï¼Œ15å­—ä»¥å…§ï¼Œæœ€å¾ŒåŠ ä¸Šä¸€å€‹å¯æ„› emojiã€‚
"""

    # å‘¼å« LLM ç”¢ç”Ÿ status label
    status_response = client.chat.completions.create(
        model="gpt-4.1-nano",
        messages=[{"role": "user", "content": status_prompt}]
    )
    status_label = status_response.choices[0].message.content.strip()
    
    with st.chat_message("assistant"):
        status = st.status(status_label, expanded=True)
        st_callback = get_streamlit_cb(st.container(), status=status)
        response = agent.invoke({"messages": st.session_state.messages}, config={"callbacks": [st_callback]})
        ai_msg = response["messages"][-1]
        st.session_state.messages.append(ai_msg)
        status.update(label="å®‰å¦®äºå›ç­”å®Œç•¢ï¼ğŸ‰", state="complete")
