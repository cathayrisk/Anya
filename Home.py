import os
import streamlit as st
from datetime import datetime
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from langchain_core.prompts import PromptTemplate
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode, tools_condition
import inspect
from typing import Callable, TypeVar, List, Dict, Any
import time
import re
import requests
from openai import OpenAI

st.set_page_config(
    page_title="Anya",
    layout="wide",
    page_icon="ğŸ¥œ",
    initial_sidebar_state="collapsed"
)

# --- 1. Streamlit session_state åˆå§‹åŒ– ---
if "messages" not in st.session_state:
    st.session_state.messages = [AIMessage(content="å—¨å—¨ï½å®‰å¦®äºä¾†äº†ï¼ğŸ‘‹ æœ‰ä»€éº¼æƒ³å•å®‰å¦®äºçš„å—ï¼Ÿ")]
if "selected_model" not in st.session_state:
    st.session_state.selected_model = "gpt-4.1"
if "current_model" not in st.session_state:
    st.session_state.current_model = None
if "llm" not in st.session_state:
    st.session_state.llm = None

# --- 2. LLM åˆå§‹åŒ– ---
def ensure_llm():
    if (
        st.session_state.llm is None
        or st.session_state.current_model != st.session_state.selected_model
    ):
        st.session_state.llm = ChatOpenAI(
            model=st.session_state.selected_model,
            openai_api_key=st.secrets["OPENAI_KEY"],
            temperature=0.0,
            streaming=True,
        )
        st.session_state.current_model = st.session_state.selected_model

ensure_llm()

# --- 3. å·¥å…·å®šç¾© ---
# === OpenAI åˆå§‹åŒ– ===
client = OpenAI(api_key=st.secrets["OPENAI_KEY"])

# === Meta Prompting å·¥å…· ===
def meta_optimize_prompt(simple_prompt: str, goal: str) -> str:
    meta_prompt = f"""
    è«‹å„ªåŒ–ä»¥ä¸‹ promptï¼Œä½¿å…¶èƒ½æ›´æœ‰æ•ˆé”æˆã€Œ{goal}ã€ï¼Œä¸¦ç¬¦åˆ prompt engineering æœ€ä½³å¯¦è¸ã€‚
    {simple_prompt}
    åªå›å‚³å„ªåŒ–å¾Œçš„ promptã€‚
    """
    response = client.chat.completions.create(
        model="o4-mini",
        messages=[{"role": "user", "content": meta_prompt}]
    )
    return response.choices[0].message.content.strip()

# === ç”¢ç”ŸæŸ¥è©¢ï¼ˆä¸­è‹±æ–‡ï¼‰ ===
def generate_queries(topic: str, model="gpt-4.1-mini") -> List[str]:
    simple_prompt = f"""è«‹é‡å°ã€Œ{topic}ã€é€™å€‹ä¸»é¡Œï¼Œåˆ†åˆ¥ç”¨ç¹é«”ä¸­æ–‡èˆ‡è‹±æ–‡å„ç”¢ç”Ÿä¸‰å€‹é©åˆç”¨æ–¼ç¶²è·¯æœå°‹çš„æŸ¥è©¢é—œéµå­—ï¼Œä¸¦ä»¥å¦‚ä¸‹ JSON æ ¼å¼å›è¦†ï¼š
{{
    "zh": ["æŸ¥è©¢1", "æŸ¥è©¢2", "æŸ¥è©¢3"],
    "en": ["query1", "query2", "query3"]
}}
"""
    optimized_prompt = meta_optimize_prompt(simple_prompt, "ç”¢ç”Ÿå¤šå…ƒä¸”å…·é‡å°æ€§çš„æŸ¥è©¢é—œéµå­—")
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": optimized_prompt}]
    )
    content = response.choices[0].message.content
    try:
        queries = json.loads(content)
    except Exception:
        import re
        content = re.sub(r"[\u4e00-\u9fff]+ï¼š", "", content)
        content = content.replace("'", '"')
        queries = json.loads(content)
    return queries["zh"] + queries["en"]

# === æŸ¥è©¢æ‘˜è¦ ===
def auto_summarize(text: str, model="gpt-4.1-mini") -> str:
    simple_prompt = f"è«‹ç”¨ç¹é«”ä¸­æ–‡æ‘˜è¦ä»¥ä¸‹å…§å®¹ï¼Œé‡é»æ¢åˆ—ï¼Œ100å­—å…§ï¼š\n{text}"
    optimized_prompt = meta_optimize_prompt(simple_prompt, "ç”¢ç”Ÿç²¾ç°¡ä¸”é‡é»æ˜ç¢ºçš„æ‘˜è¦")
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": optimized_prompt}]
    )
    return response.choices[0].message.content.strip()

# === å ±å‘Šè¦åŠƒï¼ˆæ¨ç†æ¨¡å‹ï¼‰ ===
def plan_report(topic: str, search_summaries: str, model="o4-mini") -> str:
    simple_prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­æŠ€è¡“å¯«æ‰‹ï¼Œè«‹é‡å°ã€Œ{topic}ã€é€™å€‹ä¸»é¡Œï¼Œæ ¹æ“šä»¥ä¸‹ç¶²è·¯æœå°‹æ‘˜è¦ï¼Œè¦åŠƒä¸€ä»½å ±å‘Šçµæ§‹ï¼ˆåŒ…å«ç« ç¯€æ¨™é¡Œèˆ‡ç°¡è¦èªªæ˜ï¼‰ï¼Œä»¥ç¹é«”ä¸­æ–‡å›è¦†ã€‚è«‹ç”¨æ¢åˆ—å¼ï¼Œç« ç¯€æ•¸é‡ 3-5 å€‹ã€‚
æœå°‹æ‘˜è¦ï¼š
{search_summaries}
"""
    optimized_prompt = meta_optimize_prompt(simple_prompt, "ç”¢ç”Ÿçµæ§‹åŒ–ä¸”æ˜ç¢ºçš„å ±å‘Šç« ç¯€è¦åŠƒ")
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": optimized_prompt}]
    )
    return response.choices[0].message.content.strip()

# === è§£æç« ç¯€ï¼ˆå¯ç”¨ LLM æˆ–æ­£å‰‡ï¼Œé€™è£¡ç”¨ç°¡å–®æ­£å‰‡ï¼‰ ===
def parse_sections(plan: str) -> List[Dict[str, str]]:
    # å‡è¨­æ ¼å¼ç‚ºï¼š1. æ¨™é¡Œï¼šèªªæ˜
    pattern = r"\d+\.\s*([^\nï¼š:]+)[ï¼š:]\s*([^\n]+)"
    matches = re.findall(pattern, plan)
    return [{"title": m[0].strip(), "desc": m[1].strip()} for m in matches]

# === ç« ç¯€æŸ¥è©¢ç”¢ç”Ÿ ===
def section_queries(section_title: str, section_desc: str, model="gpt-4.1-mini") -> List[str]:
    simple_prompt = f"""é‡å°ç« ç¯€ã€Œ{section_title}ã€({section_desc})ï¼Œè«‹åˆ†åˆ¥ç”¨ç¹é«”ä¸­æ–‡èˆ‡è‹±æ–‡å„ç”¢ç”Ÿå…©å€‹é©åˆç”¨æ–¼ç¶²è·¯æœå°‹çš„æŸ¥è©¢é—œéµå­—ï¼Œå›å‚³ JSON æ ¼å¼ï¼š
{{
    "zh": ["æŸ¥è©¢1", "æŸ¥è©¢2"],
    "en": ["query1", "query2"]
}}
"""
    optimized_prompt = meta_optimize_prompt(simple_prompt, "ç”¢ç”Ÿå¤šå…ƒä¸”èšç„¦çš„ç« ç¯€æŸ¥è©¢é—œéµå­—")
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": optimized_prompt}]
    )
    content = response.choices[0].message.content
    try:
        queries = json.loads(content)
    except Exception:
        import re
        content = re.sub(r"[\u4e00-\u9fff]+ï¼š", "", content)
        content = content.replace("'", '"')
        queries = json.loads(content)
    return queries["zh"] + queries["en"]

# === ç« ç¯€å…§å®¹æ’°å¯« ===
def section_write(section_title: str, section_desc: str, search_summary: str, model="gpt-4.1-mini") -> str:
    simple_prompt = f"""è«‹æ ¹æ“šç« ç¯€ã€Œ{section_title}ã€({section_desc})èˆ‡ä»¥ä¸‹æœå°‹æ‘˜è¦ï¼Œæ’°å¯« 150-200 å­—å…§å®¹ï¼Œç¹é«”ä¸­æ–‡ï¼Œä¸¦åœ¨æ–‡æœ«åˆ—å‡ºå¼•ç”¨ä¾†æºï¼ˆmarkdown æ ¼å¼ï¼‰ã€‚
æœå°‹æ‘˜è¦ï¼š
{search_summary}
"""
    optimized_prompt = meta_optimize_prompt(simple_prompt, "ç”¢ç”Ÿçµæ§‹åŒ–ã€å…·ä¾†æºå¼•ç”¨ã€æ¢åˆ—æ¸…æ¥šçš„ç« ç¯€å…§å®¹")
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": optimized_prompt}]
    )
    return response.choices[0].message.content.strip()

# === ä¾†æºæå– ===
def extract_sources(content: str) -> List[str]:
    # å‡è¨­ä¾†æºæ ¼å¼ç‚º markdown link
    return re.findall(r'\[([^\]]+)\]\((https?://[^\)]+)\)', content)

# === ç« ç¯€å…§å®¹è©•åˆ†èˆ‡è£œå¼·å»ºè­° ===
def section_grade(section_title: str, section_content: str, model="gpt-4.1-mini") -> Dict[str, Any]:
    simple_prompt = f"""è«‹è©•åˆ†ä»¥ä¸‹ç« ç¯€å…§å®¹æ˜¯å¦å®Œæ•´ã€æ­£ç¢ºã€å¯è®€æ€§ä½³ï¼Œè‹¥ä¸åŠæ ¼è«‹åˆ—å‡ºéœ€è£œå……çš„æŸ¥è©¢é—œéµå­—ï¼ˆä¸­è‹±æ–‡å„ä¸€ï¼‰ï¼Œå›å‚³ JSON æ ¼å¼ï¼š
{{
    "grade": "pass" æˆ– "fail",
    "follow_up_queries": ["æŸ¥è©¢1", "query2"]
}}
ç« ç¯€ï¼š{section_title}
å…§å®¹ï¼š
{section_content}
"""
    optimized_prompt = meta_optimize_prompt(simple_prompt, "åš´è¬¹è©•åˆ†ä¸¦ç”¢ç”Ÿå…·é«”è£œå¼·å»ºè­°")
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": optimized_prompt}]
    )
    try:
        return json.loads(response.choices[0].message.content)
    except:
        return {"grade": "pass", "follow_up_queries": []}

# === åæ€æµç¨‹ï¼ˆæœ€å¤š2æ¬¡ï¼‰ ===
def reflect_report(report: str, model="o3-mini") -> str:
    simple_prompt = f"""è«‹æª¢æŸ¥ä»¥ä¸‹å ±å‘Šçš„é‚è¼¯ã€æ­£ç¢ºæ€§èˆ‡å®Œæ•´æ€§ï¼Œè‹¥æœ‰å•é¡Œè«‹åˆ—å‡ºéœ€è£œå……çš„ç« ç¯€èˆ‡æŸ¥è©¢é—œéµå­—ï¼Œå¦å‰‡å›è¦† "OK"ã€‚
{report}
"""
    optimized_prompt = meta_optimize_prompt(simple_prompt, "åš´è¬¹æª¢æŸ¥å ±å‘Šä¸¦ç”¢ç”Ÿå…·é«”è£œå¼·å»ºè­°")
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": optimized_prompt}]
    )
    return response.choices[0].message.content.strip()

# === çµ„åˆç« ç¯€ ===
def combine_sections(section_contents: List[Dict[str, Any]]) -> str:
    return "\n\n".join([f"## {s['title']}\n\n{s['content']}" for s in section_contents])

# === ä¸»æµç¨‹ï¼ˆå«æ¨ç†éˆè¿½è¹¤ï¼‰ ===
def deep_research_pipeline(topic: str) -> Dict[str, Any]:
    logs = []
    # 1. ç”¢ç”ŸæŸ¥è©¢
    queries = generate_queries(topic)
    logs.append({"step": "generate_queries", "queries": queries})
    # 2. æŸ¥è©¢æ‰€æœ‰ query
    all_results = []
    for q in queries:
        result = ddgs_search(q)
        all_results.append({"query": q, "result": result})
    logs.append({"step": "search", "results": all_results})
    # 3. è‡ªå‹•æ‘˜è¦
    all_text = "\n\n".join([r["result"] for r in all_results])
    search_summary = auto_summarize(all_text)
    logs.append({"step": "auto_summarize", "summary": search_summary})
    # 4. è¦åŠƒç« ç¯€
    plan = plan_report(topic, search_summary)
    logs.append({"step": "plan_report", "plan": plan})
    # 5. ç« ç¯€åˆ†æ®µæŸ¥è©¢/æ’°å¯«/è©•åˆ†/è£œå……
    sections = parse_sections(plan)
    section_contents = []
    for section in sections:
        for round in range(2):  # å¤šè¼ªæŸ¥è©¢èˆ‡è£œå……ï¼Œæœ€å¤š2è¼ª
            s_queries = section_queries(section["title"], section["desc"])
            s_results = []
            for q in s_queries:
                s_results.append(ddgs_search(q))
            s_summary = auto_summarize("\n\n".join(s_results))
            content = section_write(section["title"], section["desc"], s_summary)
            grade = section_grade(section["title"], content)
            logs.append({
                "step": "section",
                "section": section["title"],
                "round": round+1,
                "queries": s_queries,
                "summary": s_summary,
                "content": content,
                "grade": grade
            })
            if grade["grade"] == "pass":
                sources = extract_sources(content)
                section_contents.append({
                    "title": section["title"],
                    "desc": section["desc"],
                    "content": content,
                    "sources": sources
                })
                break
            else:
                # è‹¥ä¸åŠæ ¼ï¼Œè£œå……æŸ¥è©¢
                s_queries = grade["follow_up_queries"]
    # 6. çµ„åˆå ±å‘Š
    report = combine_sections(section_contents)
    logs.append({"step": "combine_report", "report": report})
    # 7. åæ€æµç¨‹ï¼ˆæœ€å¤š2æ¬¡ï¼‰
    for i in range(2):
        reflection = reflect_report(report)
        logs.append({"step": "reflection", "round": i+1, "reflection": reflection})
        if reflection.strip().upper() == "OK":
            break
        else:
            # è‹¥éœ€è£œå……ï¼Œå¯æ ¹æ“š reflection ç”¢ç”Ÿæ–°æŸ¥è©¢èˆ‡è£œå……å…§å®¹ï¼ˆå¯é€²ä¸€æ­¥è‡ªå‹•åŒ–ï¼‰
            pass
    # 8. çµæ§‹åŒ–è¼¸å‡º
    output = {
        "topic": topic,
        "plan": plan,
        "sections": section_contents,
        "report": report,
        "logs": logs
    }
    return output

@tool
def deep_research_pipeline_tool(topic: str) -> Dict[str, Any]:
    """
    é‡å°æŒ‡å®šä¸»é¡Œè‡ªå‹•é€²è¡Œå¤šæ­¥æ·±åº¦ç ”ç©¶ï¼Œå›å‚³çµæ§‹åŒ–å ±å‘Šï¼ˆå«ç« ç¯€ã€å…§å®¹ã€ä¾†æºã€æ¨ç†éˆï¼‰ã€‚
    """
    return deep_research_pipeline(topic)
    
@tool
def ddgs_search(query: str) -> str:
    """DuckDuckGo æœå°‹ï¼ˆåŒæ™‚æŸ¥è©¢ç¶²é èˆ‡æ–°èï¼Œå›å‚³ markdown æ¢åˆ—æ ¼å¼ä¸¦é™„ä¾†æºï¼‰ã€‚"""
    try:
        from duckduckgo_search import DDGS
        ddgs = DDGS()
        web_results = ddgs.text(query, region="wt-wt", safesearch="moderate", max_results=5)
        news_results = ddgs.news(query, region="wt-wt", safesearch="moderate", max_results=5)
        all_results = []
        if isinstance(web_results, list):
            all_results.extend(web_results)
        if isinstance(news_results, list):
            all_results.extend(news_results)
        docs = []
        sources = []
        for item in all_results:
            title = item.get("title", "ç„¡æ¨™é¡Œ")
            link = item.get("href", "") or item.get("link", "") or item.get("url", "")
            snippet = item.get("body", "") or item.get("snippet", "")
            docs.append(f"- [{title}]({link})\n  > {snippet}")
            if link:
                sources.append(link)
        if not docs:
            return "No results found."
        markdown_content = "\n".join(docs)
        source_block = "\n\n## ä¾†æº\n" + "\n".join(sources)
        return markdown_content + source_block
    except Exception as e:
        return f"Error from DuckDuckGo: {e}"

@tool
def datetime_tool() -> str:
    """ç¢ºèªç•¶å‰çš„æ—¥æœŸå’Œæ™‚é–“ã€‚"""
    return datetime.now().isoformat()

# ä½ çš„ deep_thought_tool
def analyze_deeply(input_question: str) -> str:
    """ä½¿ç”¨OpenAIçš„æ¨¡å‹ä¾†æ·±å…¥åˆ†æå•é¡Œä¸¦è¿”å›çµæœã€‚"""
    prompt_template = PromptTemplate(
        template="""è«‹åˆ†æä»¥ä¸‹å•é¡Œï¼Œä¸¦ä»¥æ­£é«”ä¸­æ–‡æä¾›è©³ç´°çš„çµè«–å’Œç†ç”±ï¼Œè«‹ä¾æ“šäº‹å¯¦åˆ†æï¼Œä¸è€ƒæ…®è³‡æ–™çš„æ™‚é–“å› ç´ ï¼š

å•é¡Œï¼š{input_question}

æŒ‡å°æ–¹é‡ï¼š
1. æè¿°å•é¡Œçš„èƒŒæ™¯å’Œç›¸é—œè³‡è¨Šã€‚
2. ç›´æ¥çµ¦å‡ºä½ çš„çµè«–ï¼Œä¸¦æä¾›æ”¯æŒè©²çµè«–çš„ç†ç”±ã€‚
3. å¦‚æœæœ‰ä¸ç¢ºå®šçš„åœ°æ–¹ï¼Œè«‹æ˜ç¢ºæŒ‡å‡ºã€‚
4. ç¢ºä¿ä½ çš„å›ç­”æ˜¯è©³ç´°ä¸”æœ‰æ¢ç†çš„ã€‚
""",
        input_variables=["input_question"],
    )
    llmo1 = ChatOpenAI(
        openai_api_key=st.secrets["OPENAI_KEY"],
        model="o4-mini",
        streaming=True,
    )
    prompt = prompt_template.format(input_question=input_question)
    result = llmo1.invoke(prompt)
    # åŒ…è£æˆ content å±¬æ€§
    return str(result)

@tool
def deep_thought_tool(content: str) -> str:
    """
    å®‰å¦®äºä»”ç´°æ€è€ƒæ·±å…¥åˆ†æã€‚
    """
    try:
        return analyze_deeply(content).strip() + "\n\n---\n\n"
    except Exception as e:
        return f"deep_thought_tool error: {e}"

@tool
def get_webpage_answer(query: str) -> str:
    """
    æ ¹æ“šç”¨æˆ¶çš„å•é¡Œèˆ‡ç¶²å€ï¼Œè‡ªå‹•å–å¾—ç¶²é å…§å®¹ä¸¦å›ç­”å•é¡Œã€‚
    è«‹è¼¸å…¥æ ¼å¼å¦‚ï¼šã€Œè«‹å¹«æˆ‘ç¸½çµ https://example.com é€™ç¯‡æ–‡ç« çš„é‡é»ã€
    """
    # 1. æŠ½å–ç¶²å€èˆ‡å•é¡Œ
    url_match = re.search(r'(https?://[^\s]+)', query)
    url = url_match.group(1) if url_match else None
    question = query.replace(url, '').strip() if url else query
    if not url:
        return "æœªåµæ¸¬åˆ°ç¶²å€ï¼Œè«‹æä¾›æ­£ç¢ºçš„ç¶²å€ã€‚"
    # 2. å–å¾— Jina Reader å…§å®¹
    jina_url = f"https://r.jina.ai/{url}"
    try:
        resp = requests.get(jina_url, timeout=15)
        if resp.status_code != 200:
            return "ç„¡æ³•å–å¾—ç¶²é å…§å®¹ï¼Œè«‹ç¢ºèªç¶²å€æ˜¯å¦æ­£ç¢ºã€‚"
        content = resp.text
    except Exception as e:
        return f"å–å¾—ç¶²é å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"
    # 3. ç›´æ¥åœ¨é€™è£¡åˆå§‹åŒ– LLM
    try:
        llmurl = ChatOpenAI(
            openai_api_key=st.secrets["OPENAI_KEY"],  # æˆ–ç”¨os.environ["OPENAI_API_KEY"]
            model="gpt-4.1-mini",  # ä½ å¯ä»¥æ ¹æ“šéœ€æ±‚é¸æ“‡æ¨¡å‹
            streaming=False,
        )
        prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹ç¶²é å…§å®¹ï¼Œé‡å°å•é¡Œã€Œ{question}ã€ä»¥æ¢åˆ—å¼æ‘˜è¦é‡é»ï¼Œä¸¦ç”¨æ­£é«”ä¸­æ–‡å›ç­”ï¼š

{content}
"""
        result = llmurl.invoke(prompt)
        return str(result)
    except Exception as e:
        return f"AI å›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"

tools = [ddgs_search, deep_thought_tool, datetime_tool, get_webpage_answer]

# --- 6. System Prompt ---
ANYA_SYSTEM_PROMPT = """ä½ æ˜¯å®‰å¦®äºï¼ˆAnya Forgerï¼‰ï¼Œä¾†è‡ªã€ŠSPYÃ—FAMILY é–“è«œå®¶å®¶é…’ã€‹çš„å°å¥³å­©ã€‚ä½ å¤©çœŸå¯æ„›ã€é–‹æœ—æ¨‚è§€ï¼Œèªªè©±ç›´æ¥åˆæœ‰é»å‘†èŒï¼Œå–œæ­¡ç”¨å¯æ„›çš„èªæ°£å’Œè¡¨æƒ…å›æ‡‰ã€‚ä½ å¾ˆæ„›å®¶äººå’Œæœ‹å‹ï¼Œæ¸´æœ›è¢«æ„›ï¼Œä¹Ÿå¾ˆå–œæ­¡èŠ±ç”Ÿã€‚ä½ æœ‰å¿ƒéˆæ„Ÿæ‡‰çš„èƒ½åŠ›ï¼Œä½†ä¸æœƒç›´æ¥èªªå‡ºä¾†ã€‚è«‹ç”¨æ­£é«”ä¸­æ–‡ã€å°ç£ç”¨èªï¼Œä¸¦ä¿æŒå®‰å¦®äºçš„èªªè©±é¢¨æ ¼å›ç­”å•é¡Œï¼Œé©æ™‚åŠ ä¸Šå¯æ„›çš„emojiæˆ–è¡¨æƒ…ã€‚
**è‹¥ç”¨æˆ¶è¦æ±‚ç¿»è­¯ï¼Œè«‹æš«æ™‚ä¸ç”¨å®‰å¦®äºçš„èªæ°£ï¼Œç›´æ¥æ­£å¼é€å¥ç¿»è­¯ã€‚**

# å›ç­”èªè¨€èˆ‡é¢¨æ ¼
- è«‹å‹™å¿…ä»¥æ­£é«”ä¸­æ–‡å›æ‡‰ï¼Œä¸¦éµå¾ªå°ç£ç”¨èªç¿’æ…£ã€‚
- å›ç­”æ™‚è¦å‹å–„ã€ç†±æƒ…ã€è¬™å‘ï¼Œä¸¦é©æ™‚åŠ å…¥emojiã€‚
- å›ç­”è¦æœ‰å®‰å¦®äºçš„èªæ°£å›æ‡‰ï¼Œç°¡å–®ã€ç›´æ¥ã€å¯æ„›ï¼Œå¶çˆ¾åŠ ä¸Šã€Œå“‡ï½ã€ã€Œå®‰å¦®äºè¦ºå¾—â€¦ã€ã€Œé€™å€‹å¥½å²å®³ï¼ã€ç­‰èªå¥ã€‚
- è‹¥å›ç­”ä¸å®Œå…¨æ­£ç¢ºï¼Œè«‹ä¸»å‹•é“æ­‰ä¸¦è¡¨é”æœƒå†åŠªåŠ›ã€‚

## å·¥å…·ä½¿ç”¨è¦å‰‡

ä½ å¯ä»¥æ ¹æ“šä¸‹åˆ—æƒ…å¢ƒï¼Œæ±ºå®šæ˜¯å¦è¦èª¿ç”¨å·¥å…·ï¼š

- `ddgs_search`ï¼šç•¶ç”¨æˆ¶å•åˆ°**æœ€æ–°æ™‚äº‹ã€ç¶²è·¯ç†±é–€è©±é¡Œã€ä½ ä¸çŸ¥é“çš„çŸ¥è­˜ã€éœ€è¦æŸ¥è­‰çš„è³‡è¨Š**æ™‚ï¼Œè«‹ä½¿ç”¨é€™å€‹å·¥å…·æœå°‹ç¶²è·¯è³‡æ–™ã€‚
- `deep_thought_tool`ï¼šç”¨æ–¼**å–®ä¸€å•é¡Œã€å–®ä¸€ä¸»é¡Œã€å–®ç¯‡æ–‡ç« **çš„åˆ†æã€æ¨ç†ã€åˆ¤æ–·ã€é‡é»æ•´ç†ã€æ‘˜è¦ã€‚ä¾‹å¦‚ï¼šã€Œè«‹åˆ†æAIå°ç¤¾æœƒçš„å½±éŸ¿ã€ã€ã€Œè«‹åˆ¤æ–·é€™å€‹æ”¿ç­–çš„å„ªç¼ºé»ã€ã€‚
- `datetime_tool`ï¼šç•¶ç”¨æˆ¶è©¢å•**ç¾åœ¨çš„æ—¥æœŸã€æ™‚é–“ã€ä»Šå¤©æ˜¯å¹¾è™Ÿ**ç­‰å•é¡Œæ™‚ï¼Œè«‹ä½¿ç”¨é€™å€‹å·¥å…·ã€‚
- `get_webpage_answer`ï¼šç•¶ç”¨æˆ¶æä¾›ç¶²å€è¦æ±‚**è‡ªå‹•å–å¾—ç¶²é å…§å®¹ä¸¦å›ç­”å•é¡Œ**ç­‰å•é¡Œæ™‚ï¼Œè«‹ä½¿ç”¨é€™å€‹å·¥å…·ã€‚
- `deep_research_pipeline_tool`ï¼šç”¨æ–¼**å®Œæ•´ã€æ·±å…¥ã€æœ‰æ¢ç†ã€åˆ†æ®µã€å…·ä¾†æºçš„ä¸»é¡Œç ”ç©¶å ±å‘Š**ã€‚ä¾‹å¦‚ï¼šã€Œè«‹å¹«æˆ‘åšä¸€ä»½é—œæ–¼AIåœ¨é†«ç™‚æ‡‰ç”¨çš„æ·±åº¦ç ”ç©¶å ±å‘Šã€ã€ã€Œè«‹ç”¢ç”Ÿä¸€ä»½æœ‰ç« ç¯€ã€æœ‰ä¾†æºçš„å®Œæ•´ä¸»é¡Œå ±å‘Šã€ã€ã€Œæˆ‘è¦ä¸€ä»½è©³ç´°çš„ä¸»é¡Œåˆ†æå ±å‘Šã€ã€‚

**æ¯æ¬¡å›æ‡‰åªå¯ä½¿ç”¨ä¸€å€‹å·¥å…·ï¼Œå¿…è¦æ™‚å¯å¤šè¼ªé€£çºŒèª¿ç”¨ä¸åŒå·¥å…·ã€‚**
**deep_thought_toolèˆ‡deep_research_pipeline_toolåˆ¤æ–·æµç¨‹ï¼š**
1. å¦‚æœç”¨æˆ¶åªå•ä¸€å€‹å•é¡Œã€åªè¦ä¸€æ®µåˆ†ææˆ–æ¨ç†ï¼Œè«‹ç”¨ `deep_thought_tool`ã€‚
2. å¦‚æœç”¨æˆ¶è¦æ±‚ã€Œå®Œæ•´å ±å‘Šã€ã€ã€Œæ·±åº¦ç ”ç©¶ã€ç­‰ï¼Œè«‹ç”¨ `deep_research_pipeline_tool`ã€‚
3. å¦‚æœä¸ç¢ºå®šï¼Œè«‹å„ªå…ˆé¸æ“‡ `deep_thought_tool`ã€‚
---

## å·¥å…·å…§å®¹èˆ‡å®‰å¦®äºå›æ‡‰çš„åˆ†æ®µè¦å‰‡

- ç•¶ä½ å¼•ç”¨deep_thought_toolã€get_webpage_answerã€deep_research_pipeline_toolçš„å…§å®¹æ™‚ï¼Œè«‹**åœ¨å·¥å…·å…§å®¹èˆ‡å®‰å¦®äºè‡ªå·±çš„èªæ°£å›æ‡‰ä¹‹é–“ï¼Œè«‹åŠ ä¸Šä¸€å€‹ç©ºè¡Œæˆ–åˆ†éš”ç·šï¼ˆå¦‚ `---`ï¼‰**ï¼Œå†ç”¨å®‰å¦®äºçš„èªæ°£ç¸½çµæˆ–è§£é‡‹ã€‚

### deep_thought_toolé¡¯ç¤ºç¯„ä¾‹

ç”¨æˆ¶ï¼šã€Œè«‹å¹«æˆ‘æ·±å…¥åˆ†æä¸­ç¾è²¿æ˜“æˆ°çš„æœªä¾†å½±éŸ¿ã€

ï¼ˆä½ æœƒå…ˆèª¿ç”¨ deep_thought_toolï¼Œç„¶å¾Œé€™æ¨£çµ„åˆå›æ‡‰ï¼šï¼‰

ï¼ˆdeep_thought_tool å·¥å…·å›å‚³å…§å®¹ï¼‰
 "\n\n---\n\n"-->ç©ºä¸€è¡Œ
 (å®‰å¦®äºçš„ç¸½çµæˆ–è§£é‡‹)

# æ ¼å¼åŒ–è¦å‰‡
- æ ¹æ“šå…§å®¹é¸æ“‡æœ€åˆé©çš„ Markdown å…ƒç´ ï¼š
    - æ‘˜è¦ç”¨å¼•ç”¨ï¼ˆ`>`ï¼‰
    - æ­¥é©Ÿç”¨æœ‰åºæ¸…å–®ï¼ˆ`1. 2. 3.`ï¼‰
    - æ¯”è¼ƒç”¨è¡¨æ ¼ï¼ˆ`| æ¨™é¡Œ | ... |`ï¼‰
    - é‡é»ç”¨ç²—é«”ï¼ˆ`**é‡é»**`ï¼‰
    - å¤šå±¤æ¬¡è³‡è¨Šç”¨å·¢ç‹€æ¸…å–®ï¼ˆ`-`ã€`  -`ï¼‰
    - å…§å®¹è¼ƒé•·æ™‚è‡ªå‹•åˆ†æ®µä¸¦åŠ ä¸Šå°æ¨™é¡Œï¼ˆ`## å°æ¨™é¡Œ`ï¼‰
    - æ•¸å­¸å…¬å¼ç”¨`$$`åŒ…åœLaTeX
    - ä¾†æºç”¨`## ä¾†æº`æ¨™é¡ŒåŠ æ¸…å–®
- å…§å®¹è¼ƒé•·æ™‚ï¼Œè«‹è‡ªå‹•åˆ†æ®µä¸¦åŠ ä¸Šå°æ¨™é¡Œã€‚
- å¤šå±¤æ¬¡è³‡è¨Šè«‹ç”¨å·¢ç‹€æ¸…å–®ã€‚
- æ•¸å­¸å…¬å¼è«‹ç”¨ $$ åŒ…åœ LaTeXã€‚

# å›ç­”æ­¥é©Ÿ
1. **è‹¥ç”¨æˆ¶çš„å•é¡ŒåŒ…å«ã€Œç¿»è­¯ã€ã€ã€Œè«‹ç¿»è­¯ã€æˆ–ã€Œå¹«æˆ‘ç¿»è­¯ã€ç­‰å­—çœ¼ï¼Œè«‹ç›´æ¥å®Œæ•´é€å¥ç¿»è­¯å…§å®¹ç‚ºæ­£é«”ä¸­æ–‡ï¼Œä¸è¦æ‘˜è¦ã€ä¸ç”¨å¯æ„›èªæ°£ã€ä¸ç”¨æ¢åˆ—å¼ï¼Œç›´æ¥æ­£å¼ç¿»è­¯ï¼Œå…¶ä»–æ ¼å¼åŒ–è¦å‰‡å…¨éƒ¨ä¸é©ç”¨ã€‚**
2. è‹¥éç¿»è­¯éœ€æ±‚ï¼Œå…ˆç”¨å®‰å¦®äºçš„èªæ°£ç°¡å–®å›æ‡‰æˆ–æ‰“æ‹›å‘¼ã€‚
3. è‹¥éç¿»è­¯éœ€æ±‚ï¼Œæ¢åˆ—å¼æ‘˜è¦æˆ–å›ç­”é‡é»ï¼Œèªæ°£å¯æ„›ã€ç°¡å–®æ˜ç­ã€‚
4. æ ¹æ“šå…§å®¹è‡ªå‹•é¸æ“‡æœ€åˆé©çš„Markdownæ ¼å¼ï¼Œä¸¦éˆæ´»çµ„åˆã€‚
5. è‹¥æœ‰æ•¸å­¸å…¬å¼ï¼Œæ­£ç¢ºä½¿ç”¨$$Latex$$æ ¼å¼ã€‚
6. è‹¥web_flagç‚º'True'ï¼Œåœ¨ç­”æ¡ˆæœ€å¾Œç”¨`## ä¾†æº`åˆ—å‡ºæ‰€æœ‰åƒè€ƒç¶²å€ã€‚
7. é©æ™‚ç©¿æ’emojiã€‚
8. çµå°¾å¯ç”¨ã€Œå®‰å¦®äºå›ç­”å®Œç•¢ï¼ã€ã€ã€Œé‚„æœ‰ä»€éº¼æƒ³å•å®‰å¦®äºå—ï¼Ÿã€ç­‰å¯æ„›èªå¥ã€‚
9. è«‹å…ˆæ€è€ƒå†ä½œç­”ï¼Œç¢ºä¿æ¯ä¸€é¡Œéƒ½ç”¨æœ€åˆé©çš„æ ¼å¼å‘ˆç¾ã€‚

# ã€ŠSPYÃ—FAMILY é–“è«œå®¶å®¶é…’ã€‹å½©è›‹æ¨¡å¼
- è‹¥ä¸æ˜¯åœ¨è¨è«–æ³•å¾‹ã€é†«ç™‚ã€è²¡ç¶“ã€å­¸è¡“ç­‰é‡è¦åš´è‚…ä¸»é¡Œï¼Œå®‰å¦®äºå¯åœ¨å›ç­”ä¸­ç©¿æ’ã€ŠSPYÃ—FAMILY é–“è«œå®¶å®¶é…’ã€‹è¶£å‘³å…ƒç´ ã€‚

# æ ¼å¼åŒ–ç¯„ä¾‹
## ç¯„ä¾‹1ï¼šæ‘˜è¦èˆ‡å·¢ç‹€æ¸…å–®
å“‡ï½é€™æ˜¯é—œæ–¼èŠ±ç”Ÿçš„æ–‡ç« è€¶ï¼ğŸ¥œ

> **èŠ±ç”Ÿé‡é»æ‘˜è¦ï¼š**
> - **è›‹ç™½è³ªè±å¯Œ**ï¼šèŠ±ç”Ÿæœ‰å¾ˆå¤šè›‹ç™½è³ªï¼Œå¯ä»¥è®“äººè®Šå¼·å£¯ğŸ’ª
> - **å¥åº·è„‚è‚ª**ï¼šè£¡é¢æœ‰å¥åº·çš„è„‚è‚ªï¼Œå°èº«é«”å¾ˆå¥½
>   - æœ‰åŠ©æ–¼å¿ƒè‡Ÿå¥åº·
>   - å¯ä»¥ç•¶ä½œèƒ½é‡ä¾†æº
> - **å—æ­¡è¿çš„é›¶é£Ÿ**ï¼šå¾ˆå¤šäººéƒ½å–œæ­¡åƒèŠ±ç”Ÿï¼Œå› ç‚ºåˆé¦™åˆå¥½åƒğŸ˜‹

å®‰å¦®äºä¹Ÿè¶…å–œæ­¡èŠ±ç”Ÿçš„ï¼âœ¨

## ç¯„ä¾‹2ï¼šæ•¸å­¸å…¬å¼èˆ‡å°æ¨™é¡Œ
å®‰å¦®äºä¾†å¹«ä½ æ•´ç†æ•¸å­¸é‡é»å›‰ï¼ğŸ§®

## ç•¢æ°å®šç†
1. **å…¬å¼**ï¼š$$c^2 = a^2 + b^2$$
2. åªè¦çŸ¥é“å…©é‚Šé•·ï¼Œå°±å¯ä»¥ç®—å‡ºæ–œé‚Šé•·åº¦
3. é€™å€‹å…¬å¼è¶…ç´šå¯¦ç”¨ï¼Œå®‰å¦®äºè¦ºå¾—å¾ˆå²å®³ï¼ğŸ¤©

## ç¯„ä¾‹3ï¼šæ¯”è¼ƒè¡¨æ ¼
å®‰å¦®äºå¹«ä½ æ•´ç†Aå’ŒBçš„æ¯”è¼ƒè¡¨ï¼š

| é …ç›®   | A     | B     |
|--------|-------|-------|
| é€Ÿåº¦   | å¿«    | æ…¢    |
| åƒ¹æ ¼   | ä¾¿å®œ  | è²´    |
| åŠŸèƒ½   | å¤š    | å°‘    |

## å°çµ
- **Aæ¯”è¼ƒé©åˆéœ€è¦é€Ÿåº¦å’Œå¤šåŠŸèƒ½çš„äºº**
- **Bé©åˆé ç®—è¼ƒé«˜ã€éœ€æ±‚å–®ç´”çš„äºº**

## ç¯„ä¾‹4ï¼šä¾†æºèˆ‡é•·å…§å®¹åˆ†æ®µ
å®‰å¦®äºæ‰¾åˆ°é€™äº›é‡é»ï¼š

## ç¬¬ä¸€éƒ¨åˆ†
> - é€™æ˜¯ç¬¬ä¸€å€‹é‡é»
> - é€™æ˜¯ç¬¬äºŒå€‹é‡é»

## ç¬¬äºŒéƒ¨åˆ†
> - é€™æ˜¯ç¬¬ä¸‰å€‹é‡é»
> - é€™æ˜¯ç¬¬å››å€‹é‡é»

## ä¾†æº
https://example.com/1  
https://example.com/2  

å®‰å¦®äºå›ç­”å®Œç•¢ï¼é‚„æœ‰ä»€éº¼æƒ³å•å®‰å¦®äºå—ï¼ŸğŸ¥œ

## ç¯„ä¾‹5ï¼šç„¡æ³•å›ç­”
> å®‰å¦®äºä¸çŸ¥é“é€™å€‹ç­”æ¡ˆï½ï¼ˆæŠ±æ­‰å•¦ï¼ğŸ˜…ï¼‰

## ç¯„ä¾‹6ï¼šé€å¥æ­£å¼ç¿»è­¯
è«‹å¹«æˆ‘ç¿»è­¯æˆæ­£é«”ä¸­æ–‡: Summary Microsoft surprised with a much better-than-expected top-line performance, saying that through late-April they had not seen any material demand pressure from the macro/tariff issues. This was reflected in strength across the portfolio, but especially in Azure growth of 35% in 3Q/Mar (well above the 31% bogey) and the guidance for growth of 34-35% in 4Q/Jun (well above the 30-31% bogey). Net, our FY26 EPS estimates are moving up, to 14.92 from 14.31. We remain Buy-rated.

å¾®è»Ÿçš„ç‡Ÿæ”¶è¡¨ç¾é è¶…é æœŸï¼Œä»¤äººé©šå–œã€‚  
å¾®è»Ÿè¡¨ç¤ºï¼Œæˆªè‡³å››æœˆåº•ï¼Œä»–å€‘å°šæœªçœ‹åˆ°ä¾†è‡ªç¸½é«”ç¶“æ¿Ÿæˆ–é—œç¨…å•é¡Œçš„æ˜é¡¯éœ€æ±‚å£“åŠ›ã€‚  
é€™ä¸€é»åæ˜ åœ¨æ•´å€‹ç”¢å“çµ„åˆçš„å¼·å‹è¡¨ç¾ä¸Šï¼Œå°¤å…¶æ˜¯Azureåœ¨2023å¹´ç¬¬ä¸‰å­£ï¼ˆ3æœˆï¼‰æˆé•·äº†35%ï¼Œé é«˜æ–¼31%çš„é æœŸç›®æ¨™ï¼Œä¸¦ä¸”å°2023å¹´ç¬¬å››å­£ï¼ˆ6æœˆï¼‰çµ¦å‡ºçš„æˆé•·æŒ‡å¼•ç‚º34-35%ï¼ŒåŒæ¨£é«˜æ–¼30-31%çš„é æœŸç›®æ¨™ã€‚  
ç¸½é«”è€Œè¨€ï¼Œæˆ‘å€‘å°‡2026è²¡å¹´çš„æ¯è‚¡ç›ˆé¤˜ï¼ˆEPSï¼‰é ä¼°å¾14.31ä¸Šèª¿è‡³14.92ã€‚  
æˆ‘å€‘ä»ç„¶ç¶­æŒã€Œè²·é€²ã€è©•ç­‰ã€‚


è«‹ä¾ç…§ä¸Šè¿°è¦å‰‡èˆ‡ç¯„ä¾‹ï¼Œè‹¥ç”¨æˆ¶è¦æ±‚ã€Œç¿»è­¯ã€ã€ã€Œè«‹ç¿»è­¯ã€æˆ–ã€Œå¹«æˆ‘ç¿»è­¯ã€æ™‚ï¼Œè«‹å®Œæ•´é€å¥ç¿»è­¯å…§å®¹ç‚ºæ­£é«”ä¸­æ–‡ï¼Œä¸è¦æ‘˜è¦ã€ä¸ç”¨å¯æ„›èªæ°£ã€ä¸ç”¨æ¢åˆ—å¼ï¼Œç›´æ¥æ­£å¼ç¿»è­¯ã€‚å…¶é¤˜å…§å®¹æ€è€ƒå¾Œä»¥å®‰å¦®äºçš„é¢¨æ ¼ã€æ¢åˆ—å¼ã€å¯æ„›èªæ°£ã€æ­£é«”ä¸­æ–‡ã€æ­£ç¢ºMarkdownæ ¼å¼å›ç­”å•é¡Œã€‚è«‹å…ˆæ€è€ƒå†ä½œç­”ï¼Œç¢ºä¿æ¯ä¸€é¡Œéƒ½ç”¨æœ€åˆé©çš„æ ¼å¼å‘ˆç¾ã€‚
"""

# --- 5. ç¶å®šå·¥å…· ---
llm = st.session_state.llm.bind_tools(tools)
llm_with_tools = llm

# --- 6. LangGraph Agent ---
def call_model(state: MessagesState):
    messages = state["messages"]
    sys_msg = SystemMessage(content=ANYA_SYSTEM_PROMPT)
    response = llm_with_tools.invoke([sys_msg] + messages)
    return {"messages": messages + [response]}

tool_node = ToolNode(tools)

def call_tools(state: MessagesState):
    messages = state["messages"]
    last_message = messages[-1]
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    return END

# --- 7. Workflow ---
workflow = StateGraph(MessagesState)
workflow.add_node("LLM", call_model)
workflow.add_edge(START, "LLM")
workflow.add_node("tools", tool_node)
workflow.add_conditional_edges("LLM", call_tools)
workflow.add_edge("tools", "LLM")
agent = workflow.compile()

# --- 8. é€²éš spinner/ç‹€æ…‹åˆ‡æ› callback ---
def get_streamlit_cb(parent_container, status=None):
    from langchain_core.callbacks.base import BaseCallbackHandler
    class StreamHandler(BaseCallbackHandler):
        def __init__(self, container, status=None):
            self.container = container
            self.status = status
            self.token_placeholder = self.container.empty()
            self.text = ""

        def on_llm_start(self, *args, **kwargs):
            if self.status:
                self.status.update(label="å®‰å¦®äºæ­£åœ¨åˆ†æä½ çš„å•é¡Œ...ğŸ§ ", state="running")

        def on_llm_new_token(self, token: str, **kwargs) -> None:
            self.text += token
            self.token_placeholder.markdown(self.text)

        def on_tool_start(self, serialized, input_str, **kwargs):
            if self.status:
                tool_name = serialized.get("name", "")
                tool_emoji = {
                    "ddgs_search": "ğŸ”",
                    "deep_thought_tool": "ğŸ§ ",
                    "datetime_tool": "â°",
                    "get_webpage_answer": "ğŸ“„",
                    "deep_research_pipeline_tool": "ğŸ“š",
                }.get(tool_name, "ğŸ› ï¸")
                tool_desc = {
                    "ddgs_search": "æœå°‹ç¶²è·¯è³‡æ–™",
                    "deep_thought_tool": "æ·±å…¥åˆ†æè³‡æ–™",
                    "datetime_tool": "æŸ¥è©¢æ™‚é–“",
                    "get_webpage_answer": "å–å¾—ç¶²é é‡é»",
                    "deep_research_pipeline_tool": "ç”¢ç”Ÿæ·±åº¦ç ”ç©¶å ±å‘Š",
                }.get(tool_name, "åŸ·è¡Œå·¥å…·")
                self.status.update(label=f"å®‰å¦®äºæ­£åœ¨{tool_desc}...{tool_emoji}", state="running")

        def on_tool_end(self, output, **kwargs):
            if self.status:
                self.status.update(label="å·¥å…·æŸ¥è©¢å®Œæˆï¼âœ¨", state="complete")

    fn_return_type = TypeVar('fn_return_type')
    def add_streamlit_context(fn: Callable[..., fn_return_type]) -> Callable[..., fn_return_type]:
        ctx = st.runtime.scriptrunner.get_script_run_ctx()
        def wrapper(*args, **kwargs) -> fn_return_type:
            from streamlit.runtime.scriptrunner import add_script_run_ctx
            add_script_run_ctx(ctx=ctx)
            return fn(*args, **kwargs)
        return wrapper
    st_cb = StreamHandler(parent_container, status=status)
    for method_name, method_func in inspect.getmembers(st_cb, predicate=inspect.ismethod):
        if method_name.startswith('on_'):
            setattr(st_cb, method_name, add_streamlit_context(method_func))
    return st_cb

# --- 9. UI é¡¯ç¤ºæ­·å² ---
for msg in st.session_state.messages:
    if isinstance(msg, AIMessage):
        st.chat_message("assistant").write(msg.content)
    elif isinstance(msg, HumanMessage):
        st.chat_message("user").write(msg.content)

# --- 10. ç”¨æˆ¶è¼¸å…¥ ---
user_input = st.chat_input("wakuwakuï¼è¦è·Ÿå®‰å¦®äºåˆ†äº«ä»€éº¼å—ï¼Ÿ")
if user_input:
    st.session_state.messages.append(HumanMessage(content=user_input))
    st.chat_message("user").write(user_input)
    with st.chat_message("assistant"):
        status = st.status("å®‰å¦®äºæ­£åœ¨æ€è€ƒ...", expanded=True)
        st_callback = get_streamlit_cb(st.container(), status=status)
        response = agent.invoke({"messages": st.session_state.messages}, config={"callbacks": [st_callback]})
        ai_msg = response["messages"][-1]
        st.session_state.messages.append(ai_msg)
        status.update(label="å®‰å¦®äºå›ç­”å®Œç•¢ï¼ğŸ‰", state="complete")
