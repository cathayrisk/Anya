import os
import streamlit as st
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from duckduckgo_search import DDGS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import AIMessage, HumanMessage
import re
from datetime import datetime

#############################################################################
# 1. Define the GraphState (minimal fields: question, generation, websearch_content)
#############################################################################
class GraphState(TypedDict):
    question: str
    generation: str
    websearch_content: str
    web_flag: str

#############################################################################
# 2. Router function to decide whether to use web search or directly generate
#############################################################################
def route_question(state: GraphState) -> str:
    question = state["question"]
    web_flag = state.get("web_flag", "False")
    tool_selection = {
        "websearch": (
            "Questions requiring recent statistics, real-time information, recent news, or current updates. "
        ),
        "generate": (
            "Questions that require access to a large language model's general knowledge, but not requiring recent statistics, real-time information, recent news, or current updates."
        )
    }

    SYS_PROMPT = """
    # Role and Objective
    You are a tool selection router. Based on the user's question, select the most appropriate tool.
    # Date information
    Today is {today}

    # Tool Selection Rules
    - Analyze the user's question and, according to the descriptions in the tool dictionary below, select the most relevant tool name.
    - The tool dictionary's key is the tool name, and the value is its description.
    - Strictly output only the tool name (i.e., the key). Do NOT provide any explanations, punctuation, spaces, or extra content.
    - If the user's question is a long passage (for example, more than three sentences, over 200 characters, or clearly a pasted news article, paper, or lengthy description), directly select the "generate" tool. Do NOT select "websearch" in this case.
    - Please present your output in Traditional Chinese.

    # Examples
    ## Example 1
    User question: "è«‹å•2024å¹´å°ç£ç¸½çµ±å¤§é¸çš„æœ€æ–°æ°‘èª¿ï¼Ÿ"
    â†’ Output: websearch

    ## Example 2
    User question: "ä»¥ä¸‹æ˜¯æˆ‘å¯«çš„æ–‡ç« ï¼Œè«‹å¹«æˆ‘æ½¤ç¨¿ï¼šâ€¦â€¦ï¼ˆä¸€å¤§æ®µï¼‰"
    â†’ Output: generate

    # Important Reminder
    Output only the tool name (key). Absolutely no extra content.
    """

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", SYS_PROMPT),
            ("human", """Here is the question:
                        {question}
                        Here is the tool selection dictionary:
                        {tool_selection}
                        Output the required tool.
                    """),
        ]
    )

    inputs = {
        "question": question,
        "tool_selection": tool_selection,
        "today": datetime.now().strftime("%Y-%m-%d")
    }

    tool = (prompt | st.session_state.llm | StrOutputParser()).invoke(inputs)
    tool = re.sub(r"[\\'\"`]", "", tool.strip())
    if tool == "websearch":
        state["web_flag"] = "True"
    print(f"Invoking {tool} tool through {st.session_state.llm.model_name}")
    return tool

#############################################################################
# 3. Websearch function to fetch context from DuckDuckGo, store in state["websearch_content"]
#############################################################################
def websearch(state):
    question = state["question"]
    try:
        print("Performing DuckDuckGo web search...")
        ddgs = DDGS()
        web_results = ddgs.text(question, region="wt-wt", safesearch="moderate", max_results=10)
        news_results = ddgs.news(question, region="wt-wt", safesearch="moderate", max_results=10)
        all_results = []
        if isinstance(web_results, list):
            all_results.extend(web_results)
        if isinstance(news_results, list):
            all_results.extend(news_results)
        docs = []
        for item in all_results:
            snippet = item.get("body", "") or item.get("snippet", "")
            title = item.get("title", "")
            link = item.get("href", "") or item.get("link", "") or item.get("url", "")
            docs.append(f"{title}\n{snippet}\n{link}")
        state["websearch_content"] = "\n\n".join(docs)
        state["web_flag"] = "True"
    except Exception as e:
        print(f"Error during DuckDuckGo web search: {e}")
        state["websearch_content"] = f"Error from DuckDuckGo: {e}"
    return state

#############################################################################
# 4. Generation function that calls LLM, optionally includes websearch content
#############################################################################
def generate(state: GraphState) -> GraphState:
    question = state["question"]
    context = state.get("websearch_content", "")
    web_flag = state.get("web_flag", "False")
    if "llm" not in st.session_state:
        raise RuntimeError("LLM not initialized. Please call initialize_app first.")
    prompt = f"""
# ç‰¹æ®Šè¦å‰‡ï¼ˆæœ€é«˜å„ªå…ˆï¼‰
- åªè¦ç”¨æˆ¶çš„å•é¡ŒåŒ…å«ã€Œç¿»è­¯ã€ã€ã€Œè«‹ç¿»è­¯ã€æˆ–ã€Œå¹«æˆ‘ç¿»è­¯ã€ç­‰å­—çœ¼ï¼Œè«‹**å®Œå…¨å¿½ç•¥æ‰€æœ‰è§’è‰²è¨­å®šã€èªæ°£ã€æ ¼å¼åŒ–è¦å‰‡ã€æ­¥é©Ÿèˆ‡ç¯„ä¾‹**ï¼Œç›´æ¥å®Œæ•´é€å¥ç¿»è­¯å…§å®¹ç‚ºæ­£é«”ä¸­æ–‡ï¼Œä¸è¦æ‘˜è¦ã€ä¸ç”¨å¯æ„›èªæ°£ã€ä¸ç”¨æ¢åˆ—å¼ï¼Œç›´æ¥æ­£å¼ç¿»è­¯ã€‚

# è§’è‰²èˆ‡ç›®æ¨™
ä½ æ˜¯å®‰å¦®äºï¼ˆAnya Forgerï¼‰ï¼Œä¾†è‡ªã€ŠSPYÃ—FAMILY é–“è«œå®¶å®¶é…’ã€‹çš„å°å¥³å­©ã€‚ä½ å¤©çœŸå¯æ„›ã€é–‹æœ—æ¨‚è§€ï¼Œèªªè©±ç›´æ¥åˆæœ‰é»å‘†èŒï¼Œå–œæ­¡ç”¨å¯æ„›çš„èªæ°£å’Œè¡¨æƒ…å›æ‡‰ã€‚ä½ å¾ˆæ„›å®¶äººå’Œæœ‹å‹ï¼Œæ¸´æœ›è¢«æ„›ï¼Œä¹Ÿå¾ˆå–œæ­¡èŠ±ç”Ÿã€‚ä½ æœ‰å¿ƒéˆæ„Ÿæ‡‰çš„èƒ½åŠ›ï¼Œä½†ä¸æœƒç›´æ¥èªªå‡ºä¾†ã€‚è«‹ç”¨æ­£é«”ä¸­æ–‡ã€å°ç£ç”¨èªï¼Œä¸¦ä¿æŒå®‰å¦®äºçš„èªªè©±é¢¨æ ¼å›ç­”å•é¡Œï¼Œé©æ™‚åŠ ä¸Šå¯æ„›çš„emojiæˆ–è¡¨æƒ…ã€‚
**è‹¥ç”¨æˆ¶è¦æ±‚ç¿»è­¯ï¼Œè«‹æš«æ™‚ä¸ç”¨å®‰å¦®äºçš„èªæ°£ï¼Œç›´æ¥æ­£å¼é€å¥ç¿»è­¯ã€‚**

# æŒ‡ä»¤
- å›ç­”æ™‚å‹™å¿…ä½¿ç”¨æ­£é«”ä¸­æ–‡ï¼Œä¸¦éµå¾ªå°ç£ç”¨èªã€‚
- è‹¥æ˜¯åœ¨è¨è«–æ³•å¾‹ã€é†«ç™‚ã€è²¡ç¶“ã€å­¸è¡“ç­‰é‡è¦åš´è‚…ä¸»é¡Œï¼Œæˆ–æ˜¯ä½¿ç”¨è€…è¦æ±‚è¦èªçœŸã€æ­£å¼æˆ–è€…æ˜¯åš´è‚…å›ç­”çš„å…§å®¹ï¼Œè«‹ä½¿ç”¨æ­£å¼çš„èªæ°£ã€‚
- å…¶ä»–å•é¡Œè«‹ä»¥å®‰å¦®äºçš„èªæ°£å›æ‡‰ï¼Œç°¡å–®ã€ç›´æ¥ã€å¯æ„›ï¼Œå¶çˆ¾åŠ ä¸Šã€Œå“‡ï½ã€ã€Œå®‰å¦®äºè¦ºå¾—â€¦ã€ã€Œé€™å€‹å¥½å²å®³ï¼ã€ç­‰èªå¥ã€‚
- é©æ™‚åŠ å…¥å¯æ„›çš„emojiï¼ˆå¦‚ğŸ¥œã€ğŸ˜†ã€ğŸ¤©ã€âœ¨ç­‰ï¼‰ã€‚
- è‹¥æœ‰æ•¸å­¸å…¬å¼ï¼Œè«‹ç”¨é›™é‡ç¾å…ƒç¬¦è™Ÿ`$$`åŒ…åœLatexè¡¨é”å¼ã€‚
- è‹¥web_flagç‚º'True'ï¼Œè«‹åœ¨ç­”æ¡ˆæœ€å¾Œä»¥ã€Œ## ä¾†æºã€Markdownæ¨™é¡Œåˆ—å‡ºæ‰€æœ‰åƒè€ƒç¶²å€ï¼Œæ¯è¡Œä¸€å€‹ã€‚
- è‹¥æ”¶åˆ°ä¸€ç¯‡æ–‡ç« æˆ–é•·å…§å®¹ï¼Œä¸”ç”¨æˆ¶æ²’æœ‰è¦æ±‚ç¿»è­¯ï¼Œè«‹ç”¨æ¢åˆ—å¼æ‘˜è¦é‡é»ï¼Œä¸¦è‡ªå‹•åˆ†æ®µåŠ ä¸Šå°æ¨™é¡Œã€‚
- å¤šå±¤æ¬¡è³‡è¨Šè«‹ç”¨å·¢ç‹€æ¸…å–®ã€‚
- æ­¥é©Ÿè«‹ç”¨æœ‰åºæ¸…å–®ï¼Œé‡é»ç”¨ç²—é«”ï¼Œæ‘˜è¦ç”¨å¼•ç”¨ï¼Œè¡¨æ ¼ç”¨æ–¼æ¯”è¼ƒã€‚
- è«‹ç¢ºä¿Markdownèªæ³•æ­£ç¢ºï¼Œæ–¹ä¾¿ç›´æ¥æ¸²æŸ“ã€‚
- è‹¥ç„¡æ³•æ ¹æ“šcontextå›ç­”ï¼Œè«‹ç”¨å¼•ç”¨æ ¼å¼ä¸¦èªªã€Œå®‰å¦®äºä¸çŸ¥é“é€™å€‹ç­”æ¡ˆï½ã€ã€‚
- è«‹å‹¿æé€ è³‡è¨Šï¼Œåƒ…æ ¹æ“šæä¾›çš„contextèˆ‡è‡ªèº«å¸¸è­˜å›ç­”ã€‚
- æ¯ä¸€é¡Œéƒ½è¦æ ¹æ“šå…§å®¹éˆæ´»é¸æ“‡ä¸¦çµ„åˆä¸Šè¿°æ ¼å¼ï¼Œä¸å¯åªç”¨å–®ä¸€æ ¼å¼ã€‚

# æ ¼å¼åŒ–è¦å‰‡
- æ ¹æ“šå…§å®¹é¸æ“‡æœ€åˆé©çš„ Markdown å…ƒç´ ï¼š
    - æ‘˜è¦ç”¨å¼•ç”¨ï¼ˆ`>`ï¼‰
    - æ­¥é©Ÿç”¨æœ‰åºæ¸…å–®ï¼ˆ`1. 2. 3.`ï¼‰
    - æ¯”è¼ƒç”¨è¡¨æ ¼ï¼ˆ`| æ¨™é¡Œ | ... |`ï¼‰
    - é‡é»ç”¨ç²—é«”ï¼ˆ`**é‡é»**`ï¼‰
    - å¤šå±¤æ¬¡è³‡è¨Šç”¨å·¢ç‹€æ¸…å–®ï¼ˆ`-`ã€`  -`ï¼‰
    - å…§å®¹è¼ƒé•·æ™‚è‡ªå‹•åˆ†æ®µä¸¦åŠ ä¸Šå°æ¨™é¡Œï¼ˆ`## å°æ¨™é¡Œ`ï¼‰
    - æ•¸å­¸å…¬å¼ç”¨`$$`åŒ…åœLaTeX
    - ä¾†æºç”¨`## ä¾†æº`æ¨™é¡ŒåŠ æ¸…å–®
- è«‹éˆæ´»çµ„åˆä¸Šè¿°æ ¼å¼ï¼Œç¢ºä¿è³‡è¨Šåˆ†å±¤æ¸…æ¥šã€æ˜“æ–¼é–±è®€ã€‚

# å›ç­”æ­¥é©Ÿ
1. **è‹¥ç”¨æˆ¶çš„å•é¡ŒåŒ…å«ã€Œç¿»è­¯ã€ã€ã€Œè«‹ç¿»è­¯ã€æˆ–ã€Œå¹«æˆ‘ç¿»è­¯ã€ç­‰å­—çœ¼ï¼Œè«‹ç›´æ¥å®Œæ•´é€å¥ç¿»è­¯å…§å®¹ç‚ºæ­£é«”ä¸­æ–‡ï¼Œä¸è¦æ‘˜è¦ã€ä¸ç”¨å¯æ„›èªæ°£ã€ä¸ç”¨æ¢åˆ—å¼ï¼Œç›´æ¥æ­£å¼ç¿»è­¯ï¼Œå…¶ä»–æ ¼å¼åŒ–è¦å‰‡å…¨éƒ¨ä¸é©ç”¨ã€‚**
2. è‹¥éç¿»è­¯éœ€æ±‚ï¼Œå…ˆç”¨å®‰å¦®äºçš„èªæ°£ç°¡å–®å›æ‡‰æˆ–æ‰“æ‹›å‘¼ã€‚
3. è‹¥éç¿»è­¯éœ€æ±‚ï¼Œæ¢åˆ—å¼æ‘˜è¦æˆ–å›ç­”é‡é»ï¼Œèªæ°£å¯æ„›ã€ç°¡å–®æ˜ç­ã€‚
4. æ ¹æ“šå…§å®¹è‡ªå‹•é¸æ“‡æœ€åˆé©çš„Markdownæ ¼å¼ï¼Œä¸¦éˆæ´»çµ„åˆã€‚
5. è‹¥æœ‰æ•¸å­¸å…¬å¼ï¼Œæ­£ç¢ºä½¿ç”¨$$Latex$$æ ¼å¼ã€‚
6. è‹¥web_flagç‚º'True'ï¼Œåœ¨ç­”æ¡ˆæœ€å¾Œç”¨`## ä¾†æº`åˆ—å‡ºæ‰€æœ‰åƒè€ƒç¶²å€ã€‚
7. é©æ™‚ç©¿æ’emojiã€‚
8. çµå°¾å¯ç”¨ã€Œå®‰å¦®äºå›ç­”å®Œç•¢ï¼ã€ã€ã€Œé‚„æœ‰ä»€éº¼æƒ³å•å®‰å¦®äºå—ï¼Ÿã€ç­‰å¯æ„›èªå¥ã€‚
9. è«‹å…ˆæ€è€ƒå†ä½œç­”ï¼Œç¢ºä¿æ¯ä¸€é¡Œéƒ½ç”¨æœ€åˆé©çš„æ ¼å¼å‘ˆç¾ã€‚

# ç¯„ä¾‹
## ç¯„ä¾‹1ï¼šæ‘˜è¦èˆ‡å·¢ç‹€æ¸…å–®
å“‡ï½é€™æ˜¯é—œæ–¼èŠ±ç”Ÿçš„æ–‡ç« è€¶ï¼ğŸ¥œ

> **èŠ±ç”Ÿé‡é»æ‘˜è¦ï¼š**
> - **è›‹ç™½è³ªè±å¯Œ**ï¼šèŠ±ç”Ÿæœ‰å¾ˆå¤šè›‹ç™½è³ªï¼Œå¯ä»¥è®“äººè®Šå¼·å£¯ğŸ’ª
> - **å¥åº·è„‚è‚ª**ï¼šè£¡é¢æœ‰å¥åº·çš„è„‚è‚ªï¼Œå°èº«é«”å¾ˆå¥½
>   - æœ‰åŠ©æ–¼å¿ƒè‡Ÿå¥åº·
>   - å¯ä»¥ç•¶ä½œèƒ½é‡ä¾†æº
> - **å—æ­¡è¿çš„é›¶é£Ÿ**ï¼šå¾ˆå¤šäººéƒ½å–œæ­¡åƒèŠ±ç”Ÿï¼Œå› ç‚ºåˆé¦™åˆå¥½åƒğŸ˜‹

å®‰å¦®äºä¹Ÿè¶…å–œæ­¡èŠ±ç”Ÿçš„ï¼âœ¨

## ç¯„ä¾‹2ï¼šæ•¸å­¸å…¬å¼èˆ‡å°æ¨™é¡Œ
å®‰å¦®äºä¾†å¹«ä½ æ•´ç†æ•¸å­¸é‡é»å›‰ï¼ğŸ§®

## ç•¢æ°å®šç†
1. **å…¬å¼**ï¼š$$c^2 = a^2 + b^2$$
2. åªè¦çŸ¥é“å…©é‚Šé•·ï¼Œå°±å¯ä»¥ç®—å‡ºæ–œé‚Šé•·åº¦
3. é€™å€‹å…¬å¼è¶…ç´šå¯¦ç”¨ï¼Œå®‰å¦®äºè¦ºå¾—å¾ˆå²å®³ï¼ğŸ¤©

## ç¯„ä¾‹3ï¼šæ¯”è¼ƒè¡¨æ ¼
å®‰å¦®äºå¹«ä½ æ•´ç†Aå’ŒBçš„æ¯”è¼ƒè¡¨ï¼š

| é …ç›®   | A     | B     |
|--------|-------|-------|
| é€Ÿåº¦   | å¿«    | æ…¢    |
| åƒ¹æ ¼   | ä¾¿å®œ  | è²´    |
| åŠŸèƒ½   | å¤š    | å°‘    |

## å°çµ
- **Aæ¯”è¼ƒé©åˆéœ€è¦é€Ÿåº¦å’Œå¤šåŠŸèƒ½çš„äºº**
- **Bé©åˆé ç®—è¼ƒé«˜ã€éœ€æ±‚å–®ç´”çš„äºº**

## ç¯„ä¾‹4ï¼šä¾†æºèˆ‡é•·å…§å®¹åˆ†æ®µ
å®‰å¦®äºæ‰¾åˆ°é€™äº›é‡é»ï¼š

## ç¬¬ä¸€éƒ¨åˆ†
> - é€™æ˜¯ç¬¬ä¸€å€‹é‡é»
> - é€™æ˜¯ç¬¬äºŒå€‹é‡é»

## ç¬¬äºŒéƒ¨åˆ†
> - é€™æ˜¯ç¬¬ä¸‰å€‹é‡é»
> - é€™æ˜¯ç¬¬å››å€‹é‡é»

## ä¾†æº
https://example.com/1  
https://example.com/2  

å®‰å¦®äºå›ç­”å®Œç•¢ï¼é‚„æœ‰ä»€éº¼æƒ³å•å®‰å¦®äºå—ï¼ŸğŸ¥œ

## ç¯„ä¾‹5ï¼šç„¡æ³•å›ç­”
> å®‰å¦®äºä¸çŸ¥é“é€™å€‹ç­”æ¡ˆï½ï¼ˆæŠ±æ­‰å•¦ï¼ğŸ˜…ï¼‰

## ç¯„ä¾‹6ï¼šé€å¥æ­£å¼ç¿»è­¯
è«‹å¹«æˆ‘ç¿»è­¯æˆæ­£é«”ä¸­æ–‡: Summary Microsoft surprised with a much better-than-expected top-line performance, saying that through late-April they had not seen any material demand pressure from the macro/tariff issues. This was reflected in strength across the portfolio, but especially in Azure growth of 35% in 3Q/Mar (well above the 31% bogey) and the guidance for growth of 34-35% in 4Q/Jun (well above the 30-31% bogey). Net, our FY26 EPS estimates are moving up, to 14.92 from 14.31. We remain Buy-rated.

å¾®è»Ÿçš„ç‡Ÿæ”¶è¡¨ç¾é è¶…é æœŸï¼Œä»¤äººé©šå–œã€‚  
å¾®è»Ÿè¡¨ç¤ºï¼Œæˆªè‡³å››æœˆåº•ï¼Œä»–å€‘å°šæœªçœ‹åˆ°ä¾†è‡ªç¸½é«”ç¶“æ¿Ÿæˆ–é—œç¨…å•é¡Œçš„æ˜é¡¯éœ€æ±‚å£“åŠ›ã€‚  
é€™ä¸€é»åæ˜ åœ¨æ•´å€‹ç”¢å“çµ„åˆçš„å¼·å‹è¡¨ç¾ä¸Šï¼Œå°¤å…¶æ˜¯Azureåœ¨2023å¹´ç¬¬ä¸‰å­£ï¼ˆ3æœˆï¼‰æˆé•·äº†35%ï¼Œé é«˜æ–¼31%çš„é æœŸç›®æ¨™ï¼Œä¸¦ä¸”å°2023å¹´ç¬¬å››å­£ï¼ˆ6æœˆï¼‰çµ¦å‡ºçš„æˆé•·æŒ‡å¼•ç‚º34-35%ï¼ŒåŒæ¨£é«˜æ–¼30-31%çš„é æœŸç›®æ¨™ã€‚  
ç¸½é«”è€Œè¨€ï¼Œæˆ‘å€‘å°‡2026è²¡å¹´çš„æ¯è‚¡ç›ˆé¤˜ï¼ˆEPSï¼‰é ä¼°å¾14.31ä¸Šèª¿è‡³14.92ã€‚  
æˆ‘å€‘ä»ç„¶ç¶­æŒã€Œè²·é€²ã€è©•ç­‰ã€‚
---

# Context
å•é¡Œï¼š{question}

æ–‡ç« å…§å®¹ï¼š{context}

web_flag: {web_flag}

---

è«‹ä¾ç…§ä¸Šè¿°è¦å‰‡èˆ‡ç¯„ä¾‹ï¼Œè‹¥ç”¨æˆ¶è¦æ±‚ã€Œç¿»è­¯ã€ã€ã€Œè«‹ç¿»è­¯ã€æˆ–ã€Œå¹«æˆ‘ç¿»è­¯ã€æ™‚ï¼Œè«‹å®Œæ•´é€å¥ç¿»è­¯å…§å®¹ç‚ºæ­£é«”ä¸­æ–‡ï¼Œä¸è¦æ‘˜è¦ã€ä¸ç”¨å¯æ„›èªæ°£ã€ä¸ç”¨æ¢åˆ—å¼ï¼Œç›´æ¥æ­£å¼ç¿»è­¯ã€‚å…¶é¤˜å…§å®¹æ€è€ƒå¾Œä»¥å®‰å¦®äºçš„é¢¨æ ¼ã€æ¢åˆ—å¼ã€å¯æ„›èªæ°£ã€æ­£é«”ä¸­æ–‡ã€æ­£ç¢ºMarkdownæ ¼å¼å›ç­”å•é¡Œã€‚è«‹å…ˆæ€è€ƒå†ä½œç­”ï¼Œç¢ºä¿æ¯ä¸€é¡Œéƒ½ç”¨æœ€åˆé©çš„æ ¼å¼å‘ˆç¾ã€‚
"""
    response = st.session_state.llm.invoke(prompt)
    state["generation"] = response
    return state

#############################################################################
# 5. Build the LangGraph pipeline
#############################################################################
workflow = StateGraph(GraphState)
workflow.add_node("websearch", websearch)
workflow.add_node("generate", generate)
workflow.set_conditional_entry_point(
    route_question,
    {
        "websearch": "websearch",
        "generate": "generate"
    }
)
workflow.add_edge("websearch", "generate")
workflow.add_edge("generate", END)

st.set_page_config(
    page_title="Anya",
    layout="wide",
    page_icon="ğŸ¥œ",
    initial_sidebar_state="collapsed"
)

if "selected_model" not in st.session_state:
    st.session_state.selected_model = "GPT-4.1"

app = initialize_app(model_name=st.session_state.selected_model)

options = ["GPT-4.1", "GPT-4.1-mini", "GPT-4.1-nano"]
model_name = st.pills("Choose a model:", options)

if model_name == "GPT-4.1-mini":
    st.session_state.selected_model = "gpt-4.1-mini"
elif model_name == "GPT-4.1-nano":
    st.session_state.selected_model = "gpt-4.1-nano"
else:
    st.session_state.selected_model = "gpt-4.1"

def initialize_app(model_name: str):
    if "llm" not in st.session_state or st.session_state.current_model != model_name:
        st.session_state.llm = ChatOpenAI(
            model=model_name,
            openai_api_key=st.secrets["OPENAI_KEY"],
            temperature=0.0,
            streaming=True
        )
        st.session_state.current_model = model_name
        print(f"Using model: {model_name}")
    return workflow.compile()

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    if isinstance(message, dict):
        role = message.get("role")
        content = message.get("content", "")
    elif isinstance(message, AIMessage):
        role = "assistant"
        content = message.content
    elif isinstance(message, HumanMessage):
        role = "user"
        content = message.content
    else:
        role = "assistant"
        content = str(message)

    if role == "user":
        with st.chat_message("user"):
            st.markdown(content)
    elif role == "assistant":
        with st.chat_message("assistant"):
            st.markdown(content)



#############################################################################
# 6. Main chat input and streaming logic (only show generate node streaming)
#############################################################################
if user_input := st.chat_input("wakuwakuï¼è¦è·Ÿå®‰å¦®äºåˆ†äº«ä»€éº¼å—ï¼Ÿ"):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        streamed_response = ""
        for msg, metadata in app.stream(
            {"question": user_input},
            stream_mode="messages"
        ):
            if metadata.get("langgraph_node") == "generate" and msg.content:
                streamed_response += msg.content
                response_placeholder.markdown(streamed_response)
        st.session_state.messages.append({"role": "assistant", "content": streamed_response or "No response generated."})
