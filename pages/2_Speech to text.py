# -*- coding: utf-8 -*-
# æœƒè­°éŒ„éŸ³ â†’ ç›´æ’­é€å­—ï¼‹æ‘˜è¦ï¼ˆæœ€çµ‚æ•´åˆç‰ˆï¼šä¸¦è¡Œæ½¤é£¾ï¼‹å³æ™‚ Mapï¼‰
# ç‰¹è‰²ï¼š
# - STTï¼ˆProducerï¼‰â†’ã€Œæ½¤é£¾ã€RefineConsumer èˆ‡ã€Œå³æ™‚ Map æ‘˜è¦ã€SummarizerConsumer ä¸¦è¡Œ
# - èƒŒæ™¯åŸ·è¡Œç·’æ”¯æ´ ScriptRunContextï¼ˆå¯å®‰å…¨å¯« UIï¼‰ï¼Œå¤±æ•—è‡ªå‹•å›é€€ç‚ºã€Œä¸å¯« UIã€
# - ç›¸é„°è¦–çª—å»é‡ï¼ˆJaccard trigramï¼‰å–ä»£å¤§é‡ difflibï¼Œé€Ÿåº¦æ›´ç©©
# - åƒæ•¸å¯è¦–åŒ–ï¼ˆçœ‹åˆ°å¡è»Š/æƒ³çœæˆæœ¬æ™‚æ€éº¼èª¿ï¼Œéƒ½å¯«åœ¨ UI æ»‘æ¡¿æ—ï¼‰
# - æœ€å¾Œç”¨ Reduce ç”¢å‡ºæ­£å¼é•·æ–‡èˆ‡çµæ§‹åŒ– JSON

import os
import re
import json
import hashlib
import tempfile
import multiprocessing
from typing import List, Dict, Any

import streamlit as st
from openai import OpenAI
from pydub import AudioSegment, silence
from pydub.utils import which

from queue import Queue, Empty
from threading import Thread, Lock
import time
import concurrent.futures

# å˜—è©¦è¼‰å…¥ Streamlit çš„ ScriptRunContext å·¥å…·ï¼ˆå…è¨±åœ¨å­åŸ·è¡Œç·’å®‰å…¨æ›´æ–° UIï¼‰
try:
    from streamlit.runtime.scriptrunner import get_script_run_ctx, add_script_run_ctx
except Exception:
    get_script_run_ctx = None
    add_script_run_ctx = None

# ========== åŸºæœ¬è¨­å®š ==========
st.set_page_config(page_title="æœƒè­°éŒ„éŸ³ â†’ ç›´æ’­é€å­—ï¼‹æ‘˜è¦", page_icon="ğŸ“", layout="wide")

# è‡ªè¨‚æ¨£å¼
st.markdown("""
<style>
:root { --brand:#9c2b2f; --brand-weak:#9c2b2fcc; --bg:#FFF6F6; --border:#f2d9d9; }
.main .block-container{padding-top:2.2rem}
.pink-card{background:var(--bg);border:1px solid var(--border);padding:16px 22px;border-radius:12px;margin-bottom:12px;overflow:visible;}
.header-pill{display:flex;align-items:center;gap:12px;font-size:22px;font-weight:700;color:#2f2f2f;line-height:1.35;min-height:48px;}
.header-pill .emoji{font-size:22px;display:inline-block;transform:translateY(1px);}
.stTabs [data-baseweb="tab-list"]{gap:24px;border-bottom:1px solid #f0e2e2;margin-bottom:8px}
.stTabs [data-baseweb="tab"]{padding:10px 2px;color:var(--brand-weak);font-weight:600}
.stTabs [aria-selected="true"]{color:var(--brand);border-bottom:3px solid var(--brand)}
.stMarkdown p{line-height:1.8}
.transcript-readable{font-size:1.02rem;line-height:1.9;letter-spacing:0.02em;}
</style>
""", unsafe_allow_html=True)

# é ‚éƒ¨å¡ç‰‡æ¨™é¡Œ
st.markdown('<div class="pink-card header-pill"><span class="emoji">âœï¸</span> å®‰å¦®äºé–‹æœƒä¸æ¼æ¥ï¼šé€å­— Ã— æ‘˜è¦</div>', unsafe_allow_html=True)

# æª¢æŸ¥ FFmpeg
AudioSegment.converter = which("ffmpeg")
AudioSegment.ffprobe = which("ffprobe")
if not AudioSegment.converter or not AudioSegment.ffprobe:
    st.error("æ‰¾ä¸åˆ° ffmpeg/ffprobeï¼Œè«‹å…ˆæ–¼ç³»çµ±å®‰è£å¾Œå†è©¦ã€‚")
    st.stop()

# è®€å– API Key
OPENAI_KEY = st.secrets.get("OPENAI_KEY", os.getenv("OPENAI_API_KEY"))
if not OPENAI_KEY:
    st.error("æ‰¾ä¸åˆ° API Keyï¼Œè«‹åœ¨ Streamlit Secrets è¨­å®š OPENAI_KEY æˆ–ç’°å¢ƒè®Šæ•¸ OPENAI_API_KEYã€‚")
    st.stop()

client = OpenAI(api_key=OPENAI_KEY)

# ========== åƒæ•¸ ==========
MODEL_STT = "gpt-4o-mini-transcribe"  # STT å¿ å¯¦è½‰éŒ„åŸèªè¨€ï¼ˆç…§ä½ åŸå…ˆè¨­å®šï¼‰
MODEL_MAP = "gpt-5-mini"              # åˆ†æ®µæ‘˜è¦
MODEL_REDUCE = "gpt-4.1"              # ç¸½æ•´/æ½¤é£¾

# åˆ‡æ®µåƒæ•¸
MIN_SILENCE_LEN_MS = 700
KEEP_SILENCE_MS = 300
SILENCE_DB_OFFSET = 16
OVERLAP_MS = 1200

# ç‰‡æ®µé•·åº¦ä¿è­·èˆ‡å›é€€
MAX_CHUNK_MS = 30_000    # å–®æ®µæœ€é•· 30 ç§’
MIN_CHUNK_MS = 2_000     # å–®æ®µæœ€çŸ­ 2 ç§’
FALLBACK_WINDOW_MS = 20_000  # æ‰¾ä¸åˆ°éœéŸ³æ™‚ï¼Œå›ºå®šåˆ‡ 20 ç§’

DEFAULT_MAP_CHUNK_SIZE = 40

# é è¨­å·¥äººæ•¸ï¼ˆæ½¤é£¾/Map å…©é‚Šå„è‡ªçš„æœ€å¤§åŒæ™‚æ‰¹æ¬¡ï¼Œä¿å®ˆå¾ 1 èµ·ï¼‰
MAX_STREAM_WORKERS = min(2, multiprocessing.cpu_count())

CACHE_DIR = ".stt_cache"
os.makedirs(CACHE_DIR, exist_ok=True)

# ========== å·¥å…·å‡½å¼ ==========
def _hash_bytes(b: bytes) -> str:
    return hashlib.md5(b).hexdigest()

def cache_get_text(key: str) -> str | None:
    path = os.path.join(CACHE_DIR, key + ".txt")
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    return None

def cache_set_text(key: str, value: str):
    path = os.path.join(CACHE_DIR, key + ".txt")
    with open(path, "w", encoding="utf-8") as f:
        f.write(value)

def convert_to_wav(input_path: str, output_path: str, target_sr=16000):
    audio = AudioSegment.from_file(input_path)
    audio = audio.set_frame_rate(target_sr).set_channels(1)
    audio.export(output_path, format="wav")
    return output_path

def normalize_loudness(audio: AudioSegment, target_dbfs: float = -20.0) -> AudioSegment:
    gain = target_dbfs - audio.dBFS
    return audio.apply_gain(gain)

def trim_leading_silence(audio: AudioSegment, silence_threshold_db: float = -30.0, chunk_ms: int = 10) -> AudioSegment:
    trim_ms = 0
    while trim_ms < len(audio) and audio[trim_ms:trim_ms+chunk_ms].dBFS < silence_threshold_db:
        trim_ms += chunk_ms
    return audio[trim_ms:]

def apply_filters(audio: AudioSegment, use_high_pass: bool = False, hp_hz: int = 100,
                  use_low_pass: bool = False, lp_hz: int = 9500) -> AudioSegment:
    out = audio
    if use_high_pass:
        out = out.high_pass_filter(hp_hz)
    if use_low_pass:
        out = out.low_pass_filter(lp_hz)
    return out

def split_audio_on_silence_safe(audio: AudioSegment) -> List[AudioSegment]:
    silence_thresh = audio.dBFS - SILENCE_DB_OFFSET
    raw_chunks = silence.split_on_silence(
        audio,
        min_silence_len=MIN_SILENCE_LEN_MS,
        silence_thresh=silence_thresh,
        keep_silence=KEEP_SILENCE_MS
    )

    if not raw_chunks:
        chunks = []
        i = 0
        while i < len(audio):
            end = min(i + FALLBACK_WINDOW_MS, len(audio))
            chunks.append(audio[i:end])
            i = end
    else:
        filtered = []
        for c in raw_chunks:
            if len(c) < 250:
                if filtered:
                    filtered[-1] = filtered[-1] + c
                else:
                    filtered.append(c)
            else:
                filtered.append(c)
        if not filtered:
            filtered = raw_chunks

        chunks = []
        for i, c in enumerate(filtered):
            if i == 0:
                chunks.append(c)
            else:
                prev = filtered[i - 1]
                safe_overlap = min(OVERLAP_MS, len(prev))
                if safe_overlap > 0:
                    overlap = prev[-safe_overlap:]
                    chunks.append(overlap + c)
                else:
                    chunks.append(c)

    normalized = []
    for seg in chunks:
        if len(seg) <= MAX_CHUNK_MS:
            normalized.append(seg)
        else:
            start = 0
            while start < len(seg):
                end = min(start + MAX_CHUNK_MS, len(seg))
                normalized.append(seg[start:end])
                start = end

    final_chunks = []
    for seg in normalized:
        if final_chunks and len(seg) < MIN_CHUNK_MS:
            final_chunks[-1] = final_chunks[-1] + seg
        else:
            final_chunks.append(seg)

    return final_chunks

def split_sentences(text: str) -> List[str]:
    parts = re.split(r'([ã€‚ï¼ï¼Ÿï¼›;.!?\n])', text)
    result = []
    for i in range(0, len(parts) - 1, 2):
        s = (parts[i] + parts[i + 1]).strip()
        if s:
            result.append(s)
    if len(parts) % 2 != 0:
        tail = parts[-1].strip()
        if tail:
            result.append(tail)
    return result

# ====== é«˜æ•ˆå»é‡è¼”åŠ©ï¼ˆç›¸é„°è¦–çª— + Jaccard trigramï¼‰======
def _norm_for_dedupe(s: str) -> str:
    s = s.strip().lower()
    s = re.sub(r'\s+', '', s)
    s = (s.replace('ï¼Œ', ',').replace('ã€‚', '.')
           .replace('ï¼', '!').replace('ï¼Ÿ', '?')
           .replace('ï¼›', ';').replace('ï¼ˆ', '(').replace('ï¼‰', ')'))
    return s

def _jaccard_trigram(a: str, b: str) -> float:
    n = 3
    if len(a) < n or len(b) < n:
        return 1.0 if a == b else 0.0
    A = {a[i:i+n] for i in range(len(a)-n+1)}
    B = {b[i:i+n] for i in range(len(b)-n+1)}
    un = len(A | B)
    return (len(A & B) / un) if un else 0.0

def dedupe_against_prev_fast(curr: List[str], prev: List[str],
                             threshold: float = 0.88, max_prev: int = 12) -> List[str]:
    if not curr:
        return []
    tail = prev[-max_prev:] if prev else []
    tail_norm = [_norm_for_dedupe(p) for p in tail]

    out: List[str] = []
    for s in curr:
        ns = _norm_for_dedupe(s)
        if ns in tail_norm:
            continue
        similar = False
        for pn in tail_norm:
            if not pn:
                continue
            if abs(len(ns) - len(pn)) > int(max(len(ns), len(pn)) * 0.4):
                continue
            if _jaccard_trigram(ns, pn) >= threshold:
                similar = True
                break
        if not similar:
            out.append(s)
    return out

def add_cjk_spacing(text: str) -> str:
    text = re.sub(r'([\u4e00-\u9fff])([A-Za-z0-9%#@&])', r'\1 \2', text)
    text = re.sub(r'([A-Za-z0-9%#@&])([\u4e00-\u9fff])', r'\1 \2', text)
    return text

def normalize_symbols(text: str) -> str:
    text = text.replace("ï¼…", "%").replace("ï¼„", "$")
    text = text.replace("â€“", "-").replace("â€”", "-")
    text = text.replace("\u200b", "").replace("\u200c", "")
    return text

def pretty_format_sentences(sentences: List[str]) -> List[str]:
    pretty = []
    for s in sentences:
        s2 = add_cjk_spacing(s)
        s2 = normalize_symbols(s2)
        pretty.append(s2)
    return pretty

# é¡¯ç¤ºå±¤ï¼šé€è¡Œã€æ½¤é£¾ï¼‹å¿…è¦æ™‚ç¿»è­¯ã€ç‚ºæ­£é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰ï¼Œç©©å®šç‰ˆï¼ˆæ‰¹æ¬¡ï¼‹åˆ†éš”ç¬¦ï¼‰
def refine_zh_tw_via_prompt(lines: List[str]) -> List[str]:
    if not lines:
        return lines
    SEP = "\u241E"  # å¯è¦–åˆ†éš”ç¬¦ â
    MAX_BATCH_CHARS = 9000
    MAX_BATCH_LINES = 120

    def _refine_batch(batch: List[str]) -> List[str]:
        blob = SEP.join(batch)
        dev_msg = (
            "ä½ å°‡æ”¶åˆ°å¤šè¡Œé€å­—ç¨¿ï¼Œè«‹é€è¡Œã€æ½¤é£¾ï¼‹å¿…è¦æ™‚ç¿»è­¯ã€ç‚ºæ­£é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰ã€‚\n"
            "è¦æ±‚ï¼š\n"
            "1) ä¿ç•™åŸæ„ï¼Œåªåšèªå¥æ½¤é£¾èˆ‡æ­£é«”ç¿»è­¯ï¼Œä¸å¾—æé€ è³‡è¨Šã€‚\n"
            "2) è‹¥è©²è¡Œæ˜¯è‹±æ–‡æˆ–æ··é›œèªè¨€ï¼Œç¿»è­¯ç‚ºæ­£é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰ã€‚\n"
            "3) åš´ç¦åˆä½µ/æ‹†åˆ†è¡Œï¼›åš´ç¦æ’å…¥æˆ–åˆªé™¤ç©ºè¡Œï¼›è¼¸å…¥å¹¾è¡Œå°±è¼¸å‡ºå¹¾è¡Œã€‚\n"
            "4) ä¿ç•™æ•¸å­—ã€å–®ä½ã€æ™‚é–“ã€é‡‘é¡ã€emojiã€ç¶²å€ã€ç°¡çŸ­ä»£ç¢¼ç‰‡æ®µç­‰éèªæ„å…§å®¹ã€‚\n"
            "5) ç”¨è©æ¡å°ç£æ…£ç”¨ã€å£å»ç°¡æ½”å°ˆæ¥­è‡ªç„¶ã€‚\n"
            "6) è¡Œèˆ‡è¡Œç”±ç‰¹æ®Šåˆ†éš”ç¬¦ âï¼ˆU+241Eï¼‰é€£æ¥ï¼›è«‹å‹™å¿…ä¿ç•™ç›¸åŒæ•¸é‡çš„åˆ†éš”ç¬¦ï¼Œä¸å¯æ–°å¢æˆ–ç§»é™¤ã€‚\n"
            "åªè¼¸å‡ºæœ€çµ‚æ–‡æœ¬ï¼Œä¸è¦ä»»ä½•è§£é‡‹ã€‚"
        )
        try:
            resp = client.responses.create(
                model=MODEL_REDUCE,
                input=[
                    {"role": "developer", "content": [{"type": "input_text", "text": dev_msg}]},
                    {"role": "user", "content": [{"type": "input_text", "text": blob}]},
                ],
                text={"format": {"type": "text"}},
                tools=[],
            )
            out = (resp.output_text or "").rstrip("\n")
            out_lines = out.split(SEP) if SEP in out else out.split("\n")
            return out_lines if len(out_lines) == len(batch) else batch
        except Exception:
            return batch

    refined_all: List[str] = []
    batch: List[str] = []
    size = 0
    for s in lines:
        if (len(batch) >= MAX_BATCH_LINES) or (size + len(s) + 1 > MAX_BATCH_CHARS):
            refined_all.extend(_refine_batch(batch))
            batch, size = [], 0
        batch.append(s)
        size += len(s) + 1
    if batch:
        refined_all.extend(_refine_batch(batch))
    return refined_all if refined_all else lines

# ========== Map-Reduceï¼ˆGPTâ€‘5 + Responses APIï¼‰==========
def map_summarize_blocks(flat_sentences: List[str], chunk_size=DEFAULT_MAP_CHUNK_SIZE) -> List[str]:
    blocks = []
    for idx in range(0, len(flat_sentences), chunk_size):
        part = flat_sentences[idx: idx + chunk_size]
        dev_msg = (
            "ä½ æ˜¯ä¸€ä½æœƒè­°è¨˜éŒ„å°å¹«æ‰‹ï¼Œè«‹å°‡ä¸‹åˆ—é€å­—ç¨¿æ•´ç†ç‚ºæ¢åˆ—å¼é‡é»ï¼ˆç¹é«”ä¸­æ–‡ï¼‰ã€‚"
            "è¦æ±‚ï¼šæ¯é»å…·é«”ã€é¿å…ç©ºæ³›ï¼›è‹¥æœ‰æ±ºç­–/é¢¨éšª/æœªæ±ºå•é¡Œ/è¡Œå‹•é …ç›®è«‹æ¸…æ¥šæ¨™è¨˜ï¼›"
            "åªè¼¸å‡ºæ¢åˆ—é‡é»ï¼Œä¸è¦é¡å¤–èªªæ˜ã€‚"
        )
        user_msg = "\n".join(part)
        try:
            resp = client.responses.create(
                model=MODEL_MAP,
                input=[
                    {"role": "developer", "content": [{"type": "input_text", "text": dev_msg}]},
                    {"role": "user", "content": [{"type": "input_text", "text": user_msg}]},
                ],
                text={"format": {"type": "text"}},
                tools=[],
            )
            content = resp.output_text or ""
            blocks.append(content.strip())
        except Exception as e:
            blocks.append(f"ã€API æ‘˜è¦å¤±æ•—ï¼š{e}ã€‘")
    return blocks

def reduce_finalize_json(map_blocks: List[str]) -> Dict[str, Any]:
    dev_msg = (
        "ä½ æ˜¯æœƒè­°è¨˜éŒ„ç¸½æ•´å°ˆå®¶ã€‚è«‹å°‡å¤šå€‹åˆ†æ®µæ‘˜è¦åˆä½µæˆçµæ§‹åŒ– JSONï¼ŒåŒ…å«ï¼š\n"
        "- metadata: {title, date, location, participants[], duration}\n"
        "- topics[]: {title, key_points[], decisions[], risks[], open_questions[]}\n"
        "- decisions[]\n"
        "- risks[]\n"
        "- open_questions[]\n"
        "- action_items[]: {description, owner|null, due_date|null, priority|null (P0~P3), status, source_refs[]}\n"
        "- overall_summary: string\n"
        "è¦æ±‚ï¼š\n"
        "1) åš´ç¦æé€ ä¾†æºæ²’æœ‰çš„è³‡è¨Šï¼›æœªçŸ¥æ¬„ä½è«‹ç•™ç©ºæˆ– Unknownã€‚\n"
        "2) å»é‡ã€åˆä½µç›¸è¿‘é‡é»ï¼Œä½†ä¸å¾—æ”¹è®ŠåŸæ„ã€‚\n"
        "3) åªè¼¸å‡º JSON ç‰©ä»¶ï¼Œä¸è¦é¡å¤–èªªæ˜æ–‡å­—ã€‚\n"
        "4) ç¢ºä¿ç‚ºåˆæ³• JSONã€‚\n\n"
        "=== åˆ†æ®µæ‘˜è¦ ===\n"
        + "\n\n".join(f"[Part {i+1}]\n{blk}" for i, blk in enumerate(map_blocks))
    )
    try:
        resp = client.responses.create(
            model=MODEL_REDUCE,
            input=[{"role": "developer", "content": [{"type": "input_text", "text": dev_msg}]}],
            text={"format": {"type": "text"}},
            tools=[],
        )
        s = (resp.output_text or "").strip()
        start = s.find("{")
        end = s.rfind("}")
        if start != -1 and end != -1:
            s = s[start:end+1]
            return json.loads(s)
    except Exception as e:
        return {"overall_summary": f"è§£æ JSON å¤±æ•—ï¼Œè«‹é‡è©¦æˆ–èª¿æ•´æç¤ºã€‚éŒ¯èª¤ï¼š{e}", "raw": ""}
    return {"overall_summary": "è§£æ JSON å¤±æ•—ï¼ˆæœªçŸ¥åŸå› ï¼‰", "raw": ""}

def reduce_finalize_markdown(map_blocks: List[str]) -> str:
    dev_msg = (
        "ä½ æ˜¯æœƒè­°è¨˜éŒ„ç¸½æ•´å°ˆå®¶ã€‚è«‹å°‡å¤šå€‹åˆ†æ®µæ‘˜è¦æ•´ä½µç‚ºã€å–®ä¸€ä»½æœ€çµ‚æœƒè­°ç´€éŒ„ï¼ˆMarkdownï¼‰ã€ã€‚\n"
        "è¦æ±‚ï¼š\n"
        "1) åƒ…æ ¹æ“šæä¾›çš„åˆ†æ®µæ‘˜è¦æ•´ä½µï¼Œåš´ç¦æé€ ä¾†æºæ²’æœ‰çš„è³‡è¨Šã€‚\n"
        "2) ä¸è¼¸å‡º metadataï¼ˆæ¨™é¡Œ/æ—¥æœŸ/åœ°é»/åƒèˆ‡è€…/æ™‚é•·ï¼‰ï¼Œåªè¦å…§å®¹æœ¬é«”ã€‚\n"
        "3) çµæ§‹ï¼š\n"
        " - ä»¥ä¸€æ®µã€Œç¸½çµã€é–‹å ´ï¼Œ3~6 å¥ï¼Œèªªæ¸…æ¥šæ•´é«”è„ˆçµ¡èˆ‡çµè«–ã€‚\n"
        " - ä¹‹å¾Œç”¨å¤šå€‹å°ç¯€ï¼ˆ## ä¸»é¡Œåç¨±ï¼‰ï¼Œæ¯ç¯€æ¡ç”¨çŸ­æ®µè½æ•˜è¿°ç‚ºä¸»ï¼Œå¯ç©¿æ’å°‘é‡æ¢åˆ—ã€‚\n"
        " - è‹¥æœ‰æ±ºç­–/é¢¨éšª/æœªæ±ºå•é¡Œï¼Œæ–¼å°æ‡‰ä¸»é¡Œå…§ä»¥ã€æ±ºç­–ï¼šã€ã€æœªæ±ºï¼šã€ã€é¢¨éšªï¼šã€æ¨™ç¤ºã€‚\n"
        "4) åªè¼¸å‡ºç´” Markdown å…§å®¹ï¼Œä¸è¦é¡å¤–èªªæ˜ã€‚\n\n"
        "=== åˆ†æ®µæ‘˜è¦ ===\n"
        + "\n\n".join(f"[Part {i+1}]\n{blk}" for i, blk in enumerate(map_blocks))
    )
    try:
        resp = client.responses.create(
            model=MODEL_REDUCE,
            input=[{"role": "developer", "content": [{"type": "input_text", "text": dev_msg}]}],
            text={"format": {"type": "text"}},
            tools=[],
        )
        return (resp.output_text or "").strip()
    except Exception as e:
        return f"âš ï¸ ç”Ÿæˆæœƒè­°æ‘˜è¦å¤±æ•—ï¼š{e}"

# ========== ä¸¦è¡Œæ½¤é£¾ Consumer ==========
# 45~60 åˆ†é˜å»ºè­°å€¼ï¼ˆå¯åœ¨ UI slider å¾®èª¿ï¼‰
REFINE_MAX_LINES = 80     # è¶Šå°è¶Šå³æ™‚ï¼ˆAPI æ¬¡æ•¸â†‘/æˆæœ¬â†‘ï¼‰ï¼›80~100 å¸¸è¦‹
REFINE_MAX_CHARS = 6000   # 6000~9000ï¼›è¶Šå¤§è¶Šçœæˆæœ¬ä½†å›é¥‹æ…¢
REFINE_MAX_WAIT_S = 0.35  # 0.25~0.45ï¼›å¡è»Šå¯èª¿å°ï¼Œçœæˆæœ¬å¯èª¿å¤§

class RefineConsumer:
    def __init__(self, stream_container, progress_bar, workers: int = 1):
        self.q: Queue = Queue(maxsize=6)  # é©åº¦èƒŒå£“
        self.stream_container = stream_container
        self.progress_bar = progress_bar
        self.grouped_sentences: List[List[str]] = []
        self.refined_lines_all: List[str] = []
        self.unique_sentences_raw_all: List[str] = []  # ä¹Ÿç•™å­˜çµ¦æ‘˜è¦ç”¨
        self._stop = False
        self._total = 0
        self._done = 0
        self.lock = Lock()

        self.workers = max(1, int(workers))
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=self.workers)
        self.batch_id = 0
        self.next_emit_id = 0
        self.pending: Dict[int, concurrent.futures.Future] = {}
        self.batch_buffer: Dict[int, List[str]] = {}

        # UI å®‰å…¨å›é€€
        self.fallback_no_ui = False

    def set_total(self, n: int):
        self._total = n

    def put(self, item):
        self.q.put(item)

    def stop(self):
        self._stop = True
        self.q.put(None)

    def _safe_progress(self, ratio: float):
        if self.fallback_no_ui:
            return
        try:
            self.progress_bar.progress(ratio)
        except Exception:
            self.fallback_no_ui = True

    def _safe_render_all(self):
        if self.fallback_no_ui:
            return
        try:
            existing = "\n\n".join(group_into_paragraphs(self.refined_lines_all, max_chars=280, max_sents=4))
            self.stream_container.markdown(existing)
        except Exception:
            self.fallback_no_ui = True

    def _submit_batch(self, batch_lines: List[str], bid: int):
        def _task(lines: List[str]) -> List[str]:
            try:
                return refine_zh_tw_via_prompt(lines)
            except Exception:
                return lines
        fut = self.executor.submit(_task, batch_lines[:])
        self.pending[bid] = fut

    def _emit_ready(self):
        emitted_any = False
        while self.next_emit_id in self.pending and self.pending[self.next_emit_id].done():
            fut = self.pending.pop(self.next_emit_id)
            try:
                refined = fut.result()
            except Exception:
                refined = self.batch_buffer.get(self.next_emit_id, [])
            self.refined_lines_all.extend(refined)
            self._safe_render_all()
            self.batch_buffer.pop(self.next_emit_id, None)
            self.next_emit_id += 1
            emitted_any = True
        return emitted_any

    def run(self):
        batch_lines: List[str] = []
        batch_chars = 0
        last_flush = time.time()

        while True:
            try:
                item = self.q.get(timeout=0.2)
            except Empty:
                item = None

            now = time.time()
            timeup = (now - last_flush) >= REFINE_MAX_WAIT_S

            if item is None:
                if timeup and batch_lines:
                    bid = self.batch_id
                    self.batch_buffer[bid] = batch_lines[:]
                    self._submit_batch(batch_lines, bid)
                    self.batch_id += 1
                    batch_lines, batch_chars = [], 0
                    last_flush = now
                if self._stop:
                    if batch_lines:
                        bid = self.batch_id
                        self.batch_buffer[bid] = batch_lines[:]
                        self._submit_batch(batch_lines, bid)
                        self.batch_id += 1
                        batch_lines, batch_chars = [], 0
                    while self.pending:
                        self._emit_ready()
                        time.sleep(0.05)
                    break
                else:
                    self._emit_ready()
                continue

            sents = item
            unique = sents if not self.grouped_sentences else dedupe_against_prev_fast(
                sents, self.grouped_sentences[-1], threshold=0.88, max_prev=12
            )
            self.grouped_sentences.append(unique)
            flat = pretty_format_sentences(unique)
            self.unique_sentences_raw_all.extend(flat)

            flushed = False
            for s in flat:
                if not s.strip():
                    continue
                if (len(batch_lines) >= REFINE_MAX_LINES) or (batch_chars + len(s) > REFINE_MAX_CHARS) or timeup:
                    bid = self.batch_id
                    self.batch_buffer[bid] = batch_lines[:]
                    self._submit_batch(batch_lines, bid)
                    self.batch_id += 1
                    batch_lines, batch_chars = [], 0
                    last_flush = now
                    flushed = True
                batch_lines.append(s)
                batch_chars += len(s)

            self._done += 1
            if self._total:
                self._safe_progress(min(1.0, self._done / self._total))
            if flushed:
                self._emit_ready()

        self.executor.shutdown(wait=True)

# ========== ä¸¦è¡Œ Map æ‘˜è¦ Consumer ==========
MAP_MAX_LINES = 30       # å³æ™‚æ€§â†‘ï¼šèª¿å°åˆ° 24ï¼›çœæˆæœ¬â†‘ï¼šèª¿åˆ° 40~60
MAP_MAX_CHARS = 4000     # 3000~6000ï¼›è¶Šå¤§è¶Šçœæˆæœ¬
MAP_MAX_WAIT_S = 0.35    # 0.25~0.45ï¼›å¡è»Šå¯èª¿å°
MAP_WORKERS = 1          # å…ˆ 1ï¼›æ…¢å†é–‹åˆ° 2ï¼ˆé †åºå·²ç¶­æŒï¼Œæˆæœ¬â†‘ï¼‰

class SummarizerConsumer:
    def __init__(self, map_container, map_progress, workers: int = 1):
        self.q: Queue = Queue(maxsize=6)
        self.map_container = map_container
        self.map_progress = map_progress
        self._stop = False
        self._total = 0
        self._done = 0

        self.blocks: List[str] = []    # å³æ™‚ Map å€å¡Šï¼ˆMarkdownï¼‰
        self.workers = max(1, int(workers))
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=self.workers)
        self.batch_id = 0
        self.next_emit_id = 0
        self.pending: Dict[int, concurrent.futures.Future] = {}
        self.batch_buffer: Dict[int, List[str]] = {}

        self.fallback_no_ui = False

    def set_total(self, n: int):
        self._total = n

    def put(self, sents: List[str]):
        self.q.put(sents)

    def stop(self):
        self._stop = True
        self.q.put(None)

    def _safe_progress(self, ratio: float):
        if self.fallback_no_ui:
            return
        try:
            self.map_progress.progress(ratio)
        except Exception:
            self.fallback_no_ui = True

    def _safe_render_all(self):
        if self.fallback_no_ui:
            return
        try:
            self.map_container.markdown("\n\n".join(self.blocks))
        except Exception:
            self.fallback_no_ui = True

    def _submit_batch(self, batch_lines: List[str], bid: int):
        def _task(lines: List[str]) -> str:
            part = "\n".join(lines)
            dev_msg = (
                "ä½ æ˜¯ä¸€ä½æœƒè­°è¨˜éŒ„å°å¹«æ‰‹ï¼Œè«‹å°‡ä¸‹åˆ—é€å­—ç¨¿æ•´ç†ç‚ºæ¢åˆ—å¼é‡é»ï¼ˆç¹é«”ä¸­æ–‡ï¼‰ã€‚"
                "è¦æ±‚ï¼šæ¯é»å…·é«”ã€é¿å…ç©ºæ³›ï¼›è‹¥æœ‰æ±ºç­–/é¢¨éšª/æœªæ±ºå•é¡Œ/è¡Œå‹•é …ç›®è«‹æ¸…æ¥šæ¨™è¨˜ï¼›"
                "åªè¼¸å‡ºæ¢åˆ—é‡é»ï¼Œä¸è¦é¡å¤–èªªæ˜ã€‚"
            )
            try:
                resp = client.responses.create(
                    model=MODEL_MAP,
                    input=[
                        {"role": "developer", "content": [{"type": "input_text", "text": dev_msg}]},
                        {"role": "user", "content": [{"type": "input_text", "text": part}]},
                    ],
                    text={"format": {"type": "text"}},
                    tools=[],
                )
                content = (resp.output_text or "").strip()
                return f"### å³æ™‚é‡é» Part {bid+1}\n\n" + content
            except Exception as e:
                return f"### å³æ™‚é‡é» Part {bid+1}\n\n- ã€API æ‘˜è¦å¤±æ•—ï¼š{e}ã€‘"
        fut = self.executor.submit(_task, batch_lines[:])
        self.pending[bid] = fut

    def _emit_ready(self):
        emitted = False
        while self.next_emit_id in self.pending and self.pending[self.next_emit_id].done():
            fut = self.pending.pop(self.next_emit_id)
            try:
                md = fut.result()
            except Exception:
                md = "ï¼ˆæœ¬æ‰¹æ‘˜è¦å›å‚³å¤±æ•—ï¼‰"
            self.blocks.append(md)
            self._safe_render_all()
            self.batch_buffer.pop(self.next_emit_id, None)
            self.next_emit_id += 1
            emitted = True
        return emitted

    def run(self):
        batch_lines: List[str] = []
        batch_chars = 0
        last_flush = time.time()

        while True:
            try:
                item = self.q.get(timeout=0.2)
            except Empty:
                item = None

            now = time.time()
            timeup = (now - last_flush) >= MAP_MAX_WAIT_S

            if item is None:
                if timeup and batch_lines:
                    bid = self.batch_id
                    self.batch_buffer[bid] = batch_lines[:]
                    self._submit_batch(batch_lines, bid)
                    self.batch_id += 1
                    batch_lines, batch_chars = [], 0
                    last_flush = now
                if self._stop:
                    if batch_lines:
                        bid = self.batch_id
                        self.batch_buffer[bid] = batch_lines[:]
                        self._submit_batch(batch_lines, bid)
                        self.batch_id += 1
                        batch_lines, batch_chars = [], 0
                    while self.pending:
                        self._emit_ready()
                        time.sleep(0.05)
                    break
                else:
                    self._emit_ready()
                continue

            lines = pretty_format_sentences(item)
            for s in lines:
                if not s.strip():
                    continue
                if (len(batch_lines) >= MAP_MAX_LINES) or (batch_chars + len(s) > MAP_MAX_CHARS) or timeup:
                    bid = self.batch_id
                    self.batch_buffer[bid] = batch_lines[:]
                    self._submit_batch(batch_lines, bid)
                    self.batch_id += 1
                    batch_lines, batch_chars = [], 0
                    last_flush = now
                batch_lines.append(s)
                batch_chars += len(s)

            self._done += 1
            if self._total:
                self._safe_progress(min(1.0, self._done / self._total))

        self.executor.shutdown(wait=True)

# ========== STT Producer ==========
def build_prompt(prev_text: str, glossary: str, style_seed: str, max_tokens: int = 220) -> str:
    parts = []
    parts.append("è«‹å…¨ç¨‹ä½¿ç”¨æ­£é«”ä¸­æ–‡ï¼ˆç¹é«”ï¼Œå°ç£ç”¨èªï¼‰ã€‚")
    if style_seed and style_seed.strip():
        parts.append(style_seed.strip())
    if glossary and glossary.strip():
        words = [w.strip() for w in glossary.splitlines() if w.strip()]
        if words:
            parts.append("Glossary: " + ", ".join(words))
    if prev_text and prev_text.strip():
        tail = prev_text.strip()
        if len(tail) > 1200:
            tail = tail[-1200:]
        parts.append(tail)

    prompt = "\n".join(parts).strip()
    toks = prompt.split()
    if len(toks) > max_tokens:
        prompt = " ".join(toks[-max_tokens:])
    return prompt

def stream_transcribe_all(
    chunks: List[AudioSegment],
    container,
    progress_bar,
    use_prompting: bool = False,
    glossary: str = "",
    style_seed: str = "",
    on_sentences=None  # å›å‘¼ï¼šæŠŠæœ¬æ®µå¥å­åˆ†æµçµ¦æ¶ˆè²»è€…
):
    all_text = ""
    rolling_context = ""
    last_flush = 0.0
    FLUSH_INTERVAL = 0.15

    for i, chunk in enumerate(chunks):
        chunk_hash = _hash_bytes(chunk.raw_data)
        cache_key = f"stt_{MODEL_STT}_{chunk_hash}"
        cached = cache_get_text(cache_key)
        if cached:
            all_text += cached + "\n"
            rolling_context = (rolling_context + " " + cached).strip()
            if len(rolling_context) > 5000:
                rolling_context = rolling_context[-5000:]
            progress_bar.progress((i + 1) / len(chunks))
            container.markdown(all_text)
            if on_sentences:
                on_sentences(split_sentences(cached))
            continue

        full_text = ""
        tmp_path = None
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                tmp_path = tmp.name
                chunk.export(tmp_path, format="wav", parameters=["-ac", "1", "-ar", "16000"])
            with open(tmp_path, "rb") as audio_file:
                extra_kwargs = {}
                if use_prompting:
                    prompt_str = build_prompt(rolling_context, glossary, style_seed, max_tokens=220)
                    if prompt_str:
                        extra_kwargs["prompt"] = prompt_str
                try:
                    stream = client.audio.transcriptions.create(
                        model=MODEL_STT,
                        file=audio_file,
                        response_format="text",
                        prompt=(
                            "This audio contains a discussion or presentation. Always preserve the original language "
                            "of each sentence. If a sentence is in English, output it in English; if in Chinese, output it "
                            "in Traditional Chinese; if mixed, output the original mixed-language sentence. Do not translate."
                        ),
                        stream=True,
                        **extra_kwargs
                    )
                except Exception:
                    try:
                        stream = client.audio.transcriptions.create(
                            model=MODEL_STT,
                            file=audio_file,
                            response_format="text",
                            prompt=(
                                "This audio contains a discussion or presentation. Always preserve the original language "
                                "of each sentence. If a sentence is in English, output it in English; if in Chinese, output it "
                                "in Traditional Chinese; if mixed, output the original mixed-language sentence. Do not translate."
                            ),
                            stream=True
                        )
                        container.warning("æ­¤è½‰éŒ„ç«¯é»ä¸æ”¯æ´ promptï¼Œå¼•å°å·²è‡ªå‹•åœç”¨ï¼ˆæœ¬æ¬¡ï¼‰ã€‚")
                    except Exception as e2:
                        container.error(f"API è½‰éŒ„å¤±æ•—ï¼š{e2}")
                        stream = None

                if stream is not None:
                    for event in stream:
                        delta = getattr(event, "delta", None)
                        final_text = getattr(event, "text", None)
                        if delta:
                            full_text += delta
                            now = time.time()
                            if now - last_flush > FLUSH_INTERVAL:
                                container.markdown(all_text + full_text)
                                last_flush = now
                        elif final_text:
                            full_text = final_text
                            container.markdown(all_text + full_text)
        finally:
            if tmp_path:
                try:
                    os.remove(tmp_path)
                except Exception:
                    pass

        cache_set_text(cache_key, full_text.strip())
        all_text += full_text + "\n"
        rolling_context = (rolling_context + " " + full_text).strip()
        if len(rolling_context) > 5000:
            rolling_context = rolling_context[-5000:]

        if on_sentences:
            sents = split_sentences(full_text)
            on_sentences(sents)

        progress_bar.progress((i + 1) / len(chunks))
        container.markdown(all_text)

    return all_text.strip()

# é¡¯ç¤ºæ¨¡å¼å·¥å…·ï¼šæ®µè½ç¾¤çµ„
def group_into_paragraphs(sentences: List[str], max_chars: int = 260, max_sents: int = 4) -> List[str]:
    paras, cur, length = [], [], 0
    for s in sentences:
        s = s.strip()
        if not s:
            continue
        if cur and (len(cur) >= max_sents or length + len(s) > max_chars):
            paras.append(" ".join(cur))
            cur, length = [s], len(s)
        else:
            cur.append(s)
            length += len(s)
    if cur:
        paras.append(" ".join(cur))
    return paras

def render_topics_only(md: Dict[str, Any], stlib):
    stlib.markdown("#### ä¸»é¡Œ")
    topics = md.get("topics", [])
    for t in topics:
        stlib.markdown(f"##### {t.get('title','ä¸»é¡Œ')}")
        kp = t.get("key_points", [])
        if kp:
            stlib.markdown("\n".join(f"- {x}" for x in kp))
        if t.get("decisions"):
            stlib.markdown("æ±ºç­–ï¼š\n" + "\n".join(f"- {x}" for x in t.get("decisions", [])))
        if t.get("risks"):
            stlib.markdown("é¢¨éšªï¼š\n" + "\n".join(f"- {x}" for x in t.get("risks", [])))
        if t.get("open_questions"):
            stlib.markdown("æœªæ±ºå•é¡Œï¼š\n" + "\n".join(f"- {x}" for x in t.get("open_questions", [])))

# ========== ä¸Šå‚³å€ ==========
with st.expander("ä¸Šå‚³æœƒè­°éŒ„éŸ³æª”æ¡ˆ", expanded=True):
    f = st.file_uploader("è«‹ä¸Šå‚³éŸ³æª”ï¼ˆ.wav, .mp3, .m4a, .mp4, .webmï¼‰", type=["wav", "mp3", "m4a", "mp4", "webm"])
    start_btn = st.button("é–‹å§‹ Streaming è½‰éŒ„èˆ‡æ‘˜è¦")

# ========== é€²éšèª¿æ•´ ==========
with st.expander("é€²éšèª¿æ•´ï¼ˆå…¨éƒ¨è¨­å®šï¼Œå¯é¸ï¼‰", expanded=False):
    st.caption("å¹³å¸¸ç¶­æŒé è¨­å³å¯ï¼›åªæœ‰éŸ³æª”ç‰¹æ€§ç‰¹æ®Šæ™‚å†é–‹å•Ÿã€‚")

    st.markdown("###### éŸ³è¨Šå‰è™•ç†")
    cols = st.columns(2)
    with cols[0]:
        do_trim_leading = st.checkbox("å»å‰å°éœéŸ³ï¼ˆå»ºè­°é–‹ï¼‰", value=True)
        do_normalize = st.checkbox("éŸ³é‡æ­£è¦åŒ–åˆ° -20 dBFSï¼ˆå»ºè­°é–‹ï¼‰", value=True)
    with cols[1]:
        use_high_pass = st.checkbox("é«˜é€šæ¿¾æ³¢ï¼ˆé™ä½ä½é »å™ªï¼‰", value=False)
        hp_hz = st.slider("é«˜é€šæˆªæ­¢é »ç‡ (Hz)", 60, 300, 100, 10, disabled=not use_high_pass)
        use_low_pass = st.checkbox("ä½é€šæ¿¾æ³¢ï¼ˆé™é«˜é »å™ªï¼‰", value=False)
        lp_hz = st.slider("ä½é€šæˆªæ­¢é »ç‡ (Hz)", 4000, 12000, 9500, 100, disabled=not use_low_pass)

    st.markdown("###### Prompt å¼•å°ï¼ˆè‹¥ç«¯é»ä¸æ”¯æ´æœƒè‡ªå‹•å›é€€ï¼‰")
    use_prompting = st.checkbox("å•Ÿç”¨ Prompt å¼•å°ï¼ˆæ”¹å–„å°ˆæœ‰åè©æ‹¼å¯«èˆ‡é¢¨æ ¼ä¸€è‡´ï¼‰", value=False)
    glossary_input = st.text_area(
        "å°ˆæœ‰åè©æ‹¼å¯«æ¸…å–®ï¼ˆæ¯è¡Œä¸€å€‹ï¼‰",
        height=120,
        placeholder="ä¾‹ï¼š\nAimee\nShawn\nBBQ\nZyntriQix",
        disabled=not use_prompting
    )
    style_seed = st.text_area(
        "é¢¨æ ¼ç¤ºä¾‹ï¼ˆ1ï½3 å¥ç¤ºä¾‹æ–‡æœ¬ï¼Œä¸æ˜¯æŒ‡ä»¤ï¼‰",
        height=80,
        placeholder="ä¾‹ï¼š\nä¿æŒç°¡æ½”ã€æ¨™é»ä¸€è‡´ã€‚ä¾‹å¥ï¼šwe discuss quarterly outlook and risks.",
        disabled=not use_prompting
    )

    st.markdown("###### ä¸¦è¡Œæ½¤é£¾æ§åˆ¶ï¼ˆå¡è»Šæ€éº¼èª¿ï¼Ÿï¼‰")
    st.caption("å³æ™‚æ€§â†‘ï¼šæŠŠç­‰å¾…æ™‚é–“/å–®æ‰¹è¡Œæ•¸èª¿å°ï¼›çœæˆæœ¬â†‘ï¼šåä¹‹ã€‚éœ€è¦æ™‚æŠŠå·¥äººæ•¸é–‹åˆ° 2ã€‚")
    REFINE_MAX_WAIT_S = st.slider("å¾®æ‰¹æ¬¡æœ€å¤§ç­‰å¾…ç§’æ•¸ REFINE_MAX_WAIT_S", 0.10, 0.80, REFINE_MAX_WAIT_S, 0.05)
    REFINE_MAX_LINES  = st.slider("å–®æ‰¹æœ€å¤§è¡Œæ•¸ REFINE_MAX_LINES", 20, 140, REFINE_MAX_LINES, 5)
    REFINE_MAX_CHARS  = st.slider("å–®æ‰¹æœ€å¤§å­—æ•¸ REFINE_MAX_CHARS", 2000, 12000, REFINE_MAX_CHARS, 500)
    MAX_STREAM_WORKERS = st.slider("æ½¤é£¾å·¥äººæ•¸ MAX_STREAM_WORKERSï¼ˆ1ï½2ï¼‰", 1, 2, MAX_STREAM_WORKERS, 1)

    st.markdown("###### å³æ™‚ Map æ§åˆ¶ï¼ˆè®“é‡é»æ›´å¿«å‡ºç¾ï¼‰")
    st.caption("ç¬¬ä¸€æ™‚é–“çœ‹åˆ°é‡é»ï¼šæŠŠç­‰å¾…æ™‚é–“/å–®æ‰¹è¡Œæ•¸èª¿å°ï¼›æˆæœ¬å¤ªé«˜å†èª¿å¤§ã€‚")
    MAP_MAX_WAIT_S = st.slider("Map å¾®æ‰¹æ¬¡æœ€å¤§ç­‰å¾…ç§’æ•¸ MAP_MAX_WAIT_S", 0.10, 0.80, MAP_MAX_WAIT_S, 0.05)
    MAP_MAX_LINES  = st.slider("Map å–®æ‰¹æœ€å¤§è¡Œæ•¸ MAP_MAX_LINES", 10, 80, MAP_MAX_LINES, 2)
    MAP_MAX_CHARS  = st.slider("Map å–®æ‰¹æœ€å¤§å­—æ•¸ MAP_MAX_CHARS", 1000, 10000, MAP_MAX_CHARS, 250)
    MAP_WORKERS    = st.slider("Map å·¥äººæ•¸ MAP_WORKERSï¼ˆ1ï½2ï¼‰", 1, 2, MAP_WORKERS, 1)

if not (f and start_btn):
    st.stop()

# ========== ä¸»æµç¨‹ ==========
raw_bytes = f.read()
st.audio(raw_bytes)

# Tabs
tab1, tab2, tab3, tab4 = st.tabs(["è½‰éŒ„çµæœ", "é‡é»æ‘˜è¦", "å…§å®¹è§£æ", "åŸå§‹å…§å®¹"])

# Tab2 å…ˆæº–å‚™å³æ™‚ Map å®¹å™¨èˆ‡æœ€çµ‚é•·æ–‡å€
with tab2:
    st.markdown("#### å³æ™‚é‡é»ï¼ˆMap streamingï¼‰")
    map_stream_container = st.empty()
    map_progress = st.progress(0.0)
    st.divider()
    final_summary_placeholder = st.empty()

with tab1:
    with st.status("è™•ç†ä¸­...", expanded=True) as status:
        status.update(label="å„²å­˜èˆ‡è½‰æª”...")
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{f.name.split('.')[-1]}") as temp_input:
            temp_input.write(raw_bytes)
            temp_input_path = temp_input.name

        wav_path = temp_input_path
        if not f.name.lower().endswith(".wav"):
            wav_path = temp_input_path + ".wav"
            convert_to_wav(temp_input_path, wav_path)

        status.update(label="è¼‰å…¥éŸ³æª”èˆ‡å‰è™•ç†...")
        audio = AudioSegment.from_file(wav_path, format="wav")
        if st.session_state.get("_first_run_trim", True) and len(audio) > 0:
            st.session_state["_first_run_trim"] = False
        if st.session_state.get("_first_run_trim", False):
            pass
        if st.session_state.get("_first_run_trim", False):
            pass
        if True:
            # ä¾ä½¿ç”¨è€…å‹¾é¸åŸ·è¡Œ
            if st.session_state.get("_dummy", False):
                pass
        if True:
            if True:
                pass
        if True:
            pass
        if len(audio) > 0:
            if st.session_state.get("_dummy2", False):
                pass

        # å¯¦éš›å‰è™•ç†
        if True:
            if True:
                pass
        if True:
            pass

        if True:
            pass

        if True:
            pass

        if True:
            pass

        if True:
            pass

        # æ­£å¼åŸ·è¡Œå‰è™•ç†
        if do_trim_leading:
            audio = trim_leading_silence(audio, silence_threshold_db=-30.0, chunk_ms=10)
        if do_normalize:
            audio = normalize_loudness(audio, target_dbfs=-20.0)
        if use_high_pass or use_low_pass:
            audio = apply_filters(audio, use_high_pass=use_high_pass, hp_hz=hp_hz,
                                  use_low_pass=use_low_pass, lp_hz=lp_hz)

        status.update(label="éœéŸ³åˆ‡æ®µï¼ˆé™„æœ€é•·/æœ€çŸ­ä¿è­·ï¼›æ‰¾ä¸åˆ°éœéŸ³æœƒå›é€€å›ºå®šåˆ‡ï¼‰...")
        chunks = split_audio_on_silence_safe(audio)
        if not chunks:
            st.error("ç„¡æ³•åˆ‡å‡ºæœ‰æ•ˆéŸ³è¨Šæ®µï¼Œè«‹æª¢æŸ¥éŸ³æª”æˆ–èª¿æ•´åƒæ•¸ã€‚")
            st.stop()

        st.markdown("#### è½‰éŒ„çµæœ")
        stream_container = st.empty()
        progress_bar = st.progress(0.0)

        # ä¸¦è¡Œæ½¤é£¾èˆ‡ Mapï¼šå•Ÿå‹•å…©ä½æ¶ˆè²»è€…
        refine_progress = st.progress(0.0)
        consumer = RefineConsumer(stream_container, refine_progress, workers=MAX_STREAM_WORKERS)
        consumer_thread = Thread(target=consumer.run, daemon=True, name="RefineConsumer")

        summarizer = SummarizerConsumer(map_stream_container, map_progress, workers=MAP_WORKERS)
        summarizer_thread = Thread(target=summarizer.run, daemon=True, name="SummarizerConsumer")

        # æ› ScriptRunContextï¼ˆè‹¥å¤±æ•—æœƒè‡ªå‹•å›é€€æˆã€ŒèƒŒæ™¯ä¸å¯« UIã€æ¨¡å¼ï¼‰
        if get_script_run_ctx and add_script_run_ctx:
            ctx = get_script_run_ctx()
            if ctx is not None:
                try:
                    add_script_run_ctx(consumer_thread, ctx)
                    add_script_run_ctx(summarizer_thread, ctx)
                except Exception:
                    consumer.fallback_no_ui = True
                    summarizer.fallback_no_ui = True
            else:
                consumer.fallback_no_ui = True
                summarizer.fallback_no_ui = True
        else:
            consumer.fallback_no_ui = True
            summarizer.fallback_no_ui = True

        consumer_thread.start()
        summarizer_thread.start()
        consumer.set_total(len(chunks))
        summarizer.set_total(len(chunks))

        def fanout_on_sentences(sents: List[str]):
            consumer.put(sents)
            summarizer.put(sents)

        status.update(label="é€æ®µ Streaming è½‰éŒ„ä¸­ï¼ˆä¸¦è¡Œæ½¤é£¾ï¼‹å³æ™‚æ‘˜è¦ï¼‰...")
        all_text = stream_transcribe_all(
            chunks,
            stream_container,
            progress_bar,
            use_prompting=use_prompting,
            glossary=glossary_input if use_prompting else "",
            style_seed=style_seed if use_prompting else "",
            on_sentences=fanout_on_sentences
        )

        # æ”¶å°¾èˆ‡é¡¯ç¤º
        consumer.stop(); summarizer.stop()
        consumer_thread.join(); summarizer_thread.join()

        # è‹¥èƒŒæ™¯ä¸èƒ½å¯« UIï¼Œé€™è£¡ä¸€æ¬¡æŠŠå…§å®¹ç•«ä¸Šå»
        if consumer.fallback_no_ui:
            refined_lines = consumer.refined_lines_all[:] if consumer.refined_lines_all else consumer.unique_sentences_raw_all
            paras = group_into_paragraphs(refined_lines, max_chars=280, max_sents=4)
            stream_container.markdown("\n\n".join(paras))
        if summarizer.fallback_no_ui:
            if summarizer.blocks:
                map_stream_container.markdown("\n\n".join(summarizer.blocks))

        st.success("Transcription + Refine complete!")

        status.update(label="æ•´ä½µé‡é»ï¼ˆReduce ä¸­ï¼‰...")
        # Reduceï¼šä½¿ç”¨å³æ™‚ Map çš„ blocksï¼›è‹¥æ²’æœ‰å°±é€€å›ä¸€æ¬¡æ€§ Map
        map_blocks_text = summarizer.blocks[:] if summarizer.blocks else map_summarize_blocks(
            consumer.unique_sentences_raw_all if consumer.unique_sentences_raw_all else split_sentences(all_text)
        )
        final_md_summary = reduce_finalize_markdown(map_blocks_text)
        final_summary_placeholder.markdown(final_md_summary)

        # é¡å¤–æä¾›çµæ§‹åŒ– JSON èˆ‡ä¸»é¡Œæª¢è¦–
        final_minutes = reduce_finalize_json(map_blocks_text)

        with tab3:
            render_topics_only(final_minutes, st)

        with tab4:
            st.markdown("#### åŸå§‹å…§å®¹ï¼ˆæœ€åŸå§‹ä¸²æµè¼¸å‡ºï¼Œæœªåˆ†å¥ï¼æœªå»é‡ï¼‰")
            st.code(all_text.strip(), language="text")

        status.update(label="å…¨éƒ¨å®Œæˆï¼", state="complete", expanded=True)

# æ¸…ç†æš«å­˜
try:
    os.remove(temp_input_path)
    if 'wav_path' in locals() and wav_path != temp_input_path:
        os.remove(wav_path)
except Exception:
    pass
