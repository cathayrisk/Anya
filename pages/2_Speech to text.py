import streamlit as st
import os
import re
import time
import random
import tempfile
from openai import OpenAI
from pydub import AudioSegment
from pydub.utils import which
from concurrent.futures import ThreadPoolExecutor
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import CharacterTextSplitter
from langgraph.constants import Send
from langgraph.graph import END, START, StateGraph
from langchain_core.documents import Document
from langchain.chains.combine_documents.reduce import (
    acollapse_docs,
    split_list_of_docs,
)
import operator
import asyncio
import uuid
from typing import (
    Annotated,
    Any,
    Callable,
    Dict,
    List,
    Literal,
    Optional,
    Sequence,
    Type,
    Union,
    TypedDict,
)
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import (
    AIMessage,
    AnyMessage,
    BaseMessage,
    HumanMessage,
    ToolCall,
)
from langchain_core.prompt_values import PromptValue
from langchain_core.runnables import (
    Runnable,
    RunnableLambda,
)
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ValidationNode
from pydantic import BaseModel, Field, field_validator
import json
from langchain_core.callbacks.base import BaseCallbackHandler
from streamlit.delta_generator import DeltaGenerator

class StreamHandler(BaseCallbackHandler):
    def __init__(self, message_container: DeltaGenerator):
        self.text = ""
        self.message_container = message_container
        self.cursor_visible = True

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.cursor_visible = not self.cursor_visible
        cursor = '<span style="color:#2ecc71;font-weight:bold;">â–Œ</span>' if self.cursor_visible else '<span style="color:transparent;">â–Œ</span>'
        # é€™è£¡æ¯æ¬¡éƒ½è¦†è“‹åŒä¸€å€‹å€å¡Š
        self.message_container.markdown(self.text + cursor, unsafe_allow_html=True)
        time.sleep(0.04)
        
    def on_llm_end(self, *args, **kwargs):
        self.message_container.markdown(self.text, unsafe_allow_html=True)

# é…ç½® pydub ä½¿ç”¨ FFmpeg
AudioSegment.converter = which("ffmpeg")
AudioSegment.ffprobe = which("ffprobe")

# åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_KEY"]
client = OpenAI()

# åˆå§‹åŒ– LangChain çš„ ChatOpenAI æ¨¡å‹
llm = ChatOpenAI(model="gpt-4.1", temperature=0.0, streaming=True)

judge_llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0.0)

def llm_is_truncated(last_line: str, judge_llm) -> bool:
    prompt = f"""
ä½ æ˜¯ä¸€å€‹åˆ¤æ–·åŠ©æ‰‹ã€‚è«‹åˆ¤æ–·ä¸‹é¢é€™ä¸€è¡Œæ˜¯å¦æ˜¯åœ¨è«‹æ±‚ç”¨æˆ¶çºŒæ¥å…§å®¹ã€æˆ–æ˜¯çœç•¥æç¤ºï¼ˆä¾‹å¦‚ï¼šå…§å®¹éé•·ã€åƒ…å±•ç¤ºéƒ¨åˆ†å…§å®¹ã€è«‹ç¹¼çºŒã€continueã€remaining content ç­‰ï¼‰ï¼Œè€Œä¸æ˜¯ä¸€èˆ¬å…§å®¹ã€‚
å¦‚æœæ˜¯ï¼Œè«‹å›ç­”ã€Œæ˜¯ã€ï¼›å¦‚æœä¸æ˜¯ï¼Œè«‹å›ç­”ã€Œå¦ã€ã€‚
å…§å®¹ï¼š
{last_line}
"""
    response = judge_llm.invoke(prompt)
    answer = response.content.strip()
    return answer.startswith("æ˜¯")

def stream_full_formatted_transcription(chain, transcription, judge_llm, max_rounds=10):
    # ç”¨ CharacterTextSplitter åˆ†æ®µ
    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=3000, chunk_overlap=0)
    chunks = text_splitter.split_text(transcription)
    all_text = ""
    for idx, chunk in enumerate(chunks):
        message_container = st.empty()
        handler = StreamHandler(message_container)
        result = chain.invoke({"text": chunk}, config={"callbacks": [handler]})
        message_container.markdown(handler.text, unsafe_allow_html=True)
        all_text += handler.text + "\n"
    return all_text

def beautify_transcript(text):
    # 1. ä¸»é¡ŒåŠ ç²—
    text = re.sub(r'^(ä¸»é¡Œï¼š.*)$', r'**\1**', text, flags=re.MULTILINE)
    # 2. ä¸»é¡Œå¾ŒåŠ ç©ºè¡Œï¼ˆå¦‚æœæ²’æœ‰çš„è©±ï¼‰
    text = re.sub(r'(\*\*ä¸»é¡Œï¼š.*?\*\*)(\n)(?!\n)', r'\1\n\n', text)
    # 3. æ®µè½é–“åŠ ç©ºè¡Œï¼ˆå…©è¡Œä»¥ä¸Šä¸é‡è¤‡ï¼‰
    text = re.sub(r'([^\n])\n([^\n])', r'\1\n\n\2', text)
    # 4. é—œéµè©é«˜äº®ï¼ˆå¯è‡ªè¡Œæ“´å……ï¼‰
    #keywords = [
    #    'non-bank f.i.', 'msr', 'ltv', 'NPL', 'FTP', 'SPA', 'ICR', 'LTC', 'provision',
    #    'syndication', 'refinance', 'credit', 'æŠ•è³‡ç­‰ç´š', 'ä¿¡è©•', 'å¹³ç­‰', 'é æ”¾æ¯”', 'é›†ä¸­åº¦'
    #]
    #for kw in keywords:
        # é¿å…é‡è¤‡åŠ ç²—
    #    text = re.sub(rf'(?<!\*)({re.escape(kw)})(?!\*)', r'**\1**', text)
    # 5. ã€ç–‘ä¼¼éŒ¯èª¤ã€‘æ¨™ç´…
    text = re.sub(r'ã€ç–‘ä¼¼éŒ¯èª¤ã€‘', r'<span style="color:red">ã€ç–‘ä¼¼éŒ¯èª¤ã€‘</span>', text)
    # 6. ç§»é™¤å¤šé¤˜ç©ºè¡Œï¼ˆæœ€å¤šå…©è¡Œï¼‰
    text = re.sub(r'\n{3,}', '\n\n', text)
    # 7. å»é™¤é–‹é ­å¤šé¤˜ç©ºè¡Œ
    text = text.lstrip('\n')
    return text


# è¨­ç½®ç¶²é æ¨™é¡Œå’Œåœ–æ¨™
st.set_page_config(page_title="Speech to Text Transcription", layout="wide", page_icon="ğŸ‘„")
st.title("Speech to text transcription")

# å‰µå»ºä¸€å€‹è¡¨å–®ä¾†ä¸Šå‚³æ–‡ä»¶
with st.form(key="my_form"):
    f = st.file_uploader("Upload your audio file", type=["wav", "mp3", "mp4", "mpeg", "mpga", "m4a", "webm"])
    st.info("ğŸ‘† ä¸Šå‚³ä¸€å€‹éŸ³æ•ˆæ–‡ä»¶ï¼ˆæ”¯æ´ .wav, .mp3, .mp4, .mpeg, .mpga, .m4a, .wav, .webmï¼‰ã€‚")
    submit_button = st.form_submit_button(label="Transcribe")

# å®šç¾©ç”Ÿæˆå™¨å‡½æ•¸ä¾†é€æ­¥ç”¢ç”Ÿè½‰éŒ„æ–‡æœ¬
def stream_transcription(transcription_text):
    message_container = st.empty()
    text = ""
    for word in transcription_text.split():
        text += word + " "
        message_container.markdown(text)
        time.sleep(0.05)  # æ§åˆ¶æ‰“å­—æ©Ÿæ•ˆæœçš„é€Ÿåº¦

# å®šç¾©è½‰éŒ„éŸ³é »å¡Šçš„å‡½æ•¸
def transcribe_chunk(chunk, index):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as temp_mp3_file:
        chunk.export(temp_mp3_file.name, format="mp3")
        with open(temp_mp3_file.name, "rb") as audio_file:
            transcription = client.audio.transcriptions.create(
                #model="whisper-1",
                model="gpt-4o-transcribe",
                file=audio_file,
                response_format="text",
                prompt="This audio contains a discussion or presentation. If the speech is in Chinese, please transcribe the content into Traditional Chinese characters. The audio may cover various topics such as updates, feedback, or informative lectures."
            )
        os.remove(temp_mp3_file.name)
    return transcription.lower()

# ä½¿ç”¨ Streamlit çš„ç·©å­˜åŠŸèƒ½ä¾†ç·©å­˜è½‰éŒ„çµæœ
@st.cache_data
def transcribe_audio(file_path):
    # é€™è£¡æ”¾ç½®è½‰éŒ„é‚è¼¯
    # è¿”å›è½‰éŒ„çµæœ
    return full_transcription

@st.cache_data
def format_transcription(transcription):
    # ä½¿ç”¨ LangChain æ”¹å–„æ–‡æœ¬æ ¼å¼
    return chain.invoke({"text": transcription})

# å®šç¾© LangChain çš„ PromptTemplate
prompt_template = PromptTemplate(
    input_variables=["text"],
    template=(
        "# Role and Objective\n"
        "You are an expert transcription editor. Your task is to improve the formatting and readability of a raw transcription text, while strictly preserving the original wording, sentence order, and information.\n\n"
        "# Instructions\n"
        "- **Do not change the meaning, order, or language of any sentence.**\n"
        "- If the text is in Chinese, always use Traditional Chinese (ç¹é«”ä¸­æ–‡)ã€‚\n"
        "- **List the text sentence by sentence, each on a new line.**\n"
        "- **Do not merge, split, or paraphrase sentences.**\n"
        "- **Do not add or remove any content.**\n"
        "- **Do not translate.**\n"
        "- If you find a sentence that is incomplete or has obvious errors, mark it with ã€ç–‘ä¼¼éŒ¯èª¤ã€‘ at the end of the sentence.\n"
        "- If the input is extremely long, process all content in full (do not skip or summarize any part).\n\n"
        "## Formatting Rules\n"
        "1. **Add appropriate headings and subheadings** to organize the content. Use a consistent style (e.g., headings in bold, subheadings in italics).\n"
        "2. **Highlight key terms or important concepts** using bold or italics for emphasis.\n"
        "3. **Check and correct any grammatical or spelling errors** (but do not change the original meaning or sentence structure).\n"
        "4. **Add appropriate punctuation** and split run-on sentences for better readability, but do not merge or paraphrase sentences.\n"
        "5. **Divide the text into paragraphs** where appropriate, and ensure there is a blank line between paragraphs.\n"
        "6. **Preserve the original language and style.**\n\n"
        "# Reasoning Steps\n"
        "1. Analyze the input text to identify logical sections and possible headings.\n"
        "2. For each sentence, check for grammar, spelling, and punctuation issues, and correct them if needed.\n"
        "3. Highlight key terms or concepts.\n"
        "4. Organize sentences into paragraphs and insert headings/subheadings as appropriate.\n"
        "5. Mark any incomplete or obviously erroneous sentences with ã€ç–‘ä¼¼éŒ¯èª¤ã€‘.\n"
        "6. Output the result in markdown format, using bold for headings, italics for subheadings, and bold/italics for key terms.\n\n"
        "# Output Format\n"
        "- Use markdown formatting.\n"
        "- Headings: **bold**\n"
        "- Subheadings: *italics*\n"
        "- Key terms: **bold** or *italics*\n"
        "- Each sentence on a new line, in original order.\n"
        "- Blank line between paragraphs.\n"
        "- Mark incomplete or erroneous sentences with ã€ç–‘ä¼¼éŒ¯èª¤ã€‘.\n\n"
        "# Example\n"
        "## Input\n"
        "text: é€™æ˜¯ä¸€æ®µé€å­—ç¨¿å…§å®¹ã€‚ä»Šå¤©æˆ‘å€‘è¦è¨è«–äººå·¥æ™ºæ…§ã€‚AIçš„æ‡‰ç”¨è¶Šä¾†è¶Šå»£æ³›ï¼Œç‰¹åˆ¥æ˜¯åœ¨é†«ç™‚å’Œæ•™è‚²é ˜åŸŸã€‚é€™è£¡æœ‰ä¸€å€‹ä¾‹å­AIå¯ä»¥å”åŠ©é†«ç”Ÿè¨ºæ–·ç–¾ç—…ã€‚è¬è¬å¤§å®¶çš„è†è½ã€‚\n\n"
        "## Output\n"
        "**ä¸»é¡Œï¼šäººå·¥æ™ºæ…§çš„æ‡‰ç”¨**\n\n"
        "*å¼•è¨€*\n"
        "é€™æ˜¯ä¸€æ®µé€å­—ç¨¿å…§å®¹ã€‚\n"
        "ä»Šå¤©æˆ‘å€‘è¦è¨è«–**äººå·¥æ™ºæ…§**ã€‚\n\n"
        "*AIçš„æ‡‰ç”¨*\n"
        "**AI**çš„æ‡‰ç”¨è¶Šä¾†è¶Šå»£æ³›ï¼Œç‰¹åˆ¥æ˜¯åœ¨**é†«ç™‚**å’Œ**æ•™è‚²**é ˜åŸŸã€‚\n"
        "é€™è£¡æœ‰ä¸€å€‹ä¾‹å­ï¼š**AI**å¯ä»¥å”åŠ©é†«ç”Ÿè¨ºæ–·ç–¾ç—…ã€‚\n\n"
        "*çµèª*\n"
        "è¬è¬å¤§å®¶çš„è†è½ã€‚\n"
        "\n"
        "# Final Instructions\n"
        "- Think step by step. Carefully follow all formatting and output rules.\n"
        "- **ä½ å¿…é ˆå®Œæ•´è¼¸å‡ºæ‰€æœ‰å…§å®¹ï¼Œåš´ç¦åªå±•ç¤ºéƒ¨åˆ†å…§å®¹æˆ–ä»¥ä»»ä½•å½¢å¼è¦æ±‚ç”¨æˆ¶è‡ªè¡Œè™•ç†å‰©é¤˜å…§å®¹ã€‚**\n"
        "- **åš´ç¦å‡ºç¾ã€Œåƒ…å±•ç¤ºéƒ¨åˆ†å…§å®¹ã€ã€ã€Œå¾ŒçºŒå…§å®¹è«‹ä¾è¦å‰‡æŒçºŒè™•ç†ã€ç­‰å­—çœ¼ã€‚**"
        "- **å³ä½¿å…§å®¹æ¥µé•·ï¼Œä¹Ÿå¿…é ˆç›¡å¯èƒ½å®Œæ•´è¼¸å‡ºï¼Œç›´åˆ°å¹³å°å›æ‡‰é•·åº¦é”åˆ°æ¥µé™ç‚ºæ­¢ã€‚**"
        "- **å¦‚æœå…§å®¹éé•·å°è‡´ç„¡æ³•ä¸€æ¬¡è¼¸å‡ºå…¨éƒ¨ï¼Œè«‹è‡ªå‹•ç¹¼çºŒè¼¸å‡ºå‰©é¤˜å…§å®¹ï¼Œç›´åˆ°å…¨éƒ¨å…§å®¹éƒ½å·²å‘ˆç¾ï¼Œä¸”æ¯æ¬¡å›æ‡‰éƒ½åªè¼¸å‡ºå…§å®¹æœ¬èº«ï¼Œä¸è¦åŠ ä»»ä½•èªªæ˜æˆ–çœç•¥æç¤ºã€‚**"
        "- **Always preserve the original language of each sentence. If a sentence is in English, output it in English; if in Chinese, output it in Chinese; if mixed, output the original mixed-language sentence. Do not translate or alter the language.**"
        "- Output only the formatted text in markdown, no extra explanation."
        "## Input Text\n"
        "{text}\n"
    )
)

# å‰µå»ºä¸€å€‹è™•ç†éˆ
formatting_chain = prompt_template | llm | StrOutputParser()

# åˆ†å‰²æ–‡ä»¶
text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=1500, chunk_overlap=100)

token_max = 200000

# å®šç¾©ç‹€æ…‹é¡å‹
class SummaryState(TypedDict):
    content: str

class OverallState(TypedDict):
    contents: List[str]
    summaries: Annotated[list, operator.add]
    collapsed_summaries: List[Document]
    final_summary: str

# ç”Ÿæˆæ‘˜è¦
async def generate_summary(state: SummaryState):
    response = await map_chain.ainvoke(state["content"])
    return {"summaries": [response]}

# æ˜ å°„æ‘˜è¦
def map_summaries(state: OverallState):
    return [Send("generate_summary", {"content": content}) for content in state["contents"]]

# è¨ˆç®—æ–‡ä»¶é•·åº¦
def length_function(documents: List[Document]) -> int:
    return sum(llm.get_num_tokens(doc.page_content) for doc in documents)

# æ”¶é›†æ‘˜è¦
def collect_summaries(state: OverallState):
    return {"collapsed_summaries": [Document(summary) for summary in state["summaries"]]}

# ç”Ÿæˆæœ€çµ‚æ‘˜è¦
async def generate_final_summary(state: OverallState):
    response = await reduce_chain.ainvoke(state["collapsed_summaries"])
    return {"final_summary": response}

# æ‘ºç–Šæ‘˜è¦
async def collapse_summaries(state: OverallState):
    doc_lists = split_list_of_docs(state["collapsed_summaries"], length_function, token_max)
    results = []
    for doc_list in doc_lists:
        results.append(await acollapse_docs(doc_list, reduce_chain.ainvoke))
    return {"collapsed_summaries": results}

# åˆ¤æ–·æ˜¯å¦éœ€è¦æ‘ºç–Š
def should_collapse(state: OverallState) -> Literal["collapse_summaries", "generate_final_summary"]:
    num_tokens = length_function(state["collapsed_summaries"])
    if num_tokens > token_max:
        return "collapse_summaries"
    else:
        return "generate_final_summary"

# å»ºç«‹ç‹€æ…‹åœ–
graph = StateGraph(OverallState)
graph.add_node("generate_summary", generate_summary)
graph.add_node("collect_summaries", collect_summaries)
graph.add_node("generate_final_summary", generate_final_summary)
graph.add_node("collapse_summaries", collapse_summaries)

# æ·»åŠ é‚Š
graph.add_conditional_edges(START, map_summaries, ["generate_summary"])
graph.add_edge("generate_summary", "collect_summaries")
graph.add_conditional_edges("collect_summaries", should_collapse)
graph.add_conditional_edges("collapse_summaries", should_collapse)
graph.add_edge("generate_final_summary", END)

# ç·¨è­¯æ‡‰ç”¨ç¨‹å¼
app = graph.compile()

# å®šç¾©æç¤ºæ¨¡æ¿
map_template = """
Please read the following transcribed content from a recording and identify the main themes and key insights. Focus on extracting the core information and specific details from the recording.
- Use bullet points to list key insights and specific details for each theme.
- Include specific data, examples, and references to support each theme, ensuring a rich level of detail.
- Explain the cause-and-effect relationships for each key point, highlighting how different factors interact.
- Pay attention to the tone and style to ensure consistency with the conversational nature of the recording.
- Identify and highlight any important action items or decisions, and explain the rationale behind them.
- Summarize the key points that the speaker most wants to convey.
If the source is in Chinese, please respond in Traditional Chinese, not Simplified Chinese.
Content: {context}
"""

reduce_template = """
The following are summaries of key themes extracted from the recording:
{docs}
Please consolidate these into broader categories, focusing on grouping related themes under major categories. For each major category, provide a detailed summary using bullet points to highlight key insights, specific details, and explain the cause-and-effect relationships.
- Ensure all important details and examples are included to support each category's summary, enhancing the richness of the content.
- Maintain consistency in tone and style, reflecting the conversational nature of the recording.
- Ensure identification and emphasis on any important action items or decisions, and provide explanations for these recommendations.
- Summarize the key points that the speaker most wants to convey, without assuming additional strategic implications unless explicitly mentioned.
If the source is in Chinese, please respond in Traditional Chinese, not Simplified Chinese.
"""

map_prompt = ChatPromptTemplate([("human", map_template)])
reduce_prompt = ChatPromptTemplate([("human", reduce_template)])

map_chain = map_prompt | llm | StrOutputParser()
reduce_chain = reduce_prompt | llm | StrOutputParser()

# å®šç¾©é‹è¡Œæ‡‰ç”¨ç¨‹å¼çš„ç•°æ­¥å‡½æ•¸
async def run_app(split_docs):
    async for step in app.astream(
        {"contents": [doc.page_content for doc in split_docs]},
        {"recursion_limit": 100},
    ):
        pass  # é€™è£¡ä¸éœ€è¦ä»»ä½•æ“ä½œï¼Œåªæ˜¯ç­‰å¾…å®Œæˆ
    return step['generate_final_summary']['final_summary']

# è§£æçš„éƒ¨åˆ†
def _default_aggregator(messages: Sequence[AnyMessage]) -> AIMessage:
    for m in messages[::-1]:
        if m.type == "ai":
            return m
    raise ValueError("No AI message found in the sequence.")


class RetryStrategy(TypedDict, total=False):
    """The retry strategy for a tool call."""

    max_attempts: int
    """The maximum number of attempts to make."""
    fallback: Optional[
        Union[
            Runnable[Sequence[AnyMessage], AIMessage],
            Runnable[Sequence[AnyMessage], BaseMessage],
            Callable[[Sequence[AnyMessage]], AIMessage],
        ]
    ]
    """The function to use once validation fails."""
    aggregate_messages: Optional[Callable[[Sequence[AnyMessage]], AIMessage]]


def _bind_validator_with_retries(
    llm: Union[
        Runnable[Sequence[AnyMessage], AIMessage],
        Runnable[Sequence[BaseMessage], BaseMessage],
    ],
    *,
    validator: ValidationNode,
    retry_strategy: RetryStrategy,
    tool_choice: Optional[str] = None,
) -> Runnable[Union[List[AnyMessage], PromptValue], AIMessage]:
    """Binds a tool validators + retry logic to create a runnable validation graph.

    LLMs that support tool calling can generate structured JSON. However, they may not always
    perfectly follow your requested schema, especially if the schema is nested or has complex
    validation rules. This method allows you to bind a validation function to the LLM's output,
    so that any time the LLM generates a message, the validation function is run on it. If
    the validation fails, the method will retry the LLM with a fallback strategy, the simplest
    being just to add a message to the output with the validation errors and a request to fix them.

    The resulting runnable expects a list of messages as input and returns a single AI message.
    By default, the LLM can optionally NOT invoke tools, making this easier to incorporate into
    your existing chat bot. You can specify a tool_choice to force the validator to be run on
    the outputs.

    Args:
        llm (Runnable): The llm that will generate the initial messages (and optionally fallba)
        validator (ValidationNode): The validation logic.
        retry_strategy (RetryStrategy): The retry strategy to use.
            Possible keys:
            - max_attempts: The maximum number of attempts to make.
            - fallback: The LLM or function to use in case of validation failure.
            - aggregate_messages: A function to aggregate the messages over multiple turns.
                Defaults to fetching the last AI message.
        tool_choice: If provided, always run the validator on the tool output.

    Returns:
        Runnable: A runnable that can be invoked with a list of messages and returns a single AI message.
    """

    def add_or_overwrite_messages(left: list, right: Union[list, dict]) -> list:
        """Append messages. If the update is a 'finalized' output, replace the whole list."""
        if isinstance(right, dict) and "finalize" in right:
            finalized = right["finalize"]
            if not isinstance(finalized, list):
                finalized = [finalized]
            for m in finalized:
                if m.id is None:
                    m.id = str(uuid.uuid4())
            return finalized
        res = add_messages(left, right)
        if not isinstance(res, list):
            return [res]
        return res

    class State(TypedDict):
        messages: Annotated[list, add_or_overwrite_messages]
        attempt_number: Annotated[int, operator.add]
        initial_num_messages: int
        input_format: Literal["list", "dict"]

    builder = StateGraph(State)

    def dedict(x: State) -> list:
        """Get the messages from the state."""
        return x["messages"]

    model = dedict | llm | (lambda msg: {"messages": [msg], "attempt_number": 1})
    fbrunnable = retry_strategy.get("fallback")
    if fbrunnable is None:
        fb_runnable = llm
    elif isinstance(fbrunnable, Runnable):
        fb_runnable = fbrunnable  # type: ignore
    else:
        fb_runnable = RunnableLambda(fbrunnable)
    fallback = (
        dedict | fb_runnable | (lambda msg: {"messages": [msg], "attempt_number": 1})
    )

    def count_messages(state: State) -> dict:
        return {"initial_num_messages": len(state.get("messages", []))}

    builder.add_node("count_messages", count_messages)
    builder.add_node("llm", model)
    builder.add_node("fallback", fallback)

    # To support patch-based retries, we need to be able to
    # aggregate the messages over multiple turns.
    # The next sequence selects only the relevant messages
    # and then applies the validator
    select_messages = retry_strategy.get("aggregate_messages") or _default_aggregator

    def select_generated_messages(state: State) -> list:
        """Select only the messages generated within this loop."""
        selected = state["messages"][state["initial_num_messages"] :]
        return [select_messages(selected)]

    def endict_validator_output(x: Sequence[AnyMessage]) -> dict:
        if tool_choice and not x:
            return {
                "messages": [
                    HumanMessage(
                        content=f"ValidationError: please respond with a valid tool call [tool_choice={tool_choice}].",
                        additional_kwargs={"is_error": True},
                    )
                ]
            }
        return {"messages": x}

    validator_runnable = select_generated_messages | validator | endict_validator_output
    builder.add_node("validator", validator_runnable)

    class Finalizer:
        """Pick the final message to return from the retry loop."""

        def __init__(self, aggregator: Optional[Callable[[list], AIMessage]] = None):
            self._aggregator = aggregator or _default_aggregator

        def __call__(self, state: State) -> dict:
            """Return just the AI message."""
            initial_num_messages = state["initial_num_messages"]
            generated_messages = state["messages"][initial_num_messages:]
            return {
                "messages": {
                    "finalize": self._aggregator(generated_messages),
                }
            }

    # We only want to emit the final message
    builder.add_node("finalizer", Finalizer(retry_strategy.get("aggregate_messages")))

    # Define the connectivity
    builder.add_edge(START, "count_messages")
    builder.add_edge("count_messages", "llm")

    def route_validator(state: State):
        if state["messages"][-1].tool_calls or tool_choice is not None:
            return "validator"
        return END

    builder.add_conditional_edges("llm", route_validator, ["validator", END])
    builder.add_edge("fallback", "validator")
    max_attempts = retry_strategy.get("max_attempts", 3)

    def route_validation(state: State):
        if state["attempt_number"] > max_attempts:
            raise ValueError(
                f"Could not extract a valid value in {max_attempts} attempts."
            )
        for m in state["messages"][::-1]:
            if m.type == "ai":
                break
            if m.additional_kwargs.get("is_error"):
                return "fallback"
        return "finalizer"

    builder.add_conditional_edges(
        "validator", route_validation, ["finalizer", "fallback"]
    )

    builder.add_edge("finalizer", END)

    # These functions let the step be used in a MessageGraph
    # or a StateGraph with 'messages' as the key.
    def encode(x: Union[Sequence[AnyMessage], PromptValue]) -> dict:
        """Ensure the input is the correct format."""
        if isinstance(x, PromptValue):
            return {"messages": x.to_messages(), "input_format": "list"}
        if isinstance(x, list):
            return {"messages": x, "input_format": "list"}
        raise ValueError(f"Unexpected input type: {type(x)}")

    def decode(x: State) -> AIMessage:
        """Ensure the output is in the expected format."""
        return x["messages"][-1]

    return (
        encode | builder.compile().with_config(run_name="ValidationGraph") | decode
    ).with_config(run_name="ValidateWithRetries")


def bind_validator_with_retries(
    llm: BaseChatModel,
    *,
    tools: list,
    tool_choice: Optional[str] = None,
    max_attempts: int = 3,
) -> Runnable[Union[List[AnyMessage], PromptValue], AIMessage]:
    """Binds validators + retry logic ensure validity of generated tool calls.

    LLMs that support tool calling are good at generating structured JSON. However, they may
    not always perfectly follow your requested schema, especially if the schema is nested or
    has complex validation rules. This method allows you to bind a validation function to
    the LLM's output, so that any time the LLM generates a message, the validation function
    is run on it. If the validation fails, the method will retry the LLM with a fallback
    strategy, the simples being just to add a message to the output with the validation
    errors and a request to fix them.

    The resulting runnable expects a list of messages as input and returns a single AI message.
    By default, the LLM can optionally NOT invoke tools, making this easier to incorporate into
    your existing chat bot. You can specify a tool_choice to force the validator to be run on
    the outputs.

    Args:
        llm (Runnable): The llm that will generate the initial messages (and optionally fallba)
        validator (ValidationNode): The validation logic.
        retry_strategy (RetryStrategy): The retry strategy to use.
            Possible keys:
            - max_attempts: The maximum number of attempts to make.
            - fallback: The LLM or function to use in case of validation failure.
            - aggregate_messages: A function to aggregate the messages over multiple turns.
                Defaults to fetching the last AI message.
        tool_choice: If provided, always run the validator on the tool output.

    Returns:
        Runnable: A runnable that can be invoked with a list of messages and returns a single AI message.
    """
    bound_llm = llm.bind_tools(tools, tool_choice=tool_choice)
    retry_strategy = RetryStrategy(max_attempts=max_attempts)
    validator = ValidationNode(tools)
    return _bind_validator_with_retries(
        bound_llm,
        validator=validator,
        tool_choice=tool_choice,
        retry_strategy=retry_strategy,
    ).with_config(metadata={"retry_strategy": "default"})

class Respond(BaseModel):
    """Use to generate the response. Always use when responding to the user"""

    reason: str = Field(description="Step-by-step justification for the answer.")
    answer: str

tools = [Respond]

bound_llm = bind_validator_with_retries(llm, tools=tools)
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "Respond directly by calling the Respond function. If the source is in Chinese, please respond in Traditional Chinese, not Simplified Chinese."),
        ("placeholder", "{messages}"),
    ]
)

chain = prompt | bound_llm

class OutputFormat(BaseModel):
    sources: str = Field(
        ...,
        description="The raw transcript / span you could cite to justify the choice.",
    )
    content: str = Field(..., description="The chosen value.")

class Moment(BaseModel):
    quote: str = Field(..., description="The relevant quote from the transcript.")
    description: str = Field(..., description="A description of the moment.")
    expressed_preference: OutputFormat = Field(
        ..., description="The preference expressed in the moment, based on the context."
    )

class BackgroundInfo(BaseModel):
    factoid: OutputFormat = Field(
        ..., description="Important factoid about the member."
    )
    professions: Optional[List[str]] = Field(
        None, description="List of professions related to the member."
    )
    why: str = Field(..., description="Why this is important.")

class KeyMoments(BaseModel):
    topic: str = Field(..., description="The topic of the key moments.")
    happy_moments: List[Moment] = Field(
        ..., description="A list of key moments related to the topic."
    )
    tense_moments: List[Moment] = Field(
        ..., description="Moments where things were a bit tense."
    )
    sad_moments: List[Moment] = Field(
        ..., description="Moments where things where everyone was downtrodden."
    )
    background_info: List[BackgroundInfo] = Field(
        ..., description="A list of background information."
    )
    moments_summary: str = Field(..., description="A summary of the key moments.")

class InsightfulQuote(BaseModel):
    quote: OutputFormat = Field(
        ..., description="An insightful quote from the transcript."
    )
    speaker: str = Field(..., description="The name of the speaker who said the quote.")
    analysis: str = Field(
        ..., description="An analysis of the quote and its significance."
    )

class TranscriptMetadata(BaseModel):
    title: str = Field(..., description="The title of the transcript.")
    location: OutputFormat = Field(
        ..., description="The location where the interview took place. If the location cannot be identified, return 'Unknown'."
    )
    duration: str = Field(..., description="The duration of the interview.")

class TranscriptSummary(BaseModel):
    metadata: TranscriptMetadata = Field(
        ..., description="Metadata about the transcript."
    )
    key_moments: List[KeyMoments] = Field(
        ..., description="A list of key moments from the interview."
    )
    insightful_quotes: List[InsightfulQuote] = Field(
        ..., description="A list of insightful quotes from the interview."
    )
    overall_summary: str = Field(
        ..., description="An overall summary of the interview."
    )
    next_steps: List[str] = Field(
        ..., description="A list of next steps or action items based on the interview."
    )
    other_stuff: List[OutputFormat] = Field(
        ..., description="Additional relevant information."
    )

formatted_transcription = ""  # å…ˆåˆå§‹åŒ–

# è™•ç†ä¸Šå‚³çš„æ–‡ä»¶
if f is not None:
    st.audio(f)
    file_extension = f.name.split('.')[-1]  # ç²å–æ–‡ä»¶çš„å‰¯æª”å

    # åˆå§‹åŒ– summarize_transcription è®Šæ•¸
    summarize_transcription = "æ‘˜è¦å°šæœªç”Ÿæˆã€‚"

    # ä½¿ç”¨ st.status ä¾†é¡¯ç¤ºæ•´é«”è™•ç†ç‹€æ…‹
    with st.status("Processing audio file...", expanded=True) as status:
        try:
            # å°‡ä¸Šå‚³çš„æ–‡ä»¶ä¿å­˜åˆ°è‡¨æ™‚æ–‡ä»¶
            status.update(label="Saving uploaded file...")
            with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_extension}") as temp_input_file:
                temp_input_file.write(f.read())
                temp_input_file_path = temp_input_file.name

            # ä½¿ç”¨ pydub è®€å–å’Œè½‰æ›éŸ³é »æ–‡ä»¶
            status.update(label="Loading audio file...")
            audio = AudioSegment.from_file(temp_input_file_path, format=file_extension)

            # åˆ†å‰²éŸ³é »æ–‡ä»¶
            status.update(label="Splitting audio into chunks...")
            chunk_length_ms = 1 * 60 * 1000  # 1åˆ†é˜
            chunks = [audio[i:i + chunk_length_ms] for i in range(0, len(audio), chunk_length_ms)]

            if not chunks:
                st.error("No audio chunks were created. Please check the audio file format and content.")

            full_transcription = ""
            progress_bar = st.progress(0)
            total_chunks = len(chunks)

            status.update(label="Transcribing audio chunks...")
            # ä½¿ç”¨ ThreadPoolExecutor ä¾†ä¸¦è¡Œè™•ç†
            with ThreadPoolExecutor() as executor:
                futures = [executor.submit(transcribe_chunk, chunk, i) for i, chunk in enumerate(chunks)]
                for i, f in enumerate(futures):
                    full_transcription += f.result() + " "
                    progress = (i + 1) / total_chunks
                    progress_bar.progress(progress)

            progress_bar.empty()

            # ä½¿ç”¨ LangChain æ”¹å–„æ–‡æœ¬æ ¼å¼
            status.update(label="Formatting transcription...")
            formatted_transcription = stream_full_formatted_transcription(formatting_chain, full_transcription, judge_llm)

            status.update(label="Transcription complete!", state="complete", expanded=False)
        except Exception as e:
            st.error(f"Error processing audio file: {e}")
            formatted_transcription = "âš ï¸ è½‰éŒ„æˆ–æ ¼å¼åŒ–æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"

    # é¡¯ç¤º formatted_transcription
    tab1, tab2, tab3, tab4 = st.tabs(["è½‰éŒ„çµæœ", "é‡é»æ‘˜è¦", "å…§å®¹è§£æ", "åŸå§‹å…§å®¹"])
    with tab1:
        with st.container():
            st.markdown(beautify_transcript(formatted_transcription), unsafe_allow_html=True)
            st.balloons()

    # ç•°æ­¥è¨ˆç®— summarize_transcription ä¸¦åœ¨ Tab2 ä¸­é¡¯ç¤º spinner
    async def calculate_summary():
        # åˆ†å‰²è½‰éŒ„æ–‡æœ¬ä¸¦åŒ…è£æˆ Document å°è±¡
        split_docs = [Document(page_content=content) for content in text_splitter.split_text(full_transcription)]
        summarize_transcription = await run_app(split_docs)
        st.session_state['summarize_transcription'] = summarize_transcription
        return summarize_transcription

    with tab2:
        with st.spinner('Generating summary...'):
            summarize_transcription = asyncio.run(calculate_summary())
            st.markdown(summarize_transcription)

    # ä¿å­˜è½‰éŒ„çµæœåˆ° session_state
        if 'formatted_transcription' not in st.session_state:
            st.session_state['formatted_transcription'] = formatted_transcription
        if 'full_transcription' not in st.session_state:
            st.session_state['full_transcription'] = full_transcription

    with tab3:
        with st.container():
            try:
                formatted_transcription = st.session_state.get('formatted_transcription', "")
                transcript = [
                    (
                        "Speaker",
                        full_transcription,
                    ),
                ]

                formatted = "\n".join(f"{x[0]}: {x[1]}" for x in transcript)

                tools = [TranscriptSummary]
                bound_llm = bind_validator_with_retries(
                    llm,
                    tools=tools,
                )
                prompt = ChatPromptTemplate.from_messages(
                    [
                        ("system", "Respond directly using the TranscriptSummary function."),
                        ("placeholder", "{messages}"),
                    ]
                )

                bound_chain = prompt | bound_llm              
                try:
                    results = bound_chain.invoke(
                        {
                            "messages": [
                                (
                                    "user",
                                    f"Extract the summary from the following conversation:\n\n<convo>\n{formatted}\n</convo>"
                                    "\n\nRemember to respond using the TranscriptSummary function.",
                                )
                            ]
                        },
                    )
                except ValueError as e:
                    print(repr(e))
                data = results.additional_kwargs

                # æå– tool_calls
                tool_calls = data['tool_calls']

                # éæ­·æ¯å€‹ tool_call
                for tool_call in tool_calls:
                    # æå– Arguments
                    arguments = tool_call['function']['arguments']

                    # è§£æ Arguments ä¸­çš„ JSON å­—ç¬¦ä¸²
                    arguments_data = json.loads(arguments)

                    # æå– Metadata
                    metadata = arguments_data['metadata']
                    st.markdown("### Metadata")
                    st.markdown(f"- **æ¨™é¡Œ**: {metadata['title']}")
                    st.markdown(f"- **åœ°é»**: {metadata['location']['content']}")
                    st.markdown(f"- **æŒçºŒæ™‚é–“**: {metadata['duration']}")

                    # æå– Key Moments
                    key_moments = arguments_data['key_moments']
                    st.markdown("\n### Key Moments")
                    for moment in key_moments:
                        st.markdown(f"- **ä¸»é¡Œ**: {moment['topic']}")
                        st.markdown(f"  - **æ™‚åˆ»ç¸½çµ**: {moment['moments_summary']}")
                        for info in moment['background_info']:
                            st.markdown(f"    - **äº‹å¯¦**: {info['factoid']['content']}")
                            st.markdown(f"      - **ç‚ºä»€éº¼é‡è¦**: {info['why']}")

                    # æå– Insightful Quotes
                    insightful_quotes = arguments_data['insightful_quotes']
                    st.markdown("\n### Insightful Quotes")
                    for quote in insightful_quotes:
                        st.markdown(f"- **å¼•ç”¨**: {quote['quote']['content']}")
                        st.markdown(f"  - **è¬›è€…**: {quote['speaker']}")
                        st.markdown(f"  - **åˆ†æ**: {quote['analysis']}")

                    # æå– Overall Summary
                    overall_summary = arguments_data['overall_summary']
                    st.markdown("### Overall Summary:")
                    st.markdown(f"{overall_summary}")

                    # æå– Next Steps
                    next_steps = arguments_data['next_steps']
                    st.markdown("\n### Next Steps")
                    
                    for step in next_steps:
                        st.markdown(f"- {step}")

                    # æå– Other Stuff
                    other_stuff = arguments_data['other_stuff']
                    st.markdown("\n### Other Stuff")
                    for item in other_stuff:
                        st.markdown(f"- **å…§å®¹**: {item['content']}")

                    # æå–ç‰¹å®šå…§å®¹
                    title = arguments_data['metadata']['title']
                    location = arguments_data['metadata']['location']['content']
                    duration = arguments_data['metadata']['duration']
                    key_moments = arguments_data['key_moments']
                    insightful_quotes = arguments_data['insightful_quotes']
                    overall_summary = arguments_data['overall_summary']
                    next_steps = arguments_data['next_steps']

                    #st.text(f"æ¨™é¡Œ: {title}")
                    #st.text(f"åœ°é»: {location}")
                    #st.text(f"æŒçºŒæ™‚é–“: {duration}")
                    #st.text(f"é—œéµæ™‚åˆ»: {key_moments}")
                    #st.text(f"æ·±åˆ»å¼•ç”¨: {insightful_quotes}")
                    #st.text(f"æ•´é«”æ‘˜è¦: {overall_summary}")
                    #st.text(f"ä¸‹ä¸€æ­¥: {next_steps}")
            
            except Exception as e:
                st.markdown(f"ç™¼ç”ŸéŒ¯èª¤: {repr(e)}")

    with tab4:
        with st.container():
            st.markdown(full_transcription)

else:
    st.stop()
