import streamlit as st
import os
import re
import time
import random
import tempfile
from openai import OpenAI
from pydub import AudioSegment
from pydub.utils import which
from concurrent.futures import ThreadPoolExecutor
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import CharacterTextSplitter
from langgraph.constants import Send
from langgraph.graph import END, START, StateGraph
from langchain_core.documents import Document
from langchain.chains.combine_documents.reduce import (
    acollapse_docs,
    split_list_of_docs,
)
import operator
import asyncio
import uuid
from typing import (
    Annotated,
    Any,
    Callable,
    Dict,
    List,
    Literal,
    Optional,
    Sequence,
    Type,
    Union,
    TypedDict,
)
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import (
    AIMessage,
    AnyMessage,
    BaseMessage,
    HumanMessage,
    ToolCall,
)
from langchain_core.prompt_values import PromptValue
from langchain_core.runnables import (
    Runnable,
    RunnableLambda,
)
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ValidationNode
from pydantic import BaseModel, Field, field_validator
import json
from langchain_core.callbacks.base import BaseCallbackHandler
from streamlit.delta_generator import DeltaGenerator
import difflib


class StreamHandler(BaseCallbackHandler):
    def __init__(self, message_container: DeltaGenerator):
        self.text = ""
        self.message_container = message_container
        self.cursor_visible = True

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.cursor_visible = not self.cursor_visible
        cursor = '<span style="color:#4B3832;font-weight:bold;">â–Œ</span>' if self.cursor_visible else '<span style="color:transparent;">â–Œ</span>'
        # é€™è£¡æ¯æ¬¡éƒ½è¦†è“‹åŒä¸€å€‹å€å¡Š
        self.message_container.markdown(self.text + cursor, unsafe_allow_html=True)
        time.sleep(0.04)
        
    def on_llm_end(self, *args, **kwargs):
        self.message_container.markdown(self.text, unsafe_allow_html=True)

# é…ç½® pydub ä½¿ç”¨ FFmpeg
AudioSegment.converter = which("ffmpeg")
AudioSegment.ffprobe = which("ffprobe")

# åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_KEY"]
client = OpenAI()

# åˆå§‹åŒ– LangChain çš„ ChatOpenAI æ¨¡å‹
llm = ChatOpenAI(model="gpt-4.1", temperature=0.0, streaming=True)

judge_llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0.0)

def llm_is_truncated(last_line: str, judge_llm=None) -> bool:
    """
    åˆ¤æ–·æœ€å¾Œä¸€è¡Œæ˜¯å¦ç‚º LLM è¼¸å‡ºçœç•¥æç¤ºã€‚
    å…ˆç”¨æ­£å‰‡åˆ¤æ–·å¸¸è¦‹èªå¥ï¼Œä¸ç¢ºå®šæ™‚æ‰ä¸Ÿçµ¦ judge_llmã€‚
    """
    if not last_line:
        return False

    # å¸¸è¦‹çœç•¥èªï¼ˆå¯æŒçºŒæ“´å……ï¼‰
    omit_patterns = [
        r"å…§å®¹éé•·",
        r"è«‹è¦‹(ä¸‹å‰‡|çºŒç¯‡|ä¸‹ç¯‡|ä¸‹æ®µ|ä¸‹æ–‡)",
        r"è«‹ç¹¼çºŒ",
        r"continue",
        r"remaining content",
        r"only partial content",
        r"åƒ…å±•ç¤ºéƒ¨åˆ†å…§å®¹",
        r"ä¸‹æ–‡è«‹è¦‹",
        r"to be continued",
        r"see next part",
        r"see continuation",
        r"see next",
        r"ï¼ˆçºŒï¼‰",
        r"â€¦$",
        r"\.\.\.$",
        r"æœªå®Œå¾…çºŒ",
        r"ä¸‹ç¯‡ç¹¼çºŒ",
        r"ä¸‹æ®µç¹¼çºŒ",
        r"see next section",
        r"see next chapter",
        r"see next page",
        r"see next message",
        r"see next reply",
        r"see next response",
        r"see next output",
        r"see next transcript",
        r"see next conversation",
        r"see next dialogue",
        r"see next dialog",
        r"see next turn",
        r"see next utterance",
        r"see next statement",
        r"see next sentence",
        r"see next paragraph",
        r"see next line",
        r"see next text",
        r"see next content",
        r"see next part",
        r"see next portion",
        r"see next segment",
        r"see next fragment",
        r"see next excerpt",
        r"see next snippet",
        r"see next chunk",
        r"see next batch",
        r"see next block",
        r"see next section",
        r"see next division",
        r"see next subdivision",
        r"see next subdivision",
        r"see next subpart",
        r"see next subsegment",
        r"see next subchunk",
        r"see next subbatch",
        r"see next subblock",
        r"see next subsection",
        r"see next subchapter",
        r"see next subpage",
        r"see next submessage",
        r"see next subreply",
        r"see next subresponse",
        r"see next suboutput",
        r"see next subtranscript",
        r"see next subconversation",
        r"see next subdialogue",
        r"see next subdialog",
        r"see next subturn",
        r"see next subutterance",
        r"see next substatement",
        r"see next subsentence",
        r"see next subparagraph",
        r"see next subline",
        r"see next subtext",
        r"see next subcontent",
        r"see next subpart",
        r"see next subportion",
        r"see next subsegment",
        r"see next subfragment",
        r"see next subexcerpt",
        r"see next subsnippet",
        r"see next subchunk",
        r"see next subbatch",
        r"see next subblock",
        r"see next subsubsection",
        r"see next subsubchapter",
        r"see next subsubpage",
        r"see next subsubmessage",
        r"see next subsubreply",
        r"see next subsubresponse",
        r"see next subsuboutput",
        r"see next subsubtranscript",
        r"see next subsubconversation",
        r"see next subsubdialogue",
        r"see next subsubdialog",
        r"see next subsubturn",
        r"see next subsubutterance",
        r"see next subsubstatement",
        r"see next subsubsentence",
        r"see next subsubparagraph",
        r"see next subsubline",
        r"see next subsubtext",
        r"see next subsubcontent",
        r"see next subsubpart",
        r"see next subsubportion",
        r"see next subsubsegment",
        r"see next subsubfragment",
        r"see next subsubexcerpt",
        r"see next subsubsnippet",
        r"see next subsubchunk",
        r"see next subsubbatch",
        r"see next subsubblock",
    ]
    for pat in omit_patterns:
        if re.search(pat, last_line, re.IGNORECASE):
            return True

    # è‹¥é‚„æ˜¯ä¸ç¢ºå®šï¼Œå†ä¸Ÿçµ¦ judge_llm
    if judge_llm:
        prompt = f"""
ä½ æ˜¯ä¸€å€‹åˆ¤æ–·åŠ©æ‰‹ã€‚è«‹åˆ¤æ–·ä¸‹é¢é€™ä¸€è¡Œæ˜¯å¦æ˜¯åœ¨è«‹æ±‚ç”¨æˆ¶çºŒæ¥å…§å®¹ã€æˆ–æ˜¯çœç•¥æç¤ºï¼ˆä¾‹å¦‚ï¼šå…§å®¹éé•·ã€åƒ…å±•ç¤ºéƒ¨åˆ†å…§å®¹ã€è«‹ç¹¼çºŒã€continueã€remaining content ç­‰ï¼‰ï¼Œè€Œä¸æ˜¯ä¸€èˆ¬å…§å®¹ã€‚
å¦‚æœæ˜¯ï¼Œè«‹å›ç­”ã€Œæ˜¯ã€ï¼›å¦‚æœä¸æ˜¯ï¼Œè«‹å›ç­”ã€Œå¦ã€ã€‚
å…§å®¹ï¼š
{last_line}
"""
        response = judge_llm.invoke(prompt)
        answer = response.content.strip()
        return answer.startswith("æ˜¯")
    return False

#def get_full_llm_output(prompt, llm, judge_llm, continue_prompt="è«‹ç¹¼çºŒ"):
#    all_content = ""
#    current_prompt = prompt
#    while True:
#        response = llm.invoke(current_prompt)
#        content = response.content.strip()
#        all_content += content + "\n"
        # å–æœ€å¾Œä¸€è¡Œ
#        last_line = content.splitlines()[-1] if content.splitlines() else ""
        # åˆ¤æ–·æ˜¯å¦è¢«æˆªæ–·
#        if not llm_is_truncated(last_line, judge_llm):
#            break
        # è‹¥è¢«æˆªæ–·ï¼Œå‰‡ç”¨çºŒæ¥æç¤º
#        current_prompt = continue_prompt
#    return all_content

#def stream_full_formatted_transcription(chain, transcription, judge_llm, max_rounds=10):
    # ç”¨ CharacterTextSplitter åˆ†æ®µ
#    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=3000, chunk_overlap=0)
#    chunks = text_splitter.split_text(transcription)
#    all_text = ""
#    for idx, chunk in enumerate(chunks):
#        message_container = st.empty()
#        handler = StreamHandler(message_container)
#        result = chain.invoke({"text": chunk}, config={"callbacks": [handler]})
#        message_container.markdown(handler.text, unsafe_allow_html=True)
#        all_text += handler.text + "\n"
#    return all_text

#def stream_full_formatted_transcription(chain, transcription, judge_llm, max_rounds=10):
#    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=3000, chunk_overlap=0)
#    chunks = text_splitter.split_text(transcription)
#    all_text = ""
#    for idx, chunk in enumerate(chunks):
#        message_container = st.empty()
#        handler = StreamHandler(message_container)
#        current_prompt = {"text": chunk}
#        round_count = 0
#        while True:
#            result = chain.invoke(current_prompt, config={"callbacks": [handler]})
#            message_container.markdown(handler.text, unsafe_allow_html=True)
#            all_text += handler.text + "\n"
#            last_line = handler.text.splitlines()[-1] if handler.text.splitlines() else ""
#            if not llm_is_truncated(last_line, judge_llm):
#                break
            # è‹¥è¢«æˆªæ–·ï¼Œå‰‡ç”¨çºŒæ¥æç¤º
#            current_prompt = {"text": "è«‹ç¹¼çºŒ"}
#            round_count += 1
#            if round_count >= max_rounds:
#                break
#    return all_text

#def stream_full_formatted_transcription(chain, transcription, judge_llm, max_rounds=10):
#    """
#    å°‡é€å­—ç¨¿åˆ†æ®µæ ¼å¼åŒ–ï¼Œé‡åˆ°çœç•¥è‡ªå‹•çºŒæ¥ï¼Œç›´åˆ°å…§å®¹å®Œæ•´æˆ–é”åˆ° max_roundsã€‚
#    """
#    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=3000, chunk_overlap=0)
#    chunks = text_splitter.split_text(transcription)
#    all_text = ""
#    for idx, chunk in enumerate(chunks):
#        message_container = st.empty()
#        handler = StreamHandler(message_container)
#        current_prompt = {"text": chunk}
#        round_count = 0
#        while True:
#            handler.text = ""  # æ¯è¼ªéƒ½é‡è¨­
#            result = chain.invoke(current_prompt, config={"callbacks": [handler]})
#            message_container.markdown(handler.text, unsafe_allow_html=True)
#            lines = handler.text.splitlines()
#            if round_count == 0:
#                all_text += handler.text + "\n"
#            else:
                # åªåŠ æ–°çºŒæ¥çš„å…§å®¹ï¼ˆå»æ‰é‡è¤‡çš„ç¬¬ä¸€è¡Œï¼‰
#                if len(lines) > 1:
#                    all_text += "\n".join(lines[1:]) + "\n"
#            last_line = lines[-1] if lines else ""
#            if not llm_is_truncated(last_line, judge_llm):
#                break
#            current_prompt = {"text": "è«‹ç¹¼çºŒ"}
#            round_count += 1
#            if round_count >= max_rounds:
                # å¯é¸ï¼šlogè­¦å‘Š
#                print(f"Warning: chunk {idx} reached max_rounds({max_rounds}) for continuation.")
#                break
#    return all_text

def split_sentences(text):
    """
    å°‡ä¸­æ–‡æ–‡æœ¬ä¾æ“šå¥è™Ÿã€å•è™Ÿã€é©šå˜†è™Ÿã€åˆ†è™Ÿã€æ›è¡Œç­‰æ¨™é»æ–·å¥ã€‚
    """
    # ä»¥æ¨™é»ç¬¦è™Ÿæˆ–æ›è¡Œç‚ºæ–·å¥ä¾æ“š
    sentences = re.split(r'([ã€‚ï¼ï¼Ÿï¼›\n])', text)
    result = []
    for i in range(0, len(sentences)-1, 2):
        result.append(sentences[i] + sentences[i+1])
    if len(sentences) % 2 != 0:
        result.append(sentences[-1])
    # å»é™¤ç©ºç™½
    return [s.strip() for s in result if s.strip()]

def get_unprocessed_sentences(original_sentences, formatted_sentences):
    # ç”¨ difflib åˆ¤æ–·å“ªäº›åŸå§‹å¥å­é‚„æ²’å‡ºç¾åœ¨æ ¼å¼åŒ–å…§å®¹
    unprocessed = []
    formatted_text = ''.join(formatted_sentences)
    for sent in original_sentences:
        # ç”¨ in æˆ–ç›¸ä¼¼åº¦åˆ¤æ–·
        if sent not in formatted_text:
            # ä¹Ÿå¯ç”¨ difflib.SequenceMatcher(None, sent, formatted_text).ratio() < 0.7
            unprocessed.append(sent)
    return unprocessed

def stream_full_formatted_transcription(chain, transcription, judge_llm, max_rounds=15):
    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=2000, chunk_overlap=0)
    chunks = text_splitter.split_text(transcription)
    all_text = ""
    for idx, chunk in enumerate(chunks):
        message_container = st.empty()
        handler = StreamHandler(message_container)
        remaining_text = chunk
        round_count = 0
        while remaining_text and round_count < max_rounds:
            handler.text = ""
            result = chain.invoke({"text": remaining_text}, config={"callbacks": [handler]})
            message_container.markdown(handler.text, unsafe_allow_html=True)
            # åˆ†å¥
            original_sentences = split_sentences(remaining_text)
            formatted_sentences = split_sentences(handler.text)
            # æ¯”å°
            unprocessed = get_unprocessed_sentences(original_sentences, formatted_sentences)
            # åˆ¤æ–·æ˜¯å¦è¢«æˆªæ–·
            last_line = handler.text.strip().split('\n')[-1]
            if llm_is_truncated(last_line, judge_llm) or unprocessed:
                # åªé€é‚„æ²’è™•ç†çš„å¥å­
                remaining_text = ''.join(unprocessed)
                round_count += 1
            else:
                all_text += handler.text + "\n"
                break
        if round_count >= max_rounds:
            print(f"Warning: chunk {idx} reached max_rounds({max_rounds}) for continuation.")
    # ç§»é™¤æ‰€æœ‰ã€Œè«‹ç¹¼çºŒã€ç­‰å­—çœ¼
    all_text = re.sub(r"(è«‹ç¹¼çºŒ|å…§å®¹éé•·|è¦‹ä¸‹å‰‡ç¹¼çºŒ)", "", all_text, flags=re.I)
    return all_text

def beautify_transcript(text):
    # 1. ä¸»é¡ŒåŠ ç²—
    text = re.sub(r'^(ä¸»é¡Œï¼š.*)$', r'**\1**', text, flags=re.MULTILINE)
    # 2. ä¸»é¡Œå¾ŒåŠ ç©ºè¡Œï¼ˆå¦‚æœæ²’æœ‰çš„è©±ï¼‰
    text = re.sub(r'(\*\*ä¸»é¡Œï¼š.*?\*\*)(\n)(?!\n)', r'\1\n\n', text)
    # 3. æ®µè½é–“åŠ ç©ºè¡Œï¼ˆå…©è¡Œä»¥ä¸Šä¸é‡è¤‡ï¼‰
    text = re.sub(r'([^\n])\n([^\n])', r'\1\n\n\2', text)
    # 4. é—œéµè©é«˜äº®ï¼ˆå¯è‡ªè¡Œæ“´å……ï¼‰
    #keywords = [
    #    'non-bank f.i.', 'msr', 'ltv', 'NPL', 'FTP', 'SPA', 'ICR', 'LTC', 'provision',
    #    'syndication', 'refinance', 'credit', 'æŠ•è³‡ç­‰ç´š', 'ä¿¡è©•', 'å¹³ç­‰', 'é æ”¾æ¯”', 'é›†ä¸­åº¦'
    #]
    #for kw in keywords:
        # é¿å…é‡è¤‡åŠ ç²—
    #    text = re.sub(rf'(?<!\*)({re.escape(kw)})(?!\*)', r'**\1**', text)
    # 5. ã€ç–‘ä¼¼éŒ¯èª¤ã€‘æ¨™ç´…
    text = re.sub(r'ã€ç–‘ä¼¼éŒ¯èª¤ã€‘', r'<span style="color:red">ã€ç–‘ä¼¼éŒ¯èª¤ã€‘</span>', text)
    # 6. ç§»é™¤å¤šé¤˜ç©ºè¡Œï¼ˆæœ€å¤šå…©è¡Œï¼‰
    text = re.sub(r'\n{3,}', '\n\n', text)
    # 7. å»é™¤é–‹é ­å¤šé¤˜ç©ºè¡Œ
    text = text.lstrip('\n')
    return text


# è¨­ç½®ç¶²é æ¨™é¡Œå’Œåœ–æ¨™
st.set_page_config(page_title="Speech to Text Transcription", layout="wide", page_icon="ğŸ‘„")
#st.title("Speech to text transcription")

# å‰µå»ºä¸€å€‹è¡¨å–®ä¾†ä¸Šå‚³æ–‡ä»¶
with st.expander(" Speech to text transcription", expanded=True, icon="ğŸ‘„"):
    with st.form(key="my_form"):
        f = st.file_uploader("Upload your audio file", type=["wav", "mp3", "mp4", "mpeg", "mpga", "m4a", "webm"])
        st.info("ğŸ‘† ä¸Šå‚³ä¸€å€‹éŸ³æ•ˆæ–‡ä»¶ï¼ˆæ”¯æ´ .wav, .mp3, .mp4, .mpeg, .mpga, .m4a, .wav, .webmï¼‰ã€‚")
        submit_button = st.form_submit_button(label="Transcribe")

# å®šç¾©ç”Ÿæˆå™¨å‡½æ•¸ä¾†é€æ­¥ç”¢ç”Ÿè½‰éŒ„æ–‡æœ¬
def stream_transcription(transcription_text):
    message_container = st.empty()
    text = ""
    for word in transcription_text.split():
        text += word + " "
        message_container.markdown(text)
        time.sleep(0.05)  # æ§åˆ¶æ‰“å­—æ©Ÿæ•ˆæœçš„é€Ÿåº¦

# å®šç¾©è½‰éŒ„éŸ³é »å¡Šçš„å‡½æ•¸
def transcribe_chunk(chunk, index):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as temp_mp3_file:
        chunk.export(temp_mp3_file.name, format="mp3")
        with open(temp_mp3_file.name, "rb") as audio_file:
            transcription = client.audio.transcriptions.create(
                #model="whisper-1",
                model="gpt-4o-transcribe",
                file=audio_file,
                response_format="text",
                prompt="This audio contains a discussion or presentation. Always preserve the original language of each sentence. If a sentence is in English, output it in English; if in Chinese, output it in Traditional Chinese; if mixed, output the original mixed-language sentence. Do not translate or alter the language. The audio may cover various topics such as updates, feedback, or informative lectures."
            )
        os.remove(temp_mp3_file.name)
    return transcription.lower()

# ä½¿ç”¨ Streamlit çš„ç·©å­˜åŠŸèƒ½ä¾†ç·©å­˜è½‰éŒ„çµæœ
@st.cache_data
def transcribe_audio(file_path):
    # é€™è£¡æ”¾ç½®è½‰éŒ„é‚è¼¯
    # è¿”å›è½‰éŒ„çµæœ
    return full_transcription

@st.cache_data
def format_transcription(transcription):
    # ä½¿ç”¨ LangChain æ”¹å–„æ–‡æœ¬æ ¼å¼
    return chain.invoke({"text": transcription})

# å®šç¾© LangChain çš„ PromptTemplate
prompt_template = PromptTemplate(
    input_variables=["text"],
    template=(
        "# Role and Objective\n"
        "You are an expert transcription editor. Your task is to improve the formatting and readability of a raw transcription text, while strictly preserving the original wording, sentence order, and information.\n\n"
        "# Instructions\n"
        "- **Do not change the meaning, order, or language of any sentence.**\n"
        "- If the text is in Chinese, always use Traditional Chinese (ç¹é«”ä¸­æ–‡)ã€‚\n"
        "- **List the text sentence by sentence, each on a new line.**\n"
        "- **Do not merge, split, or paraphrase sentences.**\n"
        "- **Do not add or remove any content.**\n"
        "- **Do not translate.**\n"
        "- If you find a sentence that is incomplete or has obvious errors, mark it with ã€ç–‘ä¼¼éŒ¯èª¤ã€‘ at the end of the sentence.\n"
        "- If the input is extremely long, process all content in full (do not skip or summarize any part).\n\n"
        "## Formatting Rules\n"
        "1. **Add appropriate headings and subheadings** to organize the content. Use a consistent style (e.g., headings in bold, subheadings in italics).\n"
        "2. **Highlight key terms or important concepts** using bold or italics for emphasis.\n"
        "3. **Check and correct any grammatical or spelling errors** (but do not change the original meaning or sentence structure).\n"
        "4. **Add appropriate punctuation** and split run-on sentences for better readability, but do not merge or paraphrase sentences.\n"
        "5. **Divide the text into paragraphs** where appropriate, and ensure there is a blank line between paragraphs.\n"
        "6. **Preserve the original language and style.**\n\n"
        "# Reasoning Steps\n"
        "1. Analyze the input text to identify logical sections and possible headings.\n"
        "2. For each sentence, check for grammar, spelling, and punctuation issues, and correct them if needed.\n"
        "3. Highlight key terms or concepts.\n"
        "4. Organize sentences into paragraphs and insert headings/subheadings as appropriate.\n"
        "5. Mark any incomplete or obviously erroneous sentences with ã€ç–‘ä¼¼éŒ¯èª¤ã€‘.\n"
        "6. Output the result in markdown format, using bold for headings, italics for subheadings, and bold/italics for key terms.\n\n"
        "# Output Format\n"
        "- Use markdown formatting.\n"
        "- Headings: **bold**\n"
        "- Subheadings: *italics*\n"
        "- Key terms: **bold** or *italics*\n"
        "- Each sentence on a new line, in original order.\n"
        "- Blank line between paragraphs.\n"
        "- Mark incomplete or erroneous sentences with ã€ç–‘ä¼¼éŒ¯èª¤ã€‘.\n\n"
        "# Example\n"
        "## Input\n"
        "text: é€™æ˜¯ä¸€æ®µé€å­—ç¨¿å…§å®¹ã€‚ä»Šå¤©æˆ‘å€‘è¦è¨è«–äººå·¥æ™ºæ…§ã€‚AIçš„æ‡‰ç”¨è¶Šä¾†è¶Šå»£æ³›ï¼Œç‰¹åˆ¥æ˜¯åœ¨é†«ç™‚å’Œæ•™è‚²é ˜åŸŸã€‚é€™è£¡æœ‰ä¸€å€‹ä¾‹å­AIå¯ä»¥å”åŠ©é†«ç”Ÿè¨ºæ–·ç–¾ç—…ã€‚è¬è¬å¤§å®¶çš„è†è½ã€‚\n\n"
        "## Output\n"
        "**ä¸»é¡Œï¼šäººå·¥æ™ºæ…§çš„æ‡‰ç”¨**\n\n"
        "*å¼•è¨€*\n"
        "é€™æ˜¯ä¸€æ®µé€å­—ç¨¿å…§å®¹ã€‚\n"
        "ä»Šå¤©æˆ‘å€‘è¦è¨è«–**äººå·¥æ™ºæ…§**ã€‚\n\n"
        "*AIçš„æ‡‰ç”¨*\n"
        "**AI**çš„æ‡‰ç”¨è¶Šä¾†è¶Šå»£æ³›ï¼Œç‰¹åˆ¥æ˜¯åœ¨**é†«ç™‚**å’Œ**æ•™è‚²**é ˜åŸŸã€‚\n"
        "é€™è£¡æœ‰ä¸€å€‹ä¾‹å­ï¼š**AI**å¯ä»¥å”åŠ©é†«ç”Ÿè¨ºæ–·ç–¾ç—…ã€‚\n\n"
        "*çµèª*\n"
        "è¬è¬å¤§å®¶çš„è†è½ã€‚\n"
        "\n"
        "# Final Instructions\n"
        "- Think step by step. Carefully follow all formatting and output rules.\n"
        "- **ä½ å¿…é ˆå®Œæ•´è¼¸å‡ºæ‰€æœ‰å…§å®¹ï¼Œåš´ç¦åªå±•ç¤ºéƒ¨åˆ†å…§å®¹æˆ–ä»¥ä»»ä½•å½¢å¼è¦æ±‚ç”¨æˆ¶è‡ªè¡Œè™•ç†å‰©é¤˜å…§å®¹ã€‚**\n"
        "- **åš´ç¦å‡ºç¾ã€Œåƒ…å±•ç¤ºéƒ¨åˆ†å…§å®¹ã€ã€ã€Œå¾ŒçºŒå…§å®¹è«‹ä¾è¦å‰‡æŒçºŒè™•ç†ã€ç­‰å­—çœ¼ã€‚**"
        "- **å³ä½¿å…§å®¹æ¥µé•·ï¼Œä¹Ÿå¿…é ˆç›¡å¯èƒ½å®Œæ•´è¼¸å‡ºï¼Œç›´åˆ°å¹³å°å›æ‡‰é•·åº¦é”åˆ°æ¥µé™ç‚ºæ­¢ã€‚**"
        "- **å¦‚æœå…§å®¹éé•·å°è‡´ç„¡æ³•ä¸€æ¬¡è¼¸å‡ºå…¨éƒ¨ï¼Œè«‹è‡ªå‹•ç¹¼çºŒè¼¸å‡ºå‰©é¤˜å…§å®¹ï¼Œç›´åˆ°å…¨éƒ¨å…§å®¹éƒ½å·²å‘ˆç¾ï¼Œä¸”æ¯æ¬¡å›æ‡‰éƒ½åªè¼¸å‡ºå…§å®¹æœ¬èº«ï¼Œä¸è¦åŠ ä»»ä½•èªªæ˜æˆ–çœç•¥æç¤ºã€‚**"
        "- **Always preserve the original language of each sentence. If a sentence is in English, output it in English; if in Chinese, output it in Traditional Chinese; if mixed, output the original mixed-language sentence. Do not translate or alter the language.**"
        "- Output only the formatted text in markdown, no extra explanation."
        "## Input Text\n"
        "{text}\n"
    )
)

# å‰µå»ºä¸€å€‹è™•ç†éˆ
formatting_chain = prompt_template | llm | StrOutputParser()

# åˆ†å‰²æ–‡ä»¶
text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=1500, chunk_overlap=100)

token_max = 250000

# å®šç¾©ç‹€æ…‹é¡å‹
class SummaryState(TypedDict):
    content: str

class OverallState(TypedDict):
    contents: List[str]
    summaries: Annotated[list, operator.add]
    collapsed_summaries: List[Document]
    final_summary: str

# ç”Ÿæˆæ‘˜è¦
async def generate_summary(state: SummaryState):
    response = await map_chain.ainvoke(state["content"])
    return {"summaries": [response]}

# æ˜ å°„æ‘˜è¦
def map_summaries(state: OverallState):
    return [Send("generate_summary", {"content": content}) for content in state["contents"]]

# è¨ˆç®—æ–‡ä»¶é•·åº¦
def length_function(documents: List[Document]) -> int:
    return sum(llm.get_num_tokens(doc.page_content) for doc in documents)

# æ”¶é›†æ‘˜è¦
def collect_summaries(state: OverallState):
    return {"collapsed_summaries": [Document(summary) for summary in state["summaries"]]}

# ç”Ÿæˆæœ€çµ‚æ‘˜è¦
async def generate_final_summary(state: OverallState):
    response = await reduce_chain.ainvoke(state["collapsed_summaries"])
    return {"final_summary": response}

# æ‘ºç–Šæ‘˜è¦
async def collapse_summaries(state: OverallState):
    doc_lists = split_list_of_docs(state["collapsed_summaries"], length_function, token_max)
    results = []
    for doc_list in doc_lists:
        results.append(await acollapse_docs(doc_list, reduce_chain.ainvoke))
    return {"collapsed_summaries": results}

# åˆ¤æ–·æ˜¯å¦éœ€è¦æ‘ºç–Š
def should_collapse(state: OverallState) -> Literal["collapse_summaries", "generate_final_summary"]:
    num_tokens = length_function(state["collapsed_summaries"])
    if num_tokens > token_max:
        return "collapse_summaries"
    else:
        return "generate_final_summary"

# å»ºç«‹ç‹€æ…‹åœ–
graph = StateGraph(OverallState)
graph.add_node("generate_summary", generate_summary)
graph.add_node("collect_summaries", collect_summaries)
graph.add_node("generate_final_summary", generate_final_summary)
graph.add_node("collapse_summaries", collapse_summaries)

# æ·»åŠ é‚Š
graph.add_conditional_edges(START, map_summaries, ["generate_summary"])
graph.add_edge("generate_summary", "collect_summaries")
graph.add_conditional_edges("collect_summaries", should_collapse)
graph.add_conditional_edges("collapse_summaries", should_collapse)
graph.add_edge("generate_final_summary", END)

# ç·¨è­¯æ‡‰ç”¨ç¨‹å¼
app = graph.compile()

# å®šç¾©æç¤ºæ¨¡æ¿
map_template = """
# è§’è‰²èˆ‡ç›®æ¨™
ä½ æ˜¯ä¸€ä½å°ˆæ¥­é€å­—ç¨¿åˆ†æå¸«ï¼Œè«‹é–±è®€ä¸‹æ–¹é€å­—ç¨¿åˆ†æ®µå…§å®¹ï¼Œç¯©é¸å‡ºæ‰€æœ‰çœŸæ­£é‡è¦çš„ä¸»é¡Œèˆ‡é‡é»ï¼Œä¸¦é‡å°æ¯å€‹é‡é»é€²è¡Œè©³ç´°èªªæ˜ã€‚**åƒ…æ ¹æ“šæœ¬æ®µå…§å®¹ï¼Œä¸å¯è£œå……å¤–éƒ¨çŸ¥è­˜æˆ–æ¨æ¸¬æœªæ˜èªªçš„å…§å®¹ã€‚**

# æŒ‡ä»¤
- åªæ ¹æ“šæœ¬æ®µå…§å®¹ï¼Œåš´ç¦è£œå……å¤–éƒ¨çŸ¥è­˜æˆ–æ¨è«–ã€‚
- ç¯©é¸å‡ºæ‰€æœ‰æ˜ç¢ºä¸”é‡è¦çš„ä¸»é¡Œèˆ‡å­ä¸»é¡Œã€‚
- æ¯å€‹ä¸»é¡Œä¸‹ï¼Œæ¢åˆ—çœŸæ­£é‡è¦çš„é‡é»ï¼Œä¸¦é‡å°æ¯å€‹é‡é»é€²è¡Œè©³ç´°èªªæ˜ï¼ˆèªªæ˜å…§å®¹éœ€æ ¹æ“šæœ¬æ®µå…§å®¹ï¼ŒåŒ…å«èƒŒæ™¯ã€åŸå› ã€å½±éŸ¿ã€ç´°ç¯€ç­‰ï¼‰ã€‚
- è‹¥æœ¬æ®µå…§å®¹æœ‰æ˜ç¢ºçš„æ±ºç­–ã€è¡Œå‹•é …ç›®ã€å› æœé—œä¿‚ï¼Œä¹Ÿè«‹è©³ç´°èªªæ˜ã€‚
- è‹¥ç™¼ç¾æœ¬æ®µé‡é»å¯èƒ½èˆ‡å…¶ä»–æ®µè½æœ‰é—œè¯æˆ–å°šæœªå®Œæ•´ï¼Œè«‹æ˜ç¢ºæ¨™è¨»ã€Œæ­¤é‡é»å¯èƒ½éœ€èˆ‡å…¶ä»–æ®µè½åˆä½µè£œå…¨ã€ã€‚
- ä½¿ç”¨æ¸…æ¥šçš„åˆ†å±¤æ¢åˆ—æ ¼å¼ï¼š
    - ä¸»é¡Œç”¨ã€Œã€ä¸»é¡Œã€‘ã€
    - å­ä¸»é¡Œç”¨ã€Œã€å­ä¸»é¡Œã€‘ã€
    - é‡é»ç”¨ã€Œ-ã€
    - æ¯å€‹é‡é»ä¸‹æ–¹ç”¨ç¸®æ’æ–¹å¼è©³ç´°èªªæ˜ï¼ˆå¯å¤šè¡Œï¼‰ã€‚
    - é‡è¦è©å½™ç”¨å…¨å½¢æ‹¬è™Ÿï¼ˆå¦‚ã€é‡é»ã€‘ï¼‰
- å›ç­”è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚

# æ¨ç†æ­¥é©Ÿ
1. å®Œæ•´é–±è®€æœ¬æ®µå…§å®¹ã€‚
2. ç¯©é¸å‡ºæ‰€æœ‰çœŸæ­£é‡è¦çš„ä¸»é¡Œèˆ‡å­ä¸»é¡Œã€‚
3. æ¢åˆ—æ¯å€‹ä¸»é¡Œä¸‹çš„é‡è¦é‡é»ï¼Œä¸¦é‡å°æ¯å€‹é‡é»é€²è¡Œè©³ç´°èªªæ˜ï¼ˆèªªæ˜å…§å®¹éœ€æ ¹æ“šæœ¬æ®µå…§å®¹ï¼‰ã€‚
4. è‹¥æœ‰æ˜ç¢ºæ±ºç­–ã€è¡Œå‹•é …ç›®ã€å› æœé—œä¿‚ï¼Œä¹Ÿè«‹è©³ç´°èªªæ˜ã€‚
5. è‹¥ç™¼ç¾é‡é»å¯èƒ½èˆ‡å…¶ä»–æ®µè½æœ‰é—œè¯æˆ–å°šæœªå®Œæ•´ï¼Œè«‹æ˜ç¢ºæ¨™è¨»ã€‚
6. ä¸å¯è£œå……å¤–éƒ¨çŸ¥è­˜æˆ–æ¨è«–ã€‚

# è¼¸å‡ºæ ¼å¼
- ä¾ä¸»é¡Œåˆ†æ®µï¼Œä¸»é¡Œç”¨ã€Œã€ä¸»é¡Œã€‘ã€ï¼Œå­ä¸»é¡Œç”¨ã€Œã€å­ä¸»é¡Œã€‘ã€ï¼Œé‡é»ç”¨ã€Œ-ã€æ¢åˆ—ï¼Œé‡é»ä¸‹æ–¹ç”¨ç¸®æ’è©³ç´°èªªæ˜ã€‚
- è‹¥æœ‰è·¨æ®µé‡é»ï¼Œè«‹æ–¼é‡é»èªªæ˜å¾ŒåŠ è¨»ã€Œï¼ˆæ­¤é‡é»å¯èƒ½éœ€èˆ‡å…¶ä»–æ®µè½åˆä½µè£œå…¨ï¼‰ã€ã€‚

# ç¯„ä¾‹
ã€ä¸»é¡Œã€‘å¸‚å ´ç­–ç•¥
- ã€é‡é»è§€å¯Ÿã€‘ï¼šæœ¬æ¬¡æœƒè­°å¼·èª¿å¸‚å ´å¤šå…ƒåŒ–ç­–ç•¥ã€‚
    å¤šä½èˆ‡æœƒè€…èªç‚ºç¾æœ‰å¸‚å ´å·²è¶¨æ–¼é£½å’Œï¼Œå› æ­¤æå‡ºæ‡‰ç©æ¥µé–‹ç™¼æ–°èˆˆå¸‚å ´ï¼Œä»¥åˆ†æ•£é¢¨éšªä¸¦å°‹æ±‚æˆé•·å‹•èƒ½ã€‚
- ã€æ•¸æ“šæ”¯æŒã€‘ï¼š2024å¹´é è¨ˆæˆé•·20%ã€‚
    è²¡å‹™éƒ¨é–€å ±å‘ŠæŒ‡å‡ºï¼Œè‹¥èƒ½é †åˆ©æ¨å‹•å¤šå…ƒåŒ–ç­–ç•¥ï¼Œ2024å¹´ç‡Ÿæ”¶æœ‰æœ›æˆé•·20%ã€‚
- ã€æ±ºç­–ã€‘ï¼šå°‡å„ªå…ˆæŠ•å…¥æ–°èˆˆå¸‚å ´ã€‚
    ç¶“éè¨è«–å¾Œï¼Œæ±ºè­°å°‡è³‡æºå„ªå…ˆé…ç½®æ–¼æ–°èˆˆå¸‚å ´ï¼Œä¸¦æˆç«‹å°ˆæ¡ˆå°çµ„è² è²¬åŸ·è¡Œã€‚

ã€ä¸»é¡Œã€‘ç”¢å“é–‹ç™¼
- ã€æ¸¬è©¦é€²åº¦ã€‘ï¼šç›®å‰ç”¢å“æ¸¬è©¦é€²åº¦è½å¾Œã€‚
    ç ”ç™¼éƒ¨é–€å›å ±ï¼Œå› äººåŠ›è³‡æºä¸è¶³åŠéƒ¨åˆ†æŠ€è¡“ç“¶é ¸ï¼Œå°è‡´ç”¢å“æ¸¬è©¦é€²åº¦è¼ƒåŸè¨ˆç•«å»¶é²å…©é€±ã€‚ï¼ˆæ­¤é‡é»å¯èƒ½éœ€èˆ‡å…¶ä»–æ®µè½åˆä½µè£œå…¨ï¼‰

# é€å­—ç¨¿åˆ†æ®µå…§å®¹
{context}

# æœ€çµ‚æŒ‡ä»¤
è«‹å‹™å¿…åªæ ¹æ“šæœ¬æ®µå…§å®¹ç¯©é¸ä¸¦è©³ç´°èªªæ˜æ¯å€‹é‡é»ï¼Œä¸è¦è£œå……å¤–éƒ¨çŸ¥è­˜æˆ–æ¨æ¸¬æœªæ˜èªªçš„å…§å®¹ã€‚å¦‚æœ‰è·¨æ®µé‡é»ï¼Œè«‹æ˜ç¢ºæ¨™è¨»ã€‚
"""

reduce_template = """
# è§’è‰²èˆ‡ç›®æ¨™
ä½ æ˜¯ä¸€ä½è³‡æ·±é€å­—ç¨¿åˆ†æå¸«ï¼Œè«‹å°‡ä¸‹æ–¹å¤šå€‹åˆ†æ®µä¸»é¡Œæ‘˜è¦é€²è¡Œåˆä½µã€å»é‡ã€è£œå…¨èˆ‡åˆ†å±¤æ•´ç†ï¼Œä¸¦é‡å°æ¯å€‹ä¸»é¡Œèˆ‡é‡é»é€²è¡Œè©³ç´°èªªæ˜ã€‚**åƒ…æ ¹æ“šä¸»é¡Œæ‘˜è¦å…§å®¹ï¼Œä¸å¯è£œå……å¤–éƒ¨çŸ¥è­˜æˆ–æ¨æ¸¬æœªæ˜èªªçš„å…§å®¹ã€‚**

# æŒ‡ä»¤
- åªæ ¹æ“šä¸‹æ–¹ä¸»é¡Œæ‘˜è¦å…§å®¹ï¼Œåš´ç¦è£œå……å¤–éƒ¨çŸ¥è­˜æˆ–æ¨è«–ã€‚
- å°‡ç›¸é—œä¸»é¡Œæ­¸ç´ç‚ºå¤§é¡ï¼Œä¸¦æ–¼æ¯å€‹å¤§é¡ä¸‹æ¢åˆ—æ‰€æœ‰é‡è¦é‡é»ï¼Œé‡å°æ¯å€‹é‡é»é€²è¡Œè©³ç´°èªªæ˜ï¼ˆèªªæ˜å…§å®¹éœ€æ ¹æ“šæ‘˜è¦å…§å®¹ï¼‰ã€‚
- åˆä½µä¸»é¡Œæ™‚ï¼Œåƒ…åœ¨æ‘˜è¦å…§å®¹æ˜ç¢ºé¡¯ç¤ºé—œè¯æ™‚æ‰åˆä½µï¼Œä¸¦è£œå…¨è·¨æ®µé‡é»ï¼Œä½¿å…¶å…§å®¹å®Œæ•´ã€‚
- å°æ‰€æœ‰é‡é»é€²è¡Œå»é‡ã€è£œå…¨ã€åˆ†å±¤ï¼Œä¸¦æª¢æŸ¥æ˜¯å¦æœ‰éºæ¼ä¸»é¡Œæˆ–é‡é»ã€‚
- è‹¥æœ‰æ˜ç¢ºæ±ºç­–ã€è¡Œå‹•é …ç›®ã€å› æœé—œä¿‚ï¼Œä¹Ÿè«‹è©³ç´°èªªæ˜ã€‚
- ä½¿ç”¨æ¸…æ¥šçš„åˆ†å±¤æ¢åˆ—æ ¼å¼ï¼š
    - ä¸»é¡Œç”¨ã€Œã€ä¸»é¡Œã€‘ã€
    - å­ä¸»é¡Œç”¨ã€Œã€å­ä¸»é¡Œã€‘ã€
    - é‡é»ç”¨ã€Œ-ã€
    - æ¯å€‹é‡é»ä¸‹æ–¹ç”¨ç¸®æ’æ–¹å¼è©³ç´°èªªæ˜ï¼ˆå¯å¤šè¡Œï¼‰ã€‚
- å›ç­”è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚

# æ¨ç†æ­¥é©Ÿ
1. é–±è®€æ‰€æœ‰ä¸»é¡Œæ‘˜è¦ã€‚
2. æ­¸ç´ã€åˆä½µç›¸é—œä¸»é¡Œï¼ˆåƒ…é™æ˜ç¢ºé—œè¯ï¼‰ï¼Œä¸¦è£œå…¨è·¨æ®µé‡é»ã€‚
3. æ¢åˆ—æ¯å€‹å¤§é¡ä¸‹çš„é‡è¦é‡é»ï¼Œä¸¦é‡å°æ¯å€‹é‡é»é€²è¡Œè©³ç´°èªªæ˜ï¼ˆèªªæ˜å…§å®¹éœ€æ ¹æ“šæ‘˜è¦å…§å®¹ï¼‰ã€‚
4. å°æ‰€æœ‰é‡é»é€²è¡Œå»é‡ã€è£œå…¨ã€åˆ†å±¤ï¼Œä¸¦æª¢æŸ¥æ˜¯å¦æœ‰éºæ¼ä¸»é¡Œæˆ–é‡é»ã€‚
5. è‹¥æœ‰æ˜ç¢ºæ±ºç­–ã€è¡Œå‹•é …ç›®ã€å› æœé—œä¿‚ï¼Œä¹Ÿè«‹è©³ç´°èªªæ˜ã€‚
6. ä¸å¯è£œå……å¤–éƒ¨çŸ¥è­˜æˆ–æ¨è«–ã€‚

# è¼¸å‡ºæ ¼å¼
- ä¾å¤§é¡åˆ†æ®µï¼Œä¸»é¡Œç”¨ã€Œã€ä¸»é¡Œã€‘ã€ï¼Œå­ä¸»é¡Œç”¨ã€Œã€å­ä¸»é¡Œã€‘ã€ï¼Œé‡é»ç”¨ã€Œ-ã€æ¢åˆ—ï¼Œé‡é»ä¸‹æ–¹ç”¨ç¸®æ’è©³ç´°èªªæ˜ã€‚

# ç¯„ä¾‹
ã€ä¸»é¡Œã€‘å¸‚å ´ç­–ç•¥
- ã€é‡é»è§€å¯Ÿã€‘ï¼šå¼·èª¿å¸‚å ´å¤šå…ƒåŒ–ã€‚
    æœƒè­°ä¸­å¤šä½ä¸»ç®¡èªç‚ºç¾æœ‰å¸‚å ´æˆé•·æœ‰é™ï¼Œéœ€ç©æ¥µé–‹ç™¼æ–°èˆˆå¸‚å ´ä»¥åˆ†æ•£é¢¨éšªã€‚
- ã€æ±ºç­–ã€‘ï¼šå„ªå…ˆæŠ•å…¥æ–°èˆˆå¸‚å ´ã€‚
    æ±ºè­°å°‡è³‡æºå„ªå…ˆé…ç½®æ–¼æ–°èˆˆå¸‚å ´ï¼Œä¸¦æˆç«‹å°ˆæ¡ˆå°çµ„è² è²¬åŸ·è¡Œã€‚

ã€ä¸»é¡Œã€‘ç”¢å“é–‹ç™¼
- ã€æ¸¬è©¦é€²åº¦ã€‘ï¼šç”¢å“æ¸¬è©¦é€²åº¦è½å¾Œã€‚
    ç ”ç™¼éƒ¨é–€å›å ±å› äººåŠ›ä¸è¶³åŠæŠ€è¡“ç“¶é ¸ï¼Œå°è‡´æ¸¬è©¦å»¶é²å…©é€±ã€‚æ­¤é‡é»å·²æ•´åˆæ‰€æœ‰ç›¸é—œæ®µè½è³‡è¨Šã€‚
- ã€è¡Œå‹•é …ç›®ã€‘ï¼šåŠ æ´¾äººåŠ›æ”¯æ´æ¸¬è©¦ã€‚
    æœƒè­°æ±ºè­°ç”±å…¶ä»–éƒ¨é–€èª¿æ´¾äººåŠ›æ”¯æ´ï¼Œç¢ºä¿ç”¢å“å¦‚æœŸä¸Šå¸‚ã€‚

# ä¸»é¡Œæ‘˜è¦å…§å®¹
{docs}

# æœ€çµ‚æŒ‡ä»¤
è«‹å‹™å¿…åªæ ¹æ“šä¸»é¡Œæ‘˜è¦å…§å®¹æ­¸ç´ã€å»é‡ã€è£œå…¨ä¸¦è©³ç´°èªªæ˜æ¯å€‹é‡é»ï¼Œä¸è¦è£œå……å¤–éƒ¨çŸ¥è­˜æˆ–æ¨æ¸¬æœªæ˜èªªçš„å…§å®¹ã€‚ç‰¹åˆ¥æ³¨æ„è·¨æ®µé‡é»çš„æ•´åˆèˆ‡è£œå…¨ã€‚
"""

map_prompt = ChatPromptTemplate([("human", map_template)])
reduce_prompt = ChatPromptTemplate([("human", reduce_template)])

map_chain = map_prompt | llm | StrOutputParser()
reduce_chain = reduce_prompt | llm | StrOutputParser()

# å®šç¾©é‹è¡Œæ‡‰ç”¨ç¨‹å¼çš„ç•°æ­¥å‡½æ•¸
async def run_app(split_docs):
    async for step in app.astream(
        {"contents": [doc.page_content for doc in split_docs]},
        {"recursion_limit": 100},
    ):
        pass  # é€™è£¡ä¸éœ€è¦ä»»ä½•æ“ä½œï¼Œåªæ˜¯ç­‰å¾…å®Œæˆ
    return step['generate_final_summary']['final_summary']

# è§£æçš„éƒ¨åˆ†
def _default_aggregator(messages: Sequence[AnyMessage]) -> AIMessage:
    for m in messages[::-1]:
        if m.type == "ai":
            return m
    raise ValueError("No AI message found in the sequence.")


class RetryStrategy(TypedDict, total=False):
    """The retry strategy for a tool call."""

    max_attempts: int
    """The maximum number of attempts to make."""
    fallback: Optional[
        Union[
            Runnable[Sequence[AnyMessage], AIMessage],
            Runnable[Sequence[AnyMessage], BaseMessage],
            Callable[[Sequence[AnyMessage]], AIMessage],
        ]
    ]
    """The function to use once validation fails."""
    aggregate_messages: Optional[Callable[[Sequence[AnyMessage]], AIMessage]]


def _bind_validator_with_retries(
    llm: Union[
        Runnable[Sequence[AnyMessage], AIMessage],
        Runnable[Sequence[BaseMessage], BaseMessage],
    ],
    *,
    validator: ValidationNode,
    retry_strategy: RetryStrategy,
    tool_choice: Optional[str] = None,
) -> Runnable[Union[List[AnyMessage], PromptValue], AIMessage]:
    """Binds a tool validators + retry logic to create a runnable validation graph.

    LLMs that support tool calling can generate structured JSON. However, they may not always
    perfectly follow your requested schema, especially if the schema is nested or has complex
    validation rules. This method allows you to bind a validation function to the LLM's output,
    so that any time the LLM generates a message, the validation function is run on it. If
    the validation fails, the method will retry the LLM with a fallback strategy, the simplest
    being just to add a message to the output with the validation errors and a request to fix them.

    The resulting runnable expects a list of messages as input and returns a single AI message.
    By default, the LLM can optionally NOT invoke tools, making this easier to incorporate into
    your existing chat bot. You can specify a tool_choice to force the validator to be run on
    the outputs.

    Args:
        llm (Runnable): The llm that will generate the initial messages (and optionally fallba)
        validator (ValidationNode): The validation logic.
        retry_strategy (RetryStrategy): The retry strategy to use.
            Possible keys:
            - max_attempts: The maximum number of attempts to make.
            - fallback: The LLM or function to use in case of validation failure.
            - aggregate_messages: A function to aggregate the messages over multiple turns.
                Defaults to fetching the last AI message.
        tool_choice: If provided, always run the validator on the tool output.

    Returns:
        Runnable: A runnable that can be invoked with a list of messages and returns a single AI message.
    """

    def add_or_overwrite_messages(left: list, right: Union[list, dict]) -> list:
        """Append messages. If the update is a 'finalized' output, replace the whole list."""
        if isinstance(right, dict) and "finalize" in right:
            finalized = right["finalize"]
            if not isinstance(finalized, list):
                finalized = [finalized]
            for m in finalized:
                if m.id is None:
                    m.id = str(uuid.uuid4())
            return finalized
        res = add_messages(left, right)
        if not isinstance(res, list):
            return [res]
        return res

    class State(TypedDict):
        messages: Annotated[list, add_or_overwrite_messages]
        attempt_number: Annotated[int, operator.add]
        initial_num_messages: int
        input_format: Literal["list", "dict"]

    builder = StateGraph(State)

    def dedict(x: State) -> list:
        """Get the messages from the state."""
        return x["messages"]

    model = dedict | llm | (lambda msg: {"messages": [msg], "attempt_number": 1})
    fbrunnable = retry_strategy.get("fallback")
    if fbrunnable is None:
        fb_runnable = llm
    elif isinstance(fbrunnable, Runnable):
        fb_runnable = fbrunnable  # type: ignore
    else:
        fb_runnable = RunnableLambda(fbrunnable)
    fallback = (
        dedict | fb_runnable | (lambda msg: {"messages": [msg], "attempt_number": 1})
    )

    def count_messages(state: State) -> dict:
        return {"initial_num_messages": len(state.get("messages", []))}

    builder.add_node("count_messages", count_messages)
    builder.add_node("llm", model)
    builder.add_node("fallback", fallback)

    # To support patch-based retries, we need to be able to
    # aggregate the messages over multiple turns.
    # The next sequence selects only the relevant messages
    # and then applies the validator
    select_messages = retry_strategy.get("aggregate_messages") or _default_aggregator

    def select_generated_messages(state: State) -> list:
        """Select only the messages generated within this loop."""
        selected = state["messages"][state["initial_num_messages"] :]
        return [select_messages(selected)]

    def endict_validator_output(x: Sequence[AnyMessage]) -> dict:
        if tool_choice and not x:
            return {
                "messages": [
                    HumanMessage(
                        content=f"ValidationError: please respond with a valid tool call [tool_choice={tool_choice}].",
                        additional_kwargs={"is_error": True},
                    )
                ]
            }
        return {"messages": x}

    validator_runnable = select_generated_messages | validator | endict_validator_output
    builder.add_node("validator", validator_runnable)

    class Finalizer:
        """Pick the final message to return from the retry loop."""

        def __init__(self, aggregator: Optional[Callable[[list], AIMessage]] = None):
            self._aggregator = aggregator or _default_aggregator

        def __call__(self, state: State) -> dict:
            """Return just the AI message."""
            initial_num_messages = state["initial_num_messages"]
            generated_messages = state["messages"][initial_num_messages:]
            return {
                "messages": {
                    "finalize": self._aggregator(generated_messages),
                }
            }

    # We only want to emit the final message
    builder.add_node("finalizer", Finalizer(retry_strategy.get("aggregate_messages")))

    # Define the connectivity
    builder.add_edge(START, "count_messages")
    builder.add_edge("count_messages", "llm")

    def route_validator(state: State):
        if state["messages"][-1].tool_calls or tool_choice is not None:
            return "validator"
        return END

    builder.add_conditional_edges("llm", route_validator, ["validator", END])
    builder.add_edge("fallback", "validator")
    max_attempts = retry_strategy.get("max_attempts", 3)

    def route_validation(state: State):
        if state["attempt_number"] > max_attempts:
            raise ValueError(
                f"Could not extract a valid value in {max_attempts} attempts."
            )
        for m in state["messages"][::-1]:
            if m.type == "ai":
                break
            if m.additional_kwargs.get("is_error"):
                return "fallback"
        return "finalizer"

    builder.add_conditional_edges(
        "validator", route_validation, ["finalizer", "fallback"]
    )

    builder.add_edge("finalizer", END)

    # These functions let the step be used in a MessageGraph
    # or a StateGraph with 'messages' as the key.
    def encode(x: Union[Sequence[AnyMessage], PromptValue]) -> dict:
        """Ensure the input is the correct format."""
        if isinstance(x, PromptValue):
            return {"messages": x.to_messages(), "input_format": "list"}
        if isinstance(x, list):
            return {"messages": x, "input_format": "list"}
        raise ValueError(f"Unexpected input type: {type(x)}")

    def decode(x: State) -> AIMessage:
        """Ensure the output is in the expected format."""
        return x["messages"][-1]

    return (
        encode | builder.compile().with_config(run_name="ValidationGraph") | decode
    ).with_config(run_name="ValidateWithRetries")


def bind_validator_with_retries(
    llm: BaseChatModel,
    *,
    tools: list,
    tool_choice: Optional[str] = None,
    max_attempts: int = 3,
) -> Runnable[Union[List[AnyMessage], PromptValue], AIMessage]:
    """Binds validators + retry logic ensure validity of generated tool calls.

    LLMs that support tool calling are good at generating structured JSON. However, they may
    not always perfectly follow your requested schema, especially if the schema is nested or
    has complex validation rules. This method allows you to bind a validation function to
    the LLM's output, so that any time the LLM generates a message, the validation function
    is run on it. If the validation fails, the method will retry the LLM with a fallback
    strategy, the simples being just to add a message to the output with the validation
    errors and a request to fix them.

    The resulting runnable expects a list of messages as input and returns a single AI message.
    By default, the LLM can optionally NOT invoke tools, making this easier to incorporate into
    your existing chat bot. You can specify a tool_choice to force the validator to be run on
    the outputs.

    Args:
        llm (Runnable): The llm that will generate the initial messages (and optionally fallba)
        validator (ValidationNode): The validation logic.
        retry_strategy (RetryStrategy): The retry strategy to use.
            Possible keys:
            - max_attempts: The maximum number of attempts to make.
            - fallback: The LLM or function to use in case of validation failure.
            - aggregate_messages: A function to aggregate the messages over multiple turns.
                Defaults to fetching the last AI message.
        tool_choice: If provided, always run the validator on the tool output.

    Returns:
        Runnable: A runnable that can be invoked with a list of messages and returns a single AI message.
    """
    bound_llm = llm.bind_tools(tools, tool_choice=tool_choice)
    retry_strategy = RetryStrategy(max_attempts=max_attempts)
    validator = ValidationNode(tools)
    return _bind_validator_with_retries(
        bound_llm,
        validator=validator,
        tool_choice=tool_choice,
        retry_strategy=retry_strategy,
    ).with_config(metadata={"retry_strategy": "default"})

class Respond(BaseModel):
    """Use to generate the response. Always use when responding to the user"""

    reason: str = Field(description="Step-by-step justification for the answer.")
    answer: str

tools = [Respond]

bound_llm = bind_validator_with_retries(llm, tools=tools)
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
# Role and Objective
You are a professional meeting transcript analyst. Your job is to deeply analyze the provided meeting transcript (é€å­—ç¨¿), extract actionable insights, and generate a comprehensive, structured summary using the TranscriptSummary function. You must not stop until all relevant information and insights have been fully extracted and organized.

# Instructions
- Only use the provided transcript as your primary context. If you need to supplement with external knowledge, clearly indicate the source or explain your reasoning.
- If you are unsure about any information, state so explicitly rather than guessing.
- Always respond using the TranscriptSummary function, filling every field with as much valuable, relevant, and actionable content as possible.
- If a field cannot be filled from the transcript, explain why or leave it empty.
- Do not terminate your response until you are confident that all important points, insights, and recommendations have been covered.
- Use clear, concise, and professional language in Traditional Chinese.
- If the transcript is ambiguous or incomplete, list the knowledge gaps and suggest what further information would be needed.

# Reasoning Steps
1. **Comprehension**: Carefully read and understand the entire transcript.
2. **Topic Extraction**: Identify all main topics and subtopics discussed.
3. **Key Moments**: For each topic, extract key moments, decisions, and turning points, and classify them as happy, tense, or sad moments as appropriate.
4. **Background & Context**: Supplement with relevant industry background, definitions of technical terms, and connections to current regulations or market trends, if applicable.
5. **Insightful Quotes**: Select the most insightful or representative quotes, and provide analysis of their significance.
6. **Summary & Next Steps**: Synthesize the overall summary and propose concrete next steps or action items, with justifications.
7. **Knowledge Gaps**: Explicitly list any missing information or areas requiring further investigation.
8. **Chain-of-Thought**: For each step, think step by step, and do not skip any reasoning.

# Output Format
- Always use the TranscriptSummary function to structure your output.
- Fill in all fields: metadata, key_moments, insightful_quotes, overall_summary, next_steps, other_stuff.
- Use bullet points and markdown formatting for clarity.
- For each field, provide as much detail as possible, but avoid unnecessary repetition.
- If you supplement with external knowledge, clearly mark it as such and explain your reasoning.

# Example
## Input Transcript
<transcript>
ä¸»é¡Œï¼šæ–°ç”¢å“ä¸Šå¸‚æœƒè­°
ä¸»æŒäººï¼šå¤§å®¶å¥½ï¼Œä»Šå¤©æˆ‘å€‘è¨è«–æ–°ç”¢å“ä¸Šå¸‚è¨ˆç•«...
ï¼ˆé€å­—ç¨¿å…§å®¹ç•¥ï¼‰
</transcript>

## Output (TranscriptSummary function)
metadata:
  title: æ–°ç”¢å“ä¸Šå¸‚æœƒè­°
  location: æœƒè­°å®¤A
  duration: 1å°æ™‚
key_moments:
  - topic: å¸‚å ´ç­–ç•¥
    happy_moments: [...]
    tense_moments: [...]
    sad_moments: [...]
    background_info: [...]
    moments_summary: ...
insightful_quotes:
  - quote: "æˆ‘å€‘å¿…é ˆå‰µæ–°ï¼Œå¦å‰‡å°±æœƒè¢«å¸‚å ´æ·˜æ±°ã€‚"
    speaker: å¼µç¶“ç†
    analysis: é€™å¥è©±å¼·èª¿äº†å‰µæ–°å°å…¬å¸æœªä¾†ç™¼å±•çš„é‡è¦æ€§ã€‚
overall_summary: ...
next_steps:
  - é€²è¡Œå¸‚å ´èª¿æŸ¥
  - å®Œæˆç”¢å“æ¸¬è©¦
other_stuff:
  - content: æœƒè­°ä¸­æåŠçš„æ³•è¦è®Šå‹•éœ€æŒçºŒè¿½è¹¤

# Context
<transcript>
{full_transcription}
</transcript>

# Final Instructions
Think step by step, and do not stop until you have fully analyzed and summarized all important content from the transcript. If you need to supplement with external knowledge, clearly indicate so and explain your reasoning. Always respond using the TranscriptSummary function.
"""
        ),
        ("placeholder", "{messages}"),
    ]
)

chain = prompt | bound_llm

class OutputFormat(BaseModel):
    sources: str = Field(
        ...,
        description="The raw transcript / span you could cite to justify the choice.",
    )
    content: str = Field(..., description="The chosen value.")

class Moment(BaseModel):
    quote: str = Field(..., description="The relevant quote from the transcript.")
    description: str = Field(..., description="A description of the moment.")
    expressed_preference: OutputFormat = Field(
        ..., description="The preference expressed in the moment, based on the context."
    )

class BackgroundInfo(BaseModel):
    factoid: OutputFormat = Field(
        ..., description="Important factoid about the member."
    )
    professions: Optional[List[str]] = Field(
        None, description="List of professions related to the member."
    )
    why: str = Field(..., description="Why this is important.")

class KeyMoments(BaseModel):
    topic: str = Field(..., description="The topic of the key moments.")
    happy_moments: List[Moment] = Field(
        ..., description="A list of key moments related to the topic."
    )
    tense_moments: List[Moment] = Field(
        ..., description="Moments where things were a bit tense."
    )
    sad_moments: List[Moment] = Field(
        ..., description="Moments where things where everyone was downtrodden."
    )
    background_info: List[BackgroundInfo] = Field(
        ..., description="A list of background information."
    )
    moments_summary: str = Field(..., description="A summary of the key moments.")

class InsightfulQuote(BaseModel):
    quote: OutputFormat = Field(
        ..., description="An insightful quote from the transcript."
    )
    speaker: str = Field(..., description="The name of the speaker who said the quote.")
    analysis: str = Field(
        ..., description="An analysis of the quote and its significance."
    )

class TranscriptMetadata(BaseModel):
    title: str = Field(..., description="The title of the transcript.")
    location: OutputFormat = Field(
        ..., description="The location where the interview took place. If the location cannot be identified, return 'Unknown'."
    )
    duration: str = Field(..., description="The duration of the interview.")

class TranscriptSummary(BaseModel):
    metadata: TranscriptMetadata = Field(
        ..., description="Metadata about the transcript."
    )
    key_moments: List[KeyMoments] = Field(
        ..., description="A list of key moments from the interview."
    )
    insightful_quotes: List[InsightfulQuote] = Field(
        ..., description="A list of insightful quotes from the interview."
    )
    overall_summary: str = Field(
        ..., description="An overall summary of the interview."
    )
    next_steps: List[str] = Field(
        ..., description="A list of next steps or action items based on the interview."
    )
    other_stuff: List[OutputFormat] = Field(
        ..., description="Additional relevant information."
    )

formatted_transcription = ""  # å…ˆåˆå§‹åŒ–

# è™•ç†ä¸Šå‚³çš„æ–‡ä»¶
if f is not None:
    st.audio(f)
    file_extension = f.name.split('.')[-1]  # ç²å–æ–‡ä»¶çš„å‰¯æª”å

    # åˆå§‹åŒ– summarize_transcription è®Šæ•¸
    summarize_transcription = "æ‘˜è¦å°šæœªç”Ÿæˆã€‚"

    # ä½¿ç”¨ st.status ä¾†é¡¯ç¤ºæ•´é«”è™•ç†ç‹€æ…‹
    with st.status("Processing audio file...", expanded=True) as status:
        try:
            # å°‡ä¸Šå‚³çš„æ–‡ä»¶ä¿å­˜åˆ°è‡¨æ™‚æ–‡ä»¶
            status.update(label="Saving uploaded file...")
            with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_extension}") as temp_input_file:
                temp_input_file.write(f.read())
                temp_input_file_path = temp_input_file.name

            # ä½¿ç”¨ pydub è®€å–å’Œè½‰æ›éŸ³é »æ–‡ä»¶
            status.update(label="Loading audio file...")
            audio = AudioSegment.from_file(temp_input_file_path, format=file_extension)

            # åˆ†å‰²éŸ³é »æ–‡ä»¶
            status.update(label="Splitting audio into chunks...")
            chunk_length_ms = 1 * 60 * 1000  # 1åˆ†é˜
            chunks = [audio[i:i + chunk_length_ms] for i in range(0, len(audio), chunk_length_ms)]

            if not chunks:
                st.error("No audio chunks were created. Please check the audio file format and content.")

            full_transcription = ""
            progress_bar = st.progress(0)
            total_chunks = len(chunks)

            status.update(label="Transcribing audio chunks...")
            # ä½¿ç”¨ ThreadPoolExecutor ä¾†ä¸¦è¡Œè™•ç†
            with ThreadPoolExecutor() as executor:
                futures = [executor.submit(transcribe_chunk, chunk, i) for i, chunk in enumerate(chunks)]
                for i, f in enumerate(futures):
                    full_transcription += f.result() + " "
                    progress = (i + 1) / total_chunks
                    progress_bar.progress(progress)

            progress_bar.empty()

            # ä½¿ç”¨ LangChain æ”¹å–„æ–‡æœ¬æ ¼å¼
            status.update(label="Formatting transcription...")
            formatted_transcription = stream_full_formatted_transcription(formatting_chain, full_transcription, judge_llm)

            status.update(label="Transcription complete!", state="complete", expanded=False)
        except Exception as e:
            st.error(f"Error processing audio file: {e}")
            formatted_transcription = "âš ï¸ è½‰éŒ„æˆ–æ ¼å¼åŒ–æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"

    # é¡¯ç¤º formatted_transcription
    tab1, tab2, tab3, tab4 = st.tabs(["è½‰éŒ„çµæœ", "é‡é»æ‘˜è¦", "å…§å®¹è§£æ", "åŸå§‹å…§å®¹"])
    with tab1:
        with st.container():
            st.markdown(beautify_transcript(formatted_transcription), unsafe_allow_html=True)
            st.balloons()

    # ç•°æ­¥è¨ˆç®— summarize_transcription ä¸¦åœ¨ Tab2 ä¸­é¡¯ç¤º spinner
    async def calculate_summary():
        # åˆ†å‰²è½‰éŒ„æ–‡æœ¬ä¸¦åŒ…è£æˆ Document å°è±¡
        split_docs = [Document(page_content=content) for content in text_splitter.split_text(full_transcription)]
        summarize_transcription = await run_app(split_docs)
        st.session_state['summarize_transcription'] = summarize_transcription
        return summarize_transcription

    with tab2:
        with st.spinner('Generating summary...'):
            summarize_transcription = asyncio.run(calculate_summary())
            st.markdown(summarize_transcription)

    # ä¿å­˜è½‰éŒ„çµæœåˆ° session_state
        if 'formatted_transcription' not in st.session_state:
            st.session_state['formatted_transcription'] = formatted_transcription
        if 'full_transcription' not in st.session_state:
            st.session_state['full_transcription'] = full_transcription

    with tab3:
        with st.container():
            try:
                formatted_transcription = st.session_state.get('formatted_transcription', "")
                transcript = [
                    (
                        "Speaker",
                        full_transcription,
                    ),
                ]

                formatted = "\n".join(f"{x[0]}: {x[1]}" for x in transcript)

                tools = [TranscriptSummary]
                bound_llm = bind_validator_with_retries(
                    llm,
                    tools=tools,
                )
                prompt = ChatPromptTemplate.from_messages(
                    [
                        ("system", "Respond directly using the TranscriptSummary function."),
                        ("placeholder", "{messages}"),
                    ]
                )

                bound_chain = prompt | bound_llm              
                try:
                    results = bound_chain.invoke(
                        {
                            "messages": [
                                (
                                    "user",
                                    f"Extract the summary from the following conversation:\n\n<convo>\n{formatted}\n</convo>"
                                    "\n\nRemember to respond using the TranscriptSummary function.",
                                )
                            ]
                        },
                    )
                except ValueError as e:
                    print(repr(e))
                data = results.additional_kwargs

                # æå– tool_calls
                tool_calls = data['tool_calls']

                # éæ­·æ¯å€‹ tool_call
                for tool_call in tool_calls:
                    # æå– Arguments
                    arguments = tool_call['function']['arguments']

                    # è§£æ Arguments ä¸­çš„ JSON å­—ç¬¦ä¸²
                    arguments_data = json.loads(arguments)

                    # æå– Metadata
                    metadata = arguments_data['metadata']
                    st.markdown("### Metadata")
                    st.markdown(f"- **æ¨™é¡Œ**: {metadata['title']}")
                    st.markdown(f"- **åœ°é»**: {metadata['location']['content']}")
                    st.markdown(f"- **æŒçºŒæ™‚é–“**: {metadata['duration']}")

                    # æå– Key Moments
                    key_moments = arguments_data['key_moments']
                    st.markdown("\n### Key Moments")
                    for moment in key_moments:
                        st.markdown(f"- **ä¸»é¡Œ**: {moment['topic']}")
                        st.markdown(f"  - **æ™‚åˆ»ç¸½çµ**: {moment['moments_summary']}")
                        for info in moment['background_info']:
                            st.markdown(f"    - **äº‹å¯¦**: {info['factoid']['content']}")
                            st.markdown(f"      - **ç‚ºä»€éº¼é‡è¦**: {info['why']}")

                    # æå– Insightful Quotes
                    insightful_quotes = arguments_data['insightful_quotes']
                    st.markdown("\n### Insightful Quotes")
                    for quote in insightful_quotes:
                        st.markdown(f"- **å¼•ç”¨**: {quote['quote']['content']}")
                        st.markdown(f"  - **è¬›è€…**: {quote['speaker']}")
                        st.markdown(f"  - **åˆ†æ**: {quote['analysis']}")

                    # æå– Overall Summary
                    overall_summary = arguments_data['overall_summary']
                    st.markdown("### Overall Summary:")
                    st.markdown(f"{overall_summary}")

                    # æå– Next Steps
                    next_steps = arguments_data['next_steps']
                    st.markdown("\n### Next Steps")
                    
                    for step in next_steps:
                        st.markdown(f"- {step}")

                    # æå– Other Stuff
                    other_stuff = arguments_data['other_stuff']
                    st.markdown("\n### Other Stuff")
                    for item in other_stuff:
                        st.markdown(f"- **å…§å®¹**: {item['content']}")

                    # æå–ç‰¹å®šå…§å®¹
                    title = arguments_data['metadata']['title']
                    location = arguments_data['metadata']['location']['content']
                    duration = arguments_data['metadata']['duration']
                    key_moments = arguments_data['key_moments']
                    insightful_quotes = arguments_data['insightful_quotes']
                    overall_summary = arguments_data['overall_summary']
                    next_steps = arguments_data['next_steps']

                    #st.text(f"æ¨™é¡Œ: {title}")
                    #st.text(f"åœ°é»: {location}")
                    #st.text(f"æŒçºŒæ™‚é–“: {duration}")
                    #st.text(f"é—œéµæ™‚åˆ»: {key_moments}")
                    #st.text(f"æ·±åˆ»å¼•ç”¨: {insightful_quotes}")
                    #st.text(f"æ•´é«”æ‘˜è¦: {overall_summary}")
                    #st.text(f"ä¸‹ä¸€æ­¥: {next_steps}")
            
            except Exception as e:
                st.markdown(f"ç™¼ç”ŸéŒ¯èª¤: {repr(e)}")

    with tab4:
        with st.container():
            st.markdown(full_transcription)

else:
    st.stop()
