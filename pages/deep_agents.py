# pages/deep_agents.py
# -*- coding: utf-8 -*-
from __future__ import annotations

import os
import re
import io
import uuid
import math
import time
import json
import base64
import hashlib
import threading
import ast
from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple, List
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse

import streamlit as st
import numpy as np
import pandas as pd
import faiss
from pypdf import PdfReader

from openai import OpenAI
from langgraph.errors import GraphRecursionError

try:
    import fitz  # pymupdf
    HAS_PYMUPDF = True
except Exception:
    HAS_PYMUPDF = False


# =========================
# Streamlit config
# =========================
st.set_page_config(page_title="ç ”ç©¶å ±å‘ŠåŠ©æ‰‹ï¼ˆDeepAgent + Badgesï¼‰", layout="wide")
st.title("ç ”ç©¶å ±å‘ŠåŠ©æ‰‹ï¼ˆDeepAgent + Badgesï¼‰")
# âœ… ä¾ä½ è¦æ±‚ï¼šä¸æ³¨å…¥ä»»ä½• CSSï¼ˆå›åˆ° Streamlit é è¨­æ’ç‰ˆï¼‰


# =========================
# DeepAgents / LangChain importsï¼ˆå¯è¨ºæ–·ç‰ˆï¼‰
# =========================
HAS_DEEPAGENTS = False
DEEPAGENTS_IMPORT_ERRORS: list[str] = []

create_deep_agent = None
init_chat_model = None
ChatOpenAI = None

try:
    from deepagents import create_deep_agent as _create_deep_agent
    create_deep_agent = _create_deep_agent
except Exception as e:
    DEEPAGENTS_IMPORT_ERRORS.append(f"deepagents import failed: {repr(e)}")

try:
    from langchain.chat_models import init_chat_model as _init_chat_model
    init_chat_model = _init_chat_model
except Exception as e:
    DEEPAGENTS_IMPORT_ERRORS.append(f"langchain.chat_models.init_chat_model import failed: {repr(e)}")

try:
    from langchain_openai import ChatOpenAI as _ChatOpenAI
    ChatOpenAI = _ChatOpenAI
except Exception as e:
    DEEPAGENTS_IMPORT_ERRORS.append(f"langchain_openai.ChatOpenAI import failed: {repr(e)}")

HAS_DEEPAGENTS = (create_deep_agent is not None) and ((init_chat_model is not None) or (ChatOpenAI is not None))


def _require_deepagents() -> None:
    if HAS_DEEPAGENTS:
        return
    st.error("DeepAgent ä¾è³´è¼‰å…¥å¤±æ•—ï¼ˆä¸ä¸€å®šæ˜¯æ²’å®‰è£ï¼Œå¯èƒ½æ˜¯ç‰ˆæœ¬/ä¾è³´ä¸ç›¸å®¹ï¼‰ã€‚")
    if DEEPAGENTS_IMPORT_ERRORS:
        st.markdown("### ä¾è³´éŒ¯èª¤ç´°ç¯€ï¼ˆè«‹æŠŠé€™æ®µè²¼çµ¦æˆ‘ï¼Œæˆ‘å°±èƒ½ç²¾æº–æŒ‡ä½ è©²è£å“ªå€‹ç‰ˆæœ¬ï¼‰")
        for msg in DEEPAGENTS_IMPORT_ERRORS:
            st.code(msg)
    else:
        st.info("ï¼ˆæ²’æœ‰æ•æ‰åˆ°éŒ¯èª¤ç´°ç¯€ï¼‰")
    st.stop()


def _make_langchain_llm(model_name: str, temperature: float = 0.0, reasoning_effort: Optional[str] = None):
    """
    å›å‚³ LangChain çš„ chat model instanceï¼š
    - å„ªå…ˆ init_chat_model
    - fallback ChatOpenAI
    """
    if init_chat_model is not None:
        if model_name.startswith("openai:"):
            return init_chat_model(model=model_name, temperature=temperature)
        return init_chat_model(model=f"openai:{model_name}", temperature=temperature)

    if ChatOpenAI is not None:
        if model_name.startswith("openai:"):
            model_name = model_name.split("openai:", 1)[1]
        kwargs = dict(
            model=model_name,
            temperature=temperature,
            use_responses_api=True,
            max_completion_tokens=None,
        )
        if reasoning_effort in ("low", "medium", "high"):
            kwargs["reasoning"] = {"effort": reasoning_effort}
        return ChatOpenAI(**kwargs)

    raise RuntimeError("No LangChain LLM factory available.")


# =========================
# æ¨¡å‹è¨­å®šï¼ˆä¾ä½ è¦æ±‚ï¼šgpt-5.2ï¼‰
# =========================
EMBEDDING_MODEL = "text-embedding-3-small"

MODEL_MAIN = "gpt-5.2"
MODEL_GRADER = "gpt-5.2"
MODEL_WEB = "gpt-5.2"

REASONING_EFFORT = "medium"


# =========================
# æ•ˆèƒ½åƒæ•¸ï¼ˆå›ºå®šé è¨­ï¼›ä¸æä¾› UI èª¿æ•´ï¼‰
# =========================
EMBED_BATCH_SIZE = 256
OCR_MAX_WORKERS = 2

CORPUS_DEFAULT_MAX_CHUNKS = 24
CORPUS_PER_REPORT_QUOTA = 6

DA_MAX_DOC_SEARCH_CALLS = 14
DA_MAX_WEB_SEARCH_CALLS = 4
DA_MAX_REWRITE_ROUNDS = 2
DA_MAX_CLAIMS = 10

# å›ºå®šé è¨­ï¼ˆä¸æä¾› UIï¼‰
DEFAULT_RECURSION_LIMIT = 200
DEFAULT_CITATION_STALL_STEPS = 12
DEFAULT_CITATION_STALL_MIN_CHARS = 450

DEFAULT_SOURCES_BADGE_MAX_TITLES_INLINE = 4
DEFAULT_SOURCES_BADGE_MAX_PAGES_PER_TITLE = 10


# =========================
# Regex / citations
# =========================
CHUNK_ID_LEAK_PAT = re.compile(r"(chunk_id\s*=\s*|_p(?:na|\d+)_c\d+)", re.IGNORECASE)

# é‡è¦ï¼šå…§éƒ¨ evidence ä¸è©²å‡ºç¾åœ¨æœ‰æ•ˆå¼•ç”¨
EVIDENCE_PATH_IN_CIT_RE = re.compile(r"\[(?:/)?evidence/[^ \]]+?\s+p(\d+|-)\s*\]", re.IGNORECASE)

CIT_RE = re.compile(r"\[[^\]]+?\s+p(\d+|-)\s*\]")
BULLET_RE = re.compile(r"^\s*(?:[-â€¢*]|\d+\.)\s+")
CIT_PARSE_RE = re.compile(r"\[([^\]]+?)\s+p(\d+|-)\s*\]")

# ç§»é™¤å…§éƒ¨æµç¨‹/æª”åæ´©æ¼ï¼ˆåªç•™ã€ŒæŸ¥å¾—åˆ°çš„ã€å…§å®¹ï¼‰
INTERNAL_LEAK_PAT = re.compile(
    r"(Budget exceeded|/evidence|doc_[\w\-]+\.md|web_[\w\-]+\.md|é¡åº¦ä¸è¶³|å ä½|å‘é‡åº«|å…§éƒ¨æ–‡ä»¶|å·¥ä½œæµ|æµç¨‹|å·¥å…·é ç®—)",
    re.IGNORECASE,
)


# =========================
# å°å·¥å…·
# =========================
def norm_space(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())


def estimate_tokens_from_chars(n_chars: int) -> int:
    if n_chars <= 0:
        return 0
    return max(1, int(math.ceil(n_chars / 3.6)))


def chunk_text(text: str, chunk_size: int = 900, overlap: int = 150) -> list[str]:
    text = norm_space(text)
    if not text:
        return []
    out = []
    i = 0
    while i < len(text):
        j = min(len(text), i + chunk_size)
        out.append(text[i:j])
        if j == len(text):
            break
        i = max(0, j - overlap)
    return out


def sha1_bytes(data: bytes) -> str:
    return hashlib.sha1(data).hexdigest()


def truncate_filename(name: str, max_len: int = 44) -> str:
    if len(name) <= max_len:
        return name
    base, ext = os.path.splitext(name)
    keep = max(10, max_len - len(ext) - 1)
    return f"{base[:keep]}â€¦{ext}"


def _dedup_keep_order(items: list[str]) -> list[str]:
    seen = set()
    out = []
    for x in items:
        if x in seen:
            continue
        seen.add(x)
        out.append(x)
    return out


def _hash_norm_text(s: str) -> str:
    return sha1_bytes(norm_space(s).encode("utf-8"))


def has_visible_citations(text: str) -> bool:
    raw = (text or "").strip()
    if not raw:
        return False
    cits = [m.group(0) for m in re.finditer(r"\[[^\]]+?\s+p(\d+|-)\s*\]", raw)]
    cits = [c for c in cits if not EVIDENCE_PATH_IN_CIT_RE.search(c)]
    return bool(cits)


def _try_parse_json_or_py_literal(text: str) -> Optional[Any]:
    t = (text or "").strip()
    if not t:
        return None
    if t.startswith("{") or t.startswith("["):
        try:
            return json.loads(t)
        except Exception:
            pass
    if t.startswith("{") and t.endswith("}"):
        try:
            return ast.literal_eval(t)
        except Exception:
            return None
    return None

def get_recent_chat_messages(max_messages: int = 10) -> list[dict]:
    """
    å–æœ€è¿‘ N å‰‡ã€Œtextã€è¨Šæ¯ç•¶çŸ­æœŸè¨˜æ†¶ï¼ˆæ’é™¤ default å¤§åŒ…è¼¸å‡ºï¼‰ï¼Œé¿å… prompt çˆ†é•·ã€‚
    å›å‚³æ ¼å¼ï¼š[{role:"user"/"assistant", content:"..."}]
    """
    msgs: list[dict] = []
    for m in st.session_state.get("chat_history", []):
        if m.get("kind") != "text":
            continue
        role = m.get("role")
        if role not in ("user", "assistant"):
            continue
        content = (m.get("content") or "").strip()
        if not content:
            continue
        # é¿å…å¤ªé•·ï¼ˆå¯è¦–éœ€è¦èª¿ï¼‰
        if len(content) > 2000:
            content = content[:2000] + "â€¦"
        msgs.append({"role": role, "content": content})

    return msgs[-max_messages:]


def _domain(u: str) -> str:
    try:
        host = urlparse(u).netloc.lower()
        if host.startswith("www."):
            host = host[4:]
        return host or "web"
    except Exception:
        return "web"


def web_sources_from_openai_sources(sources: Optional[list[dict]]) -> Dict[str, List[Tuple[str, str]]]:
    """
    å°‡ OpenAI web_search sources è½‰æˆï¼š
    {domain: [(title, url), ...]}
    """
    out: Dict[str, List[Tuple[str, str]]] = {}
    if not sources:
        return out
    for s in sources:
        if not isinstance(s, dict):
            continue
        title = (s.get("title") or s.get("source") or "source").strip()
        url = (s.get("url") or "").strip()
        if not url:
            continue
        dom = _domain(url)
        out.setdefault(dom, []).append((title, url))

    # å»é‡
    for dom in list(out.keys()):
        seen = set()
        uniq: List[Tuple[str, str]] = []
        for t, u in out[dom]:
            key = (t, u)
            if key in seen:
                continue
            seen.add(key)
            uniq.append((t, u))
        out[dom] = uniq
    return out


def render_web_sources_list(web_sources: Dict[str, List[Tuple[str, str]]], max_domains: int = 6, max_per_domain: int = 6) -> None:
    """
    B æ–¹æ¡ˆï¼šbadge åªé¡¯ç¤º domainï¼›URL ç”¨æ¸…å–®åˆ—åœ¨ä¸‹é¢ã€‚
    """
    if not web_sources:
        return

    st.markdown("#### Web Sources")
    domains = sorted(web_sources.keys())
    show = domains[:max_domains]
    more = domains[max_domains:]

    def _render(domains_list: list[str]):
        for dom in domains_list:
            items = web_sources.get(dom, [])
            if not items:
                continue
            st.markdown(f"- **{dom}**")
            for title, url in items[:max_per_domain]:
                st.markdown(f"  - {title} â€” {url}")

    _render(show)
    if more:
        with st.expander(f"æ›´å¤š Web Sourcesï¼ˆ{len(more)}ï¼‰", expanded=False):
            _render(more)

def ensure_web_citation_token(text: str, domain: str) -> str:
    """
    ä¿è­‰å›ç­”ä¸­è‡³å°‘æœ‰ä¸€å€‹ [WebSearch:<domain> p-]ï¼Œ
    è®“ä½ çš„ UI èƒ½é¡¯ç¤º web badgeã€‚
    """
    if not text:
        return text
    if re.search(r"\[WebSearch:[^\]]+\s+p-\s*\]", text, re.IGNORECASE):
        return text
    dom = (domain or "web").strip() or "web"
    return (text.rstrip() + f"\n\n[WebSearch:{dom} p-]").strip()

def _domain_from_url(u: str) -> str:
    try:
        host = urlparse(u).netloc or ""
        host = host.lower()
        if host.startswith("www."):
            host = host[4:]
        return host or "web"
    except Exception:
        return "web"


def strip_internal_process_lines(md: str) -> str:
    """
    ä½ æŒ‡å®šã€Œåªå¯«æœ‰æŸ¥åˆ°çš„ã€ï¼Œæ‰€ä»¥ï¼š
    - ä»»ä½•ä¸»è¦åœ¨è¬›é¡åº¦ä¸è¶³/å…§éƒ¨æª”å/æµç¨‹çš„è¡Œï¼Œç›´æ¥ç§»é™¤
    """
    lines = (md or "").splitlines()
    kept = []
    for line in lines:
        if INTERNAL_LEAK_PAT.search(line):
            continue
        kept.append(line)
    return "\n".join(kept).strip()


# =========================
# OpenAI client + wrappers
# =========================
def get_openai_api_key() -> str:
    if "OPENAI_KEY" in st.secrets and st.secrets["OPENAI_KEY"]:
        return st.secrets["OPENAI_KEY"]
    if os.environ.get("OPENAI_API_KEY"):
        return os.environ["OPENAI_API_KEY"]
    if os.environ.get("OPENAI_KEY"):
        return os.environ["OPENAI_KEY"]
    raise RuntimeError("Missing OpenAI API key. Set st.secrets['OPENAI_KEY'] or env OPENAI_API_KEY.")


def get_client(api_key: str) -> OpenAI:
    return OpenAI(api_key=api_key)


def _to_messages(system: str, user: Any) -> list[Dict[str, Any]]:
    return [{"role": "system", "content": system}, {"role": "user", "content": user}]


def call_gpt(
    client: OpenAI,
    *,
    model: str,
    system: str,
    user: Any,
    reasoning_effort: Optional[str] = None,
    tools: Optional[list] = None,
    include_sources: bool = False,
) -> Tuple[str, Optional[list[Dict[str, Any]]]]:
    messages = _to_messages(system, user)
    resp = client.responses.create(
        model=model,
        input=messages,
        tools=tools,
        tool_choice="auto" if tools else "none",
        parallel_tool_calls=True if tools else None,
        reasoning={"effort": reasoning_effort} if reasoning_effort in ("low", "medium", "high") else None,
        include=["web_search_call.action.sources"] if (tools and include_sources) else None,
        truncation="auto",
    )
    out_text = resp.output_text
    sources = None
    if tools and include_sources:
        try:
            sources_list = []
            if hasattr(resp, "output") and resp.output:
                for item in resp.output:
                    d = item if isinstance(item, dict) else getattr(item, "__dict__", {})
                    if isinstance(d, dict) and d.get("type") == "web_search_call":
                        action = d.get("action", {}) or {}
                        if isinstance(action, dict) and action.get("sources"):
                            sources_list.extend(action["sources"])
            sources = sources_list if sources_list else None
        except Exception:
            sources = None
    return out_text, sources


def embed_texts(client: OpenAI, texts: list[str]) -> np.ndarray:
    resp = client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=texts,
        encoding_format="float",
    )
    vecs = np.array([d.embedding for d in resp.data], dtype=np.float32)
    norms = np.linalg.norm(vecs, axis=1, keepdims=True)
    norms = np.where(norms == 0, 1.0, norms)
    return vecs / norms


# =========================
# OCR / PDF
# =========================
def extract_pdf_text_pages_pypdf(pdf_bytes: bytes) -> list[Tuple[int, str]]:
    reader = PdfReader(io.BytesIO(pdf_bytes))
    out: list[Tuple[int, str]] = []
    for i, p in enumerate(reader.pages):
        try:
            t = p.extract_text() or ""
        except Exception:
            t = ""
        out.append((i + 1, norm_space(t)))
    return out


def extract_pdf_text_pages_pymupdf(pdf_bytes: bytes) -> list[Tuple[int, str]]:
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    out: list[Tuple[int, str]] = []
    for i in range(doc.page_count):
        page = doc.load_page(i)
        t = page.get_text("text") or ""
        out.append((i + 1, norm_space(t)))
    return out


def extract_pdf_text_pages(pdf_bytes: bytes) -> list[Tuple[int, str]]:
    if HAS_PYMUPDF:
        try:
            return extract_pdf_text_pages_pymupdf(pdf_bytes)
        except Exception:
            return extract_pdf_text_pages_pypdf(pdf_bytes)
    return extract_pdf_text_pages_pypdf(pdf_bytes)


def analyze_pdf_text_quality(
    pdf_pages: list[Tuple[int, str]],
    min_chars_per_page: int = 40,
) -> Tuple[int, int, float, int, float]:
    if not pdf_pages:
        return 0, 0, 1.0, 0, 0.0
    lens = [len(t) for _, t in pdf_pages]
    blank = sum(1 for L in lens if L <= min_chars_per_page)
    total_pages = max(1, len(lens))
    blank_ratio = blank / total_pages
    text_pages = total_pages - blank
    text_pages_ratio = text_pages / total_pages
    return sum(lens), blank, blank_ratio, text_pages, text_pages_ratio


def should_suggest_ocr(ext: str, pages: Optional[int], extracted_chars: int, blank_ratio: Optional[float]) -> bool:
    if ext != ".pdf":
        return False
    if pages is None or pages <= 0:
        return True
    if blank_ratio is not None and blank_ratio >= 0.6:
        return True
    avg = extracted_chars / max(1, pages)
    return avg < 120


def _img_bytes_to_data_url(img_bytes: bytes, mime: str = "image/png") -> str:
    b64 = base64.b64encode(img_bytes).decode("utf-8")
    return f"data:{mime};base64,{b64}"


def ocr_image_bytes(client: OpenAI, image_bytes: bytes, mime: str = "image/png") -> str:
    system = "ä½ æ˜¯ä¸€å€‹OCRå·¥å…·ã€‚åªè¼¸å‡ºå¯è¦‹æ–‡å­—èˆ‡è¡¨æ ¼å…§å®¹ï¼ˆè‹¥æœ‰è¡¨æ ¼ç”¨ Markdown è¡¨æ ¼ï¼‰ã€‚ä¸­æ–‡è«‹ç”¨ç¹é«”ä¸­æ–‡ã€‚ä¸è¦åŠ è©•è«–ã€‚"
    user_content = [
        {"type": "input_text", "text": "è«‹æ“·å–åœ–ç‰‡ä¸­æ‰€æœ‰å¯è¦‹æ–‡å­—ï¼ˆåŒ…å«å°å­—/è¨»è…³ï¼‰ã€‚è‹¥ç„¡æ³•è¾¨è­˜è«‹æ¨™è¨˜[ç„¡æ³•è¾¨è­˜]ã€‚"},
        {"type": "input_image", "image_url": _img_bytes_to_data_url(image_bytes, mime=mime)},
    ]
    text, _ = call_gpt(client, model=MODEL_GRADER, system=system, user=user_content, reasoning_effort=None)
    return text


def ocr_pdf_pages_parallel(client: OpenAI, pdf_bytes: bytes, dpi: int = 180) -> list[Tuple[int, str]]:
    if not HAS_PYMUPDF:
        raise RuntimeError("æœªå®‰è£ pymupdfï¼ˆfitzï¼‰ï¼Œç„¡æ³•åš PDF OCRã€‚è«‹ pip install pymupdf")

    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    zoom = dpi / 72.0
    mat = fitz.Matrix(zoom, zoom)

    def render_page(i: int) -> Tuple[int, bytes]:
        page = doc.load_page(i)
        pix = page.get_pixmap(matrix=mat, alpha=False)
        return i + 1, pix.tobytes("png")

    page_imgs = [render_page(i) for i in range(doc.page_count)]
    results: Dict[int, str] = {}

    with ThreadPoolExecutor(max_workers=OCR_MAX_WORKERS) as ex:
        futs = {ex.submit(ocr_image_bytes, client, img_bytes, "image/png"): page_no for page_no, img_bytes in page_imgs}
        for fut in as_completed(futs):
            page_no = futs[fut]
            try:
                results[page_no] = norm_space(fut.result())
            except Exception:
                results[page_no] = ""

    return [(p, results.get(p, "")) for p, _ in page_imgs]


# =========================
# FAISS store
# =========================
@dataclass
class Chunk:
    chunk_id: str
    report_id: str
    title: str
    page: Optional[int]
    text: str


class FaissStore:
    def __init__(self, dim: int):
        self.index = faiss.IndexFlatIP(dim)
        self.chunks: list[Chunk] = []

    def add(self, vecs: np.ndarray, chunks: list[Chunk]) -> None:
        self.index.add(vecs)
        self.chunks.extend(chunks)

    def search(self, qvec: np.ndarray, k: int = 8) -> list[Tuple[float, Chunk]]:
        if self.index.ntotal == 0:
            return []
        scores, idx = self.index.search(qvec.astype(np.float32), k)
        out: list[Tuple[float, Chunk]] = []
        for s, i in zip(scores[0], idx[0]):
            if i < 0 or i >= len(self.chunks):
                continue
            out.append((float(s), self.chunks[i]))
        return out


# =========================
# File rows
# =========================
@dataclass
class FileRow:
    file_id: str
    file_sig: str
    name: str
    ext: str
    bytes_len: int
    pages: Optional[int]
    extracted_chars: int
    token_est: int
    text_pages: Optional[int]
    text_pages_ratio: Optional[float]
    blank_pages: Optional[int]
    blank_ratio: Optional[float]
    likely_scanned: bool
    use_ocr: bool


def build_indices_incremental_no_kg(
    client: OpenAI,
    file_rows: list[FileRow],
    file_bytes_map: Dict[str, bytes],
    store: Optional[FaissStore],
    processed_keys: set,
    chunk_size: int = 900,
    overlap: int = 150,
) -> Tuple[FaissStore, Dict[str, Any], set]:
    if store is None:
        dim = embed_texts(client, ["dim_probe"]).shape[1]
        store = FaissStore(dim)

    stats = {"new_reports": 0, "new_chunks": 0}
    new_chunks: list[Chunk] = []
    new_texts: list[str] = []

    to_process: list[FileRow] = []
    for r in file_rows:
        key = (r.file_sig, bool(r.use_ocr))
        if key not in processed_keys:
            to_process.append(r)

    for row in to_process:
        data = file_bytes_map[row.file_id]
        report_id = row.file_id
        title = os.path.splitext(row.name)[0]
        stats["new_reports"] += 1

        if row.ext == ".pdf":
            pages = ocr_pdf_pages_parallel(client, data) if row.use_ocr else extract_pdf_text_pages(data)
        elif row.ext == ".txt":
            pages = [(None, norm_space(data.decode("utf-8", errors="ignore")))]
        elif row.ext in (".png", ".jpg", ".jpeg"):
            mime = "image/jpeg" if row.ext in (".jpg", ".jpeg") else "image/png"
            txt = norm_space(ocr_image_bytes(client, data, mime=mime))
            pages = [(None, txt)]
        else:
            pages = [(None, "")]

        for page_no, page_text in pages:
            if not page_text:
                continue
            for i, ch in enumerate(chunk_text(page_text, chunk_size=chunk_size, overlap=overlap)):
                cid = f"{report_id}_p{page_no if page_no else 'na'}_c{i}"
                new_chunks.append(Chunk(cid, report_id, title, page_no if isinstance(page_no, int) else None, ch))
                new_texts.append(ch)

        processed_keys.add((row.file_sig, bool(row.use_ocr)))

    if new_texts:
        vecs_list: list[np.ndarray] = []
        for i in range(0, len(new_texts), EMBED_BATCH_SIZE):
            vecs_list.append(embed_texts(client, new_texts[i:i + EMBED_BATCH_SIZE]))
        vecs = np.vstack(vecs_list)
        store.add(vecs, new_chunks)

    stats["new_chunks"] = len(new_chunks)
    return store, stats, processed_keys


# =========================
# citations / rendering
# =========================
def file_to_text(file_obj: Any) -> str:
    if file_obj is None:
        return ""

    if isinstance(file_obj, dict):
        if "data" in file_obj:
            return file_to_text(file_obj.get("data"))
        if "content" in file_obj:
            return file_to_text(file_obj.get("content"))
        for k in ("text", "answer", "final", "output", "message"):
            if k in file_obj:
                return file_to_text(file_obj.get(k))
        try:
            return json.dumps(file_obj, ensure_ascii=False, indent=2)
        except Exception:
            return str(file_obj)

    if isinstance(file_obj, (bytes, bytearray)):
        return file_obj.decode("utf-8", errors="ignore")

    if isinstance(file_obj, str):
        return file_obj

    if isinstance(file_obj, (list, tuple)):
        parts: list[str] = []
        for x in file_obj:
            t = file_to_text(x).strip()
            if t:
                parts.append(t)
        return "\n".join(parts)

    return str(file_obj)


def get_files_text(files: Optional[dict], key: str) -> str:
    if not isinstance(files, dict) or key not in files:
        return ""
    return file_to_text(files.get(key)).strip()


def _badge_directive(label: str, color: str) -> str:
    safe = label.replace("[", "(").replace("]", ")")
    return f":{color}-badge[{safe}]"


def _extract_main_text_from_payload(payload: Any) -> Optional[str]:
    if isinstance(payload, dict):
        for k in ("content", "answer", "final", "output", "text", "message"):
            if k not in payload:
                continue
            v = payload.get(k)
            if isinstance(v, str) and v.strip():
                return v
            if isinstance(v, (list, tuple)):
                joined = file_to_text(v).strip()
                if joined:
                    return joined
        msgs = payload.get("messages")
        if isinstance(msgs, list) and msgs:
            last = msgs[-1]
            if isinstance(last, dict):
                c = last.get("content")
                if isinstance(c, (str, list, tuple, dict)):
                    out = file_to_text(c).strip()
                    if out:
                        return out
            out = file_to_text(last).strip()
            return out or None
        return None

    if isinstance(payload, list):
        out = file_to_text(payload).strip()
        return out or None

    return None


def _strip_citations_from_text(text: str) -> str:
    """
    ç§»é™¤å¼•ç”¨ tokenï¼Œä½†ä¿ç•™æ›è¡Œï¼ˆé¿å… Markdown é»æˆä¸€å¨ï¼‰
    """
    if not text:
        return ""
    pat = re.compile(r"[ \t]*\[[^\]]*?\s+p(\d+|-)(?:-\d+)?[^\]]*?\][ \t]*")
    out_lines: list[str] = []
    for line in text.splitlines():
        out_lines.append(pat.sub("", line).rstrip())
    return "\n".join(out_lines).strip()


def _extract_citation_items(text: str) -> list[tuple[str, str]]:
    """
    æ”¯æ´ï¼š
    - [Title p12]
    - [Title p2-3; Another Title p8]
    - [A p2][B p8]
    """
    if not text:
        return []

    items: list[tuple[str, str]] = []
    for m in re.finditer(r"\[([^\]]+)\]", text):
        inner = (m.group(1) or "").strip()
        if not inner:
            continue

        parts = [p.strip() for p in re.split(r"[;ï¼›]", inner) if p.strip()]
        for p in parts:
            mm = re.search(r"^(.*)\s+p(\d+(?:-\d+)?|-)\s*$", p)
            if not mm:
                continue
            title = norm_space(mm.group(1))
            page = mm.group(2).strip()
            if EVIDENCE_PATH_IN_CIT_RE.search(f"[{title} p{page}]"):
                continue
            items.append((title, page))

    return items


def _title_to_display_domain(title: str) -> str:
    """
    B æ–¹æ¡ˆï¼šbadge åªé¡¯ç¤º domain
    - doc: ê·¸ëŒ€ë¡œç”¨ title
    - web: [WebSearch:domain p-] -> é¡¯ç¤º domain
    """
    t = (title or "").strip()
    low = t.lower()
    if low.startswith("websearch:"):
        dom = t.split(":", 1)[1].strip() if ":" in t else "web"
        return dom or "web"
    return t


def render_markdown_answer_with_sources_badges(answer_text: str) -> None:
    raw = (answer_text or "").strip()

    if raw and CHUNK_ID_LEAK_PAT.search(raw):
        raw = CHUNK_ID_LEAK_PAT.sub("", raw)

    payload = _try_parse_json_or_py_literal(raw)
    if payload is not None:
        extracted = _extract_main_text_from_payload(payload)
        if extracted is not None:
            raw = extracted.strip()

    # âœ… æœ€çµ‚è¼¸å‡ºå‰å†ä¿éšªï¼šå»å…§éƒ¨æµç¨‹/æª”å
    raw = strip_internal_process_lines(raw)

    cit_items = _extract_citation_items(raw)

    clean = _strip_citations_from_text(raw)
    st.markdown(clean if clean else "ï¼ˆç„¡å…§å®¹ï¼‰")

    if not cit_items:
        return

    grouped: dict[str, list[str]] = {}
    for title, page in cit_items:
        grouped.setdefault(title, []).append(page)

    def _key(p: str):
        if p.isdigit():
            return (0, int(p))
        if re.fullmatch(r"\d+-\d+", p):
            a, b = p.split("-", 1)
            return (1, int(a), int(b))
        if p == "-":
            return (9, 10**9)
        return (10, p)

    for t in list(grouped.keys()):
        pages = _dedup_keep_order([p.strip() for p in grouped[t] if p.strip()])
        grouped[t] = sorted(pages, key=_key)

    titles_sorted = sorted(grouped.keys(), key=lambda x: (x.strip().lower().startswith("websearch:"), x.lower()))
    max_inline = int(st.session_state.get("sources_badge_max_titles_inline", DEFAULT_SOURCES_BADGE_MAX_TITLES_INLINE))
    max_pages = int(st.session_state.get("sources_badge_max_pages_per_title", DEFAULT_SOURCES_BADGE_MAX_PAGES_PER_TITLE))

    inline_titles = titles_sorted[:max_inline]
    extra_titles = titles_sorted[max_inline:]

    def _pages_str(pages: list[str]) -> str:
        if not pages:
            return "p-"
        if len(pages) <= max_pages:
            return "p" + ",".join(pages)
        return "p" + ",".join(pages[:max_pages]) + "â€¦"

    def _render_badges(titles: list[str]) -> None:
        doc_badges: list[str] = []
        web_badges: list[str] = []
        for title in titles:
            show_title = _title_to_display_domain(title)
            label = f"{show_title} {_pages_str(grouped.get(title, []))}"
            if title.strip().lower().startswith("websearch:"):
                web_badges.append(_badge_directive(label, "violet"))
            else:
                doc_badges.append(_badge_directive(label, "green"))
        if doc_badges:
            st.markdown(" ".join(doc_badges))
        if web_badges:
            st.markdown(" ".join(web_badges))

    st.markdown("### ä¾†æº")
    _render_badges(inline_titles)
    if extra_titles:
        with st.expander(f"æ›´å¤šä¾†æºï¼ˆ{len(extra_titles)}ï¼‰", expanded=False):
            _render_badges(extra_titles)


# =========================
# Formatterï¼ˆå¯æ„›ä½†å…‹åˆ¶ï¼‰
# =========================
FORMATTER_SYSTEM_PROMPT = r"""
ä½ æ˜¯å®‰å¦®äºï¼ˆAnya Forgerï¼Œã€ŠSPYÃ—FAMILYã€‹ï¼‰é¢¨æ ¼çš„ã€Œå¯é å°å¹«æ‰‹ã€ï¼Œä½†ä½ çš„æœ¬è·æ˜¯ï¼šMarkdown è¼¸å‡ºæ’ç‰ˆç¾åŒ–ï¼ˆformatterï¼‰ã€‚
é¢¨æ ¼ç›®æ¨™ï¼šå¯æ„›ä½†å…‹åˆ¶ã€é‡é»å…ˆè¡Œã€ä¸å‡ºéŒ¯ã€‚

ä»»å‹™ï¼ˆåªåšç‰ˆé¢ï¼Œä¸æ”¹å…§å®¹ï¼‰ï¼š
- èª¿æ•´æ¨™é¡Œå±¤ç´šï¼ˆ# / ## / ###ï¼‰ã€è£œç©ºè¡Œã€åˆ†æ®µ
- éé•·æ®µè½å¯æ”¹æˆ bulletsï¼ˆæ¯é»ä¸€ä»¶äº‹ï¼‰
- çµ±ä¸€ç« ç¯€çµæ§‹ã€è®“é–±è®€æ›´æ¸…æ¥š

åš´æ ¼ç¦æ­¢ï¼š
- æ–°å¢ä»»ä½•äº‹å¯¦ã€æ•¸å­—ã€æ—¥æœŸã€ä¸»å¼µã€æ¨è«–ã€æ¡ˆä¾‹
- æ”¹è®ŠåŸæ–‡æ„æ€
- æé€ /è£œä¸Šä¸å­˜åœ¨çš„å¼•ç”¨

å¼•ç”¨ token ç¡¬è¦å‰‡ï¼ˆå¿…é ˆé€å­—ä¿ç•™ï¼‰ï¼š
- å½¢å¦‚ [å ±å‘Šåç¨± pé ]ã€[WebSearch:... p-]ã€[A p2][B p8] çš„ token ä¸å¯æ”¹å¯«ã€ä¸å¯åˆªé™¤ã€ä¸å¯åˆä½µ
- è‹¥ä½ æŠŠæ®µè½æ”¹æˆ bulletï¼Œå¼•ç”¨ token è¦æ”¾å›å°æ‡‰ bullet å¥å°¾

å¦å¤–ï¼š
- ä¸å¾—æåŠå…§éƒ¨æµç¨‹ã€æª”åã€é¡åº¦ä¸è¶³ã€Budget exceeded ç­‰å­—æ¨£ã€‚
- è‹¥é‡åˆ°ã€Œç¼ºå£/ä¸è¶³ã€æè¿°ï¼Œè«‹ç›´æ¥çœç•¥è©²æ®µè½ï¼ˆä½¿ç”¨è€…åªæƒ³çœ‹æœ‰æŸ¥åˆ°çš„å…§å®¹ï¼‰ã€‚

è¼¸å‡ºï¼š
- åªè¼¸å‡ºæ’ç‰ˆå¾Œçš„ Markdownï¼Œä¸è¦è§£é‡‹ã€ä¸åŠ å‰è¨€

# æ ¼å¼åŒ–è¦å‰‡
- æ ¹æ“šå…§å®¹é¸æ“‡æœ€åˆé©çš„ Markdown æ ¼å¼åŠå½©è‰²å¾½ç« ï¼ˆcolored badgesï¼‰å…ƒç´ è¡¨é”ã€‚
- å¯æ„›èªæ°£èˆ‡å½©è‰²å…ƒç´ æ˜¯è¼”åŠ©é–±è®€çš„è£é£¾ï¼Œè€Œä¸æ˜¯ä¸»è¦çµæ§‹ï¼›**ä¸å¯å–ä»£æ¸…æ¥šçš„æ¨™é¡Œã€æ¢åˆ—èˆ‡æ®µè½çµ„ç¹”**ã€‚

# Markdown æ ¼å¼èˆ‡ emojiï¼é¡è‰²ç”¨æ³•èªªæ˜
## åŸºæœ¬åŸå‰‡
- æ ¹æ“šå…§å®¹é¸æ“‡æœ€åˆé©çš„å¼·èª¿æ–¹å¼ï¼Œè®“å›æ‡‰æ¸…æ¥šã€æ˜“è®€ã€æœ‰å±¤æ¬¡ï¼Œé¿å…éåº¦ä½¿ç”¨å½©è‰²æ–‡å­—èˆ‡ emoji é€ æˆè¦–è¦ºè² æ“”ã€‚
- åªç”¨ Streamlit æ”¯æ´çš„ Markdown èªæ³•ï¼Œä¸è¦ç”¨ HTML æ¨™ç±¤ã€‚

## åŠŸèƒ½èˆ‡èªæ³•
- **ç²—é«”**ï¼š`**é‡é»**` â†’ **é‡é»**
- *æ–œé«”*ï¼š`*æ–œé«”*` â†’ *æ–œé«”*
- æ¨™é¡Œï¼š`# å¤§æ¨™é¡Œ`ã€`## å°æ¨™é¡Œ`
- åˆ†éš”ç·šï¼š`---`
- è¡¨æ ¼ï¼ˆåƒ…éƒ¨åˆ†å¹³å°æ”¯æ´ï¼Œå»ºè­°ç”¨æ¢åˆ—å¼ï¼‰
- å¼•ç”¨ï¼š`> é€™æ˜¯é‡é»æ‘˜è¦`
- emojiï¼šç›´æ¥è¼¸å…¥æˆ–è²¼ä¸Šï¼Œå¦‚ ğŸ˜„
- Material Symbolsï¼š`:material/star:`
- LaTeX æ•¸å­¸å…¬å¼ï¼š`$å…¬å¼$` æˆ– `$$å…¬å¼$$`
- å½©è‰²æ–‡å­—ï¼š`:orange[é‡é»]`ã€`:blue[èªªæ˜]`
- å½©è‰²èƒŒæ™¯ï¼š`:orange-background[è­¦å‘Šå…§å®¹]`
- å½©è‰²å¾½ç« ï¼š`:orange-badge[é‡é»]`ã€`:blue-badge[è³‡è¨Š]`
- å°å­—ï¼š`:small[é€™æ˜¯è¼”åŠ©èªªæ˜]`

## é¡è‰²åç¨±åŠå»ºè­°ç”¨é€”ï¼ˆæ¢åˆ—å¼ï¼Œè·¨å¹³å°ç©©å®šï¼‰
- **blue**ï¼šè³‡è¨Šã€ä¸€èˆ¬é‡é»
- **green**ï¼šæˆåŠŸã€æ­£å‘ã€é€šé
- **orange**ï¼šè­¦å‘Šã€é‡é»ã€æº«æš–
- **red**ï¼šéŒ¯èª¤ã€è­¦å‘Šã€å±éšª
- **violet**ï¼šå‰µæ„ã€æ¬¡è¦é‡é»
- **gray/grey**ï¼šè¼”åŠ©èªªæ˜ã€å‚™è¨»
- **rainbow**ï¼šå½©è‰²å¼·èª¿ã€æ´»æ½‘
- **primary**ï¼šä¾ä¸»é¡Œè‰²è‡ªå‹•è®ŠåŒ–

**æ³¨æ„ï¼š**
- åªèƒ½ä½¿ç”¨ä¸Šè¿°é¡è‰²ã€‚**è«‹å‹¿ä½¿ç”¨ yellowï¼ˆé»ƒè‰²ï¼‰**ï¼Œå¦‚éœ€é»ƒè‰²æ•ˆæœï¼Œè«‹æ”¹ç”¨ orange æˆ–é»ƒè‰² emojiï¼ˆğŸŸ¡ã€âœ¨ã€ğŸŒŸï¼‰å¼·èª¿ã€‚
- ä¸æ”¯æ´ HTML æ¨™ç±¤ï¼Œè«‹å‹¿ä½¿ç”¨ `<span>`ã€`<div>` ç­‰èªæ³•ã€‚
- å»ºè­°åªç”¨æ¨™æº– Markdown èªæ³•ï¼Œä¿è­‰è·¨å¹³å°é¡¯ç¤ºæ­£å¸¸ã€‚
"""


def format_markdown_output_preserve_citations(client: OpenAI, md: str) -> str:
    raw = (md or "").strip()
    if not raw:
        return ""
    out, _ = call_gpt(
        client,
        model=MODEL_MAIN,
        system=FORMATTER_SYSTEM_PROMPT,
        user=raw,
        reasoning_effort=None,
        tools=None,
        include_sources=False,
    )
    return (out or "").strip() or raw


# =========================
# Default outputs (summary/claims/chain)ï¼ˆä¿ç•™åŸæ¨£ï¼‰
# =========================
def _split_default_bundle(text: str) -> Dict[str, str]:
    t = (text or "").strip()
    pattern = re.compile(
        r"###\s*SUMMARY\s*(.*?)###\s*CLAIMS\s*(.*?)###\s*CHAIN\s*(.*)$",
        re.IGNORECASE | re.DOTALL,
    )
    m = pattern.search(t)
    if not m:
        return {"summary": "", "claims": "", "chain": ""}
    return {"summary": m.group(1).strip(), "claims": m.group(2).strip(), "chain": m.group(3).strip()}


def pick_corpus_chunks_for_default(all_chunks: list[Chunk]) -> list[Chunk]:
    by_title: Dict[str, list[Chunk]] = {}
    for c in all_chunks:
        by_title.setdefault(c.title, []).append(c)

    kw = re.compile(
        r"(outlook|risk|implication|forecast|scenario|inflation|rate|credit|spread|cap rate|vacancy|supply|demand|rental|office|retail|residential|logistics|hotel|reits)",
        re.I,
    )

    def score(c: Chunk) -> float:
        s = 0.0
        if kw.search(c.text or ""):
            s += 6.0
        if c.page is not None:
            s += max(0.0, 2.0 - min(2.0, float(c.page) / 6.0))
        s += min(2.0, len(c.text) / 1400.0)
        return s

    chosen: list[Chunk] = []
    for title, chunks in sorted(by_title.items(), key=lambda x: x[0]):
        by_page: Dict[int, list[Chunk]] = {}
        for c in chunks:
            p = c.page if c.page is not None else 0
            by_page.setdefault(p, []).append(c)

        page_best: list[Chunk] = []
        for p, cs in by_page.items():
            cs = sorted(cs, key=score, reverse=True)
            page_best.append(cs[0])

        page_best = sorted(page_best, key=score, reverse=True)
        chosen.extend(page_best[:CORPUS_PER_REPORT_QUOTA])

    chosen = sorted(chosen, key=score, reverse=True)[:CORPUS_DEFAULT_MAX_CHUNKS]
    return chosen


def render_chunks_for_model(chunks: list[Chunk], max_chars_each: int = 900) -> str:
    parts = []
    for c in chunks:
        head = f"[{c.title} p{c.page if c.page is not None else '-'}]"
        parts.append(head + "\n" + c.text[:max_chars_each])
    return "\n\n".join(parts)


def bullets_all_have_citations(md: str) -> bool:
    lines = (md or "").splitlines()
    if not any(BULLET_RE.match(l) for l in lines):
        return False
    for line in lines:
        if BULLET_RE.match(line):
            cits = [m.group(0) for m in re.finditer(r"\[[^\]]+?\s+p(\d+|-)\s*\]", line)]
            cits = [c for c in cits if not EVIDENCE_PATH_IN_CIT_RE.search(c)]
            if not cits:
                return False
    return True


def generate_default_outputs_bundle(client: OpenAI, title: str, ctx: str, max_retries: int = 2) -> Dict[str, str]:
    system = (
        "ä½ æ˜¯åš´è¬¹çš„ç ”ç©¶åŠ©ç†ï¼Œåªèƒ½æ ¹æ“šæˆ‘æä¾›çš„è³‡æ–™å›ç­”ï¼Œä¸å¯è…¦è£œã€‚\n"
        "ç¡¬æ€§è¦å‰‡ï¼š\n"
        "1) ä½ å¿…é ˆè¼¸å‡ºä¸‰å€‹å€å¡Šï¼Œä¸”é †åº/æ¨™é¡Œå›ºå®šï¼š### SUMMARYã€### CLAIMSã€### CHAINã€‚\n"
        "2) æ¯å€‹å€å¡Šéƒ½å¿…é ˆæ˜¯ç´” bulletï¼ˆæ¯è¡Œä»¥ - é–‹é ­ï¼‰ï¼Œä¸è¦æ®µè½ã€‚\n"
        "3) æ¯å€‹ bullet å¥å°¾å¿…é ˆé™„å¼•ç”¨ï¼Œæ ¼å¼å›ºå®šï¼š[å ±å‘Šåç¨± pé ]\n"
        "4) å¼•ç”¨ä¸­çš„ã€å ±å‘Šåç¨±ã€å¿…é ˆæ˜¯è³‡æ–™ç‰‡æ®µæ–¹æ‹¬è™Ÿå…§çš„é‚£å€‹åç¨±ã€‚\n"
        "5) ä¸å¯ä½¿ç”¨ /evidence/*.md ç•¶ä½œå ±å‘Šåç¨±ã€‚\n"
        "6) è‹¥åŒä¸€å¥éœ€è¦å¤šå€‹å¼•ç”¨ï¼Œè«‹ç”¨å¤šå€‹æ–¹æ‹¬è™Ÿé€£çºŒé™„åœ¨å¥å°¾ï¼Œä¾‹å¦‚ï¼š[A p2][B p8]ï¼Œä¸è¦åœ¨åŒä¸€å° [] å…§ç”¨åˆ†è™Ÿå¡å¤šç­†ã€‚\n"
    )
    user = (
        f"è«‹é‡å°ã€Š{title}ã€‹ä¸€æ¬¡è¼¸å‡ºä¸‰ä»½å…§å®¹ï¼ˆèåˆå¤šä»½å ±å‘Šï¼‰ï¼š\n"
        f"- SUMMARYï¼š8~14 bullets\n"
        f"- CLAIMSï¼š8~14 bullets\n"
        f"- CHAINï¼š6~12 bullets\n\n"
        f"è³‡æ–™ï¼š\n{ctx}\n"
    )

    last = ""
    for _ in range(max_retries + 1):
        out, _ = call_gpt(client, model=MODEL_MAIN, system=system, user=user, reasoning_effort=REASONING_EFFORT)
        parts = _split_default_bundle(out)
        ok = bullets_all_have_citations(parts["summary"]) and bullets_all_have_citations(parts["claims"]) and bullets_all_have_citations(parts["chain"])
        if ok:
            return parts
        last = out
        user += "\n\nã€å¼·åˆ¶ä¿®æ­£ã€‘æ•´ä»½é‡å¯«ï¼šä¸‰å€å¡Šçš†ç‚ºç´” bulletï¼Œä¸”æ¯å€‹ bullet å¥å°¾éƒ½æœ‰ [å ±å‘Šåç¨± pé ]ï¼›ä¸å¾—å‡ºç¾ /evidence/*.mdã€‚å¤šå¼•ç”¨ç”¨ [A p2][B p8]ã€‚"

    return _split_default_bundle(last)


# =========================
# Direct mode todos.jsonï¼ˆä½ è¦æ±‚ direct ä¹Ÿè¦ç”¢ï¼‰
# =========================
def build_todos_json_for_question(client: OpenAI, question: str, *, enable_web: bool, has_index: bool, planned_mode: str) -> str:
    system = (
        "ä½ æ˜¯ä»»å‹™è¦åŠƒå™¨ã€‚è«‹è¼¸å‡º todos.jsonï¼ˆJSON array of stringsï¼‰ã€‚\n"
        "ç›®çš„ï¼šç”¨ä¾†è¦åŠƒã€å¦‚ä½•å›ç­”ä½¿ç”¨è€…å•é¡Œã€ã€‚\n"
        "ç¡¬è¦å‰‡ï¼š\n"
        "1) åªè¼¸å‡º JSON arrayï¼ˆä¸è¦ markdownã€ä¸è¦è§£é‡‹ï¼‰ã€‚\n"
        "2) 5~9 å€‹ stepsã€‚\n"
        "3) ç¬¬ 1 æ­¥è¦èªªæ˜ enable_web/has_index/planned_mode èˆ‡æœ¬é¡Œç›®æ¨™ã€‚\n"
    )
    user = f"enable_web={str(enable_web).lower()}\nhas_index={str(has_index).lower()}\nplanned_mode={planned_mode}\n\nå•é¡Œï¼š{question}"
    out, _ = call_gpt(
        client,
        model=MODEL_MAIN,
        system=system,
        user=user,
        reasoning_effort=None,
        tools=None,
        include_sources=False,
    )
    data = _try_parse_json_or_py_literal(out)
    if isinstance(data, list):
        steps = [str(x) for x in data if str(x).strip()]
        return json.dumps(steps[:12], ensure_ascii=False, indent=2)

    steps = [
        f"èªªæ˜ enable_web={str(enable_web).lower()}ã€has_index={str(has_index).lower()}ã€planned_mode={planned_mode} èˆ‡æœ¬é¡Œç›®æ¨™",
        "é‡æ¸…å•é¡Œç¯„åœèˆ‡é—œéµåè©ï¼Œé¿å…èª¤è§£",
        "åˆ—å‡ºéœ€è¦å¼•ç”¨/æŸ¥è­‰çš„è³‡è¨Šé»",
        "è‹¥å¯ç”¨æ–‡ä»¶ç´¢å¼•ï¼Œè¦åŠƒå¦‚ä½•æª¢ç´¢èˆ‡å¼•ç”¨ï¼›è‹¥å•Ÿç”¨ç¶²æœï¼Œè¦åŠƒè¦æŸ¥çš„ä¸»é¡Œèˆ‡ä¾†æºé¡å‹",
        "æ•´ç†å¯ç”¨è­‰æ“šå¾Œï¼Œç”¢å‡ºçµæ§‹åŒ–å›è¦†ï¼ˆçµè«–å…ˆè¡Œ + åˆ†é»ï¼‰",
        "æœ€çµ‚æª¢æŸ¥ï¼šä¸æå…§éƒ¨æµç¨‹èˆ‡æª”åï¼Œåªä¿ç•™æœ‰ä¾†æºæ”¯æ’çš„å…§å®¹",
    ]
    return json.dumps(steps, ensure_ascii=False, indent=2)


# =========================
# DeepAgent
# =========================
def ensure_deep_agent(client: OpenAI, store: FaissStore, enable_web: bool):
    _require_deepagents()
    from langchain_core.tools import BaseTool, StructuredTool

    st.session_state.setdefault("deep_agent", None)
    st.session_state.setdefault("deep_agent_web_flag", None)
    st.session_state.setdefault("da_usage", {"doc_search_calls": 0, "web_search_calls": 0})

    if (st.session_state.deep_agent is not None) and (st.session_state.deep_agent_web_flag == bool(enable_web)):
        return st.session_state.deep_agent

    lock = threading.Lock()
    usage = {"doc_search_calls": 0, "web_search_calls": 0}
    st.session_state["da_usage"] = usage

    def _inc(name: str, limit: int) -> bool:
        with lock:
            usage[name] += 1
            st.session_state["da_usage"] = usage
            return usage[name] <= limit

    def _get_usage_fn() -> str:
        with lock:
            return json.dumps(usage, ensure_ascii=False)

    def _doc_list_fn() -> str:
        by_title: Dict[str, int] = {}
        for c in store.chunks:
            by_title[c.title] = by_title.get(c.title, 0) + 1
        lines = [f"- {t} (chunks={n})" for t, n in sorted(by_title.items(), key=lambda x: x[0])]
        return "\n".join(lines) if lines else "ï¼ˆç›®å‰æ²’æœ‰ä»»ä½•å·²ç´¢å¼•æ–‡ä»¶ï¼‰"

    def _doc_search_fn(query: str, k: int = 8) -> str:
        if not _inc("doc_search_calls", DA_MAX_DOC_SEARCH_CALLS):
            return json.dumps({"hits": [], "error": f"Budget exceeded: doc_search_calls > {DA_MAX_DOC_SEARCH_CALLS}"}, ensure_ascii=False)

        q = (query or "").strip()
        if not q:
            return json.dumps({"hits": []}, ensure_ascii=False)

        qvec = embed_texts(client, [q])
        hits = store.search(qvec, k=max(1, min(12, int(k))))

        payload = {"hits": []}
        for score, ch in hits:
            payload["hits"].append({
                "title": ch.title,
                "page": str(ch.page) if ch.page is not None else "-",
                "chunk_id": ch.chunk_id,  # internal only
                "text": (ch.text or "")[:1200],
                "score": float(score),
            })
        return json.dumps(payload, ensure_ascii=False)

    def _doc_get_chunk_fn(chunk_id: str, max_chars: int = 2600) -> str:
        cid = (chunk_id or "").strip()
        if not cid:
            return ""
        for c in store.chunks:
            if c.chunk_id == cid:
                return (c.text or "")[:max_chars]
        return ""

    def _mk_tool(fn, name: str, description: str) -> BaseTool:
        return StructuredTool.from_function(fn, name=name, description=description)

    tool_get_usage = _mk_tool(_get_usage_fn, "get_usage", "Get current tool usage counters as JSON (budget/debug).")
    tool_doc_list = _mk_tool(_doc_list_fn, "doc_list", "List indexed documents and chunk counts.")
    tool_doc_search = _mk_tool(_doc_search_fn, "doc_search", "Semantic search over indexed chunks. Returns JSON hits with title/page/chunk_id/text.")
    tool_doc_get_chunk = _mk_tool(_doc_get_chunk_fn, "doc_get_chunk", "Fetch full text for a given chunk_id for close reading. Returns text only.")

    tools: list[BaseTool] = [tool_get_usage, tool_doc_list, tool_doc_search, tool_doc_get_chunk]

    tool_web_search_summary: Optional[BaseTool] = None
    if enable_web:
        def _web_search_summary_fn(query: str) -> str:
            if not _inc("web_search_calls", DA_MAX_WEB_SEARCH_CALLS):
                # ä»å›å‚³å¼•ç”¨æ¨™é ­ï¼Œä½†ä¸è¦æŠŠ Budget exceeded å¯«é€²æˆå“å…§å®¹ï¼ˆwriter æœƒçœç•¥ç„¡ä¾†æºæ®µè½ï¼‰
                return "[WebSearch:web p-]\nSources:\n- web | (budget exceeded) |"

            q = (query or "").strip()
            if not q:
                return "[WebSearch:web p-]\nSources:\n- web | (empty query) |"

            system = (
                "ä½ æ˜¯ç ”ç©¶åŠ©ç†ã€‚ç”¨ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰æ•´ç† web_search çµæœã€‚\n"
                "è¼¸å‡ºæ ¼å¼å¿…é ˆå›ºå®šï¼š\n"
                "1) å…ˆç”¨ 3~8 å€‹ bullets æ‘˜è¦ï¼ˆæ¯é»ä¸€å¥ï¼Œæ¸…æ¥šã€å¸¶æ—¥æœŸ/æ•¸å­—å‰‡ä¿ç•™ï¼‰ã€‚\n"
                "2) æœ€å¾Œä¸€å®šè¦æœ‰ä¸€æ®µ Sources:ï¼Œç”¨æ¢åˆ—åˆ—å‡ºä¾†æºã€‚\n"
                "   æ¯åˆ—æ ¼å¼ï¼š- <domain> | <title> | <url>\n"
                "è¦å‰‡ï¼š\n"
                "- ä¸è¦æåˆ°å·¥å…·æµç¨‹/é¡åº¦/Budget exceededã€‚\n"
                "- è‹¥æ‰¾ä¸åˆ°å¯é ä¾†æºï¼šæ‘˜è¦å¯ç‚ºç©ºï¼Œä½†ä»è¦è¼¸å‡º Sources:ï¼ˆå¯èƒ½ç‚ºç©ºï¼‰ã€‚\n"
            )
            user = f"Search term: {q}"
            text, sources = call_gpt(
                client,
                model=MODEL_WEB,
                system=system,
                user=user,
                reasoning_effort=None,
                tools=[{"type": "web_search"}],
                include_sources=True,
            )

            src_lines = []
            domains = []
            for s in (sources or [])[:10]:
                if isinstance(s, dict):
                    t = (s.get("title") or s.get("source") or "source").strip()
                    u = (s.get("url") or "").strip()
                    dom = _domain_from_url(u) if u else "web"
                    domains.append(dom)
                    if u:
                        src_lines.append(f"- {dom} | {t} | {u}")

            out_text = (text or "").strip()
            # âœ… ç”¨ç¬¬ä¸€å€‹ä¾†æº domain ç•¶ citation titleï¼ˆbadge å°±åªæœƒé¡¯ç¤º domainï¼‰
            primary_domain = domains[0] if domains else "web"

            # ä¿è­‰æœ‰ Sources æ®µ
            if src_lines:
                out_text = (out_text + "\n\nSources:\n" + "\n".join(src_lines)).strip()
            else:
                out_text = (out_text + "\n\nSources:").strip()

            return f"[WebSearch:{primary_domain} p-]\n" + out_text[:2400]

        tool_web_search_summary = _mk_tool(_web_search_summary_fn, "web_search_summary", "Run web_search and return a short Traditional Chinese summary with sources + domain.")
        tools.append(tool_web_search_summary)

    if enable_web:
        def _web_search_summary_fn(query: str) -> str:
            if not _inc("web_search_calls", DA_MAX_WEB_SEARCH_CALLS):
                # âœ… ä¸è¦æŠŠ Budget exceeded çš„æ–‡å­—å¡é€² evidence/è‰ç¨¿ï¼Œé¿å…æ±¡æŸ“æœ€çµ‚è¼¸å‡º
                return "[WebSearch:web p-]\nSources:"

            q = (query or "").strip()
            if not q:
                return "[WebSearch:web p-]\nSources:"

            system = (
                "ä½ æ˜¯ç ”ç©¶åŠ©ç†ã€‚ç”¨ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰æ•´ç† web_search çµæœã€‚\n"
                "è¼¸å‡ºæ ¼å¼å¿…é ˆå›ºå®šï¼š\n"
                "1) å…ˆç”¨ 3~8 å€‹ bullets æ‘˜è¦ï¼ˆæ¯é»ä¸€å¥ï¼Œæ¸…æ¥šã€å¸¶æ—¥æœŸ/æ•¸å­—å‰‡ä¿ç•™ï¼‰ã€‚\n"
                "2) æœ€å¾Œä¸€å®šè¦æœ‰ä¸€æ®µ Sources:ï¼Œç”¨æ¢åˆ—åˆ—å‡ºä¾†æºã€‚\n"
                "   æ¯åˆ—æ ¼å¼ï¼š- <domain> | <title> | <url>\n"
                "è¦å‰‡ï¼š\n"
                "- ä¸è¦æåˆ°å·¥å…·æµç¨‹/é¡åº¦/Budget exceededã€‚\n"
                "- è‹¥æ‰¾ä¸åˆ°å¯é ä¾†æºï¼šæ‘˜è¦å¯ç‚ºç©ºï¼Œä½†ä»è¦è¼¸å‡º Sources:ã€‚\n"
            )
            user = f"Search term: {q}"
            text, sources = call_gpt(
                client,
                model=MODEL_WEB,
                system=system,
                user=user,
                reasoning_effort=None,
                tools=[{"type": "web_search"}],
                include_sources=True,
            )

            def _domain(u: str) -> str:
                try:
                    host = urlparse(u).netloc.lower()
                    if host.startswith("www."):
                        host = host[4:]
                    return host or "web"
                except Exception:
                    return "web"

        # âœ… ç”¨ç¬¬ä¸€å€‹ä¾†æºçš„ domain ç•¶ citation headerï¼š [WebSearch:<domain> p-]
            primary_domain = "web"
            if isinstance(sources, list) and sources:
                first = sources[0] if isinstance(sources[0], dict) else {}
                u0 = (first.get("url") or "").strip()
                if u0:
                    primary_domain = _domain(u0)

            src_lines = []
            for s in (sources or [])[:10]:
                if isinstance(s, dict):
                    t = (s.get("title") or s.get("source") or "source").strip()
                    u = (s.get("url") or "").strip()
                    if u:
                        src_lines.append(f"- {_domain(u)} | {t} | {u}")

            out_text = (text or "").strip()
            if src_lines:
                out_text = (out_text + "\n\nSources:\n" + "\n".join(src_lines)).strip()
            else:
                # ä»ä¿ç•™ Sources: æ¨™é ­ï¼Œè®“ downstream å¥½è§£æ
                out_text = (out_text + "\n\nSources:").strip()

            return f"[WebSearch:{primary_domain} p-]\n" + out_text[:2400]

        tool_web_search_summary = _mk_tool(
            _web_search_summary_fn,
            "web_search_summary",
            "Run web_search and return a short Traditional Chinese summary with sources (domain|title|url).",
        )
        tools.append(tool_web_search_summary)
    
    retriever_prompt = f"""
ä½ æ˜¯æ–‡ä»¶æª¢ç´¢å°ˆå®¶ï¼ˆåªå…è¨±ä½¿ç”¨ doc_list/doc_search/doc_get_chunk/get_usageï¼‰ã€‚

facet å­ä»»å‹™æ ¼å¼ï¼š
facet_slug: <è‹±æ–‡å°å¯«_åº•ç·š>
facet_goal: <é€™å€‹é¢å‘è¦å›ç­”ä»€éº¼>
hints: <å¯èƒ½çš„é—œéµå­—/æŒ‡æ¨™/åè©ï¼ˆå¯ç©ºï¼‰>

ç¡¬è¦å‰‡ï¼š
- ä½ è¦å¯«å…¥ /evidence/doc_<facet_slug>.md
- evidence å…§å®¹åªèƒ½åŒ…å«ï¼š
  1) å¼•ç”¨æ¨™é ­ï¼š[å ±å‘Šåç¨± pé ]ï¼ˆä¸å¾—åŒ…å« /evidence/ è·¯å¾‘ï¼›ä¸å¾—ç”¨ doc_*.md ç•¶å ±å‘Šåç¨±ï¼‰
  2) åŸæ–‡ç‰‡æ®µï¼ˆå¯æˆªæ–·ï¼‰
  3) ä¸€è¡Œèªªæ˜ã€Œé€™æ®µæ”¯æŒä»€éº¼ã€
- ä½ å¯ä»¥ç”¨ doc_search æ‹¿åˆ° chunk_idï¼Œç„¶å¾Œç”¨ doc_get_chunk(chunk_id=...) ç²¾è®€ï¼Œ
  ä½† chunk_id çµ•å°ä¸èƒ½å¯«é€² evidenceã€‚

è‹¥é‡åˆ° Budget exceededï¼šåœæ­¢ï¼Œä¸”ä¸è¦æŠŠéŒ¯èª¤å­—ä¸²æŠ„é€² evidenceï¼ˆåªè¦åœæ­¢å³å¯ï¼‰ã€‚
æœ€å¾Œå›è¦† orchestratorï¼šâ‰¤150 å­—æ‘˜è¦ï¼ˆæ‰¾åˆ°ä»€éº¼ + æœ€å¤§ç¼ºå£ï¼‰
"""

    writer_prompt = f"""
ä½ æ˜¯å¯«ä½œ/æ•´ç†å°ˆå®¶ï¼ˆç”¨ read_file/glob/grep/write_file/edit_file/lsï¼‰ã€‚
ä½ å¿…é ˆæ•´åˆ /evidence/ åº•ä¸‹æ‰€æœ‰æª”æ¡ˆï¼ˆdoc_*.md èˆ‡å¯é¸ web_*.mdï¼‰ã€‚

ä»»å‹™é¡å‹åˆ¤æ–·ï¼š
- å®Œæ•´å ±å‘Š/ç« ç¯€ â†’ REPORT
- å–®é¡Œå›ç­” â†’ QA
- æ•´ç†çŸ¥è­˜è„ˆçµ¡ â†’ KNOWLEDGE
- ä½¿ç”¨è€…è²¼è‰ç¨¿è¦æŸ¥æ ¸/é™¤éŒ¯ â†’ VERIFY_DRAFTï¼ˆæœ€å¤š {DA_MAX_CLAIMS} æ¢ä¸»å¼µï¼‰

å¼•ç”¨è¦å‰‡ï¼ˆåš´æ ¼ï¼‰ï¼š
- QAï¼šç´” bulletï¼ˆæ¯è¡Œ -ï¼‰ï¼Œæ¯å€‹ bullet å¥å°¾å¿…æœ‰å¼•ç”¨ [å ±å‘Šåç¨± pé ] æˆ– [WebSearch:* p-]
- REPORT/KNOWLEDGE/VERIFYï¼šMarkdownï¼›æ¯å€‹éæ¨™é¡Œæ®µè½è‡³å°‘ 1 å€‹å¼•ç”¨
- enable_web=falseï¼šä¸å¾—å‡ºç¾ WebSearch
- draft çµ•å°ä¸èƒ½å‡ºç¾ chunk_id
- å ±å‘Šåç¨±ä¸å¾—æ˜¯ /evidence/*.md
- è‹¥åŒä¸€å¥éœ€è¦å¤šå€‹å¼•ç”¨ï¼Œè«‹ç”¨å¤šå€‹æ–¹æ‹¬è™Ÿé€£çºŒé™„åœ¨å¥å°¾ï¼Œä¾‹å¦‚ï¼š[A p2][B p8]

è¼¸å‡ºç¦å‰‡ï¼ˆéå¸¸é‡è¦ï¼‰ï¼š
- æˆå“ /draft.md ä¸å¾—æåŠå…§éƒ¨æµç¨‹æˆ–æª”åï¼šä¸å¾—å‡ºç¾ã€Œ/evidenceã€ã€ã€Œdoc_*.mdã€ã€ã€Œweb_*.mdã€ã€ã€ŒBudget exceededã€ã€ã€Œé¡åº¦ä¸è¶³ã€ã€ã€Œå ä½ã€ã€ã€Œå‘é‡åº«ã€ç­‰å­—æ¨£ã€‚
- è‹¥æŸé¢å‘æ‰¾ä¸åˆ°å¯å¼•ç”¨ä¾†æºï¼šåœ¨ /draft.md ç›´æ¥çœç•¥è©²é¢å‘ï¼Œä¸è¦å¯«ã€Œè­‰æ“šä¸è¶³/é¡åº¦ä¸è¶³/æœªèƒ½å–å¾—ã€ç­‰æ®µè½ã€‚
- åªå¯«ä½ æœ‰å¼•ç”¨æ”¯æ’ã€èƒ½èªªæ¸…æ¥šçš„å…§å®¹ã€‚

# æ ¼å¼åŒ–è¦å‰‡
- æ ¹æ“šå…§å®¹é¸æ“‡æœ€åˆé©çš„ Markdown æ ¼å¼åŠå½©è‰²å¾½ç« ï¼ˆcolored badgesï¼‰å…ƒç´ è¡¨é”ã€‚
- å¯æ„›èªæ°£èˆ‡å½©è‰²å…ƒç´ æ˜¯è¼”åŠ©é–±è®€çš„è£é£¾ï¼Œè€Œä¸æ˜¯ä¸»è¦çµæ§‹ï¼›**ä¸å¯å–ä»£æ¸…æ¥šçš„æ¨™é¡Œã€æ¢åˆ—èˆ‡æ®µè½çµ„ç¹”**ã€‚

# Markdown æ ¼å¼èˆ‡ emojiï¼é¡è‰²ç”¨æ³•èªªæ˜
## åŸºæœ¬åŸå‰‡
- æ ¹æ“šå…§å®¹é¸æ“‡æœ€åˆé©çš„å¼·èª¿æ–¹å¼ï¼Œè®“å›æ‡‰æ¸…æ¥šã€æ˜“è®€ã€æœ‰å±¤æ¬¡ï¼Œé¿å…éåº¦ä½¿ç”¨å½©è‰²æ–‡å­—èˆ‡ emoji é€ æˆè¦–è¦ºè² æ“”ã€‚
- åªç”¨ Streamlit æ”¯æ´çš„ Markdown èªæ³•ï¼Œä¸è¦ç”¨ HTML æ¨™ç±¤ã€‚

## åŠŸèƒ½èˆ‡èªæ³•
- **ç²—é«”**ï¼š`**é‡é»**` â†’ **é‡é»**
- *æ–œé«”*ï¼š`*æ–œé«”*` â†’ *æ–œé«”*
- æ¨™é¡Œï¼š`# å¤§æ¨™é¡Œ`ã€`## å°æ¨™é¡Œ`
- åˆ†éš”ç·šï¼š`---`
- è¡¨æ ¼ï¼ˆåƒ…éƒ¨åˆ†å¹³å°æ”¯æ´ï¼Œå»ºè­°ç”¨æ¢åˆ—å¼ï¼‰
- å¼•ç”¨ï¼š`> é€™æ˜¯é‡é»æ‘˜è¦`
- emojiï¼šç›´æ¥è¼¸å…¥æˆ–è²¼ä¸Šï¼Œå¦‚ ğŸ˜„
- Material Symbolsï¼š`:material/star:`
- LaTeX æ•¸å­¸å…¬å¼ï¼š`$å…¬å¼$` æˆ– `$$å…¬å¼$$`
- å½©è‰²æ–‡å­—ï¼š`:orange[é‡é»]`ã€`:blue[èªªæ˜]`
- å½©è‰²èƒŒæ™¯ï¼š`:orange-background[è­¦å‘Šå…§å®¹]`
- å½©è‰²å¾½ç« ï¼š`:orange-badge[é‡é»]`ã€`:blue-badge[è³‡è¨Š]`
- å°å­—ï¼š`:small[é€™æ˜¯è¼”åŠ©èªªæ˜]`

## é¡è‰²åç¨±åŠå»ºè­°ç”¨é€”ï¼ˆæ¢åˆ—å¼ï¼Œè·¨å¹³å°ç©©å®šï¼‰
- **blue**ï¼šè³‡è¨Šã€ä¸€èˆ¬é‡é»
- **green**ï¼šæˆåŠŸã€æ­£å‘ã€é€šé
- **orange**ï¼šè­¦å‘Šã€é‡é»ã€æº«æš–
- **red**ï¼šéŒ¯èª¤ã€è­¦å‘Šã€å±éšª
- **violet**ï¼šå‰µæ„ã€æ¬¡è¦é‡é»
- **gray/grey**ï¼šè¼”åŠ©èªªæ˜ã€å‚™è¨»
- **rainbow**ï¼šå½©è‰²å¼·èª¿ã€æ´»æ½‘
- **primary**ï¼šä¾ä¸»é¡Œè‰²è‡ªå‹•è®ŠåŒ–

**æ³¨æ„ï¼š**
- åªèƒ½ä½¿ç”¨ä¸Šè¿°é¡è‰²ã€‚**è«‹å‹¿ä½¿ç”¨ yellowï¼ˆé»ƒè‰²ï¼‰**ï¼Œå¦‚éœ€é»ƒè‰²æ•ˆæœï¼Œè«‹æ”¹ç”¨ orange æˆ–é»ƒè‰² emojiï¼ˆğŸŸ¡ã€âœ¨ã€ğŸŒŸï¼‰å¼·èª¿ã€‚
- ä¸æ”¯æ´ HTML æ¨™ç±¤ï¼Œè«‹å‹¿ä½¿ç”¨ `<span>`ã€`<div>` ç­‰èªæ³•ã€‚
- å»ºè­°åªç”¨æ¨™æº– Markdown èªæ³•ï¼Œä¿è­‰è·¨å¹³å°é¡¯ç¤ºæ­£å¸¸ã€‚

## æœ€çµ‚æŠŠçµæœå¯«åˆ° /draft.md
"""

    verifier_prompt = f"""
ä½ æ˜¯å¯©ç¨¿æŸ¥æ ¸å°ˆå®¶ï¼ˆç”¨ read_file/edit_file/grepï¼‰ã€‚
ä»»å‹™ï¼šæª¢æŸ¥ /draft.md æ˜¯å¦ç¬¦åˆå¼•ç”¨è¦†è“‹ï¼Œä¸¦åšã€æœ€å°‘æ”¹å‹•ã€ä¿®æ­£ã€‚

è¦å‰‡ï¼š
- QAï¼šæ¯å€‹ bullet å¥å°¾å¿…æœ‰ [.. p..]
- å…¶ä»–ï¼šæ¯å€‹éæ¨™é¡Œæ®µè½è‡³å°‘ 1 å€‹å¼•ç”¨ [.. p..]
- enable_web=falseï¼šä¸å¾—å‡ºç¾ WebSearch
- è‹¥ /draft.md å‡ºç¾ chunk_id ç—•è·¡ï¼ˆchunk_id= æˆ– _p*_c*ï¼‰ï¼Œå¿…é ˆç§»é™¤ã€‚
- å¼•ç”¨æ¨™é ­ä¸å¾—ä½¿ç”¨ /evidence/*.md

é¡å¤–è¦å‰‡ï¼š
- /draft.md è‹¥å‡ºç¾ã€Œ/evidenceã€doc_ã€web_ã€Budget exceededã€é¡åº¦ä¸è¶³ã€å ä½ã€å‘é‡åº«ã€ç­‰å…§éƒ¨å­—æ¨£ï¼Œå¿…é ˆç§»é™¤ã€‚
- è‹¥æ•´æ®µä¸»è¦åœ¨è¬›ã€Œæ‰¾ä¸åˆ°è³‡æ–™/é¡åº¦ä¸è¶³ã€ï¼Œè«‹åˆªé™¤è©²æ®µè½ï¼ˆä½¿ç”¨è€…åªæƒ³çœ‹æœ‰æŸ¥åˆ°çš„å…§å®¹ï¼‰ã€‚

æœ€å¤šä¿®æ­£ {DA_MAX_REWRITE_ROUNDS} è¼ªï¼š
- æ¯è¼ªï¼šread /draft.md â†’ edit_file ä¿®æ­£ â†’ write /review.md è¨˜éŒ„
"""

    subagents = [
        {
            "name": "retriever",
            "description": "å¾ä¸Šå‚³æ–‡ä»¶å‘é‡åº«æ‰¾è­‰æ“šï¼Œå¯« /evidence/doc_*.mdï¼ˆä¸å« chunk_idï¼‰",
            "system_prompt": retriever_prompt,
            "tools": [tool_get_usage, tool_doc_list, tool_doc_search, tool_doc_get_chunk],
            "model": f"openai:{MODEL_MAIN}",
        },
        {
            "name": "writer",
            "description": "æ•´åˆ /evidence/ â†’ ç”¢ç”Ÿ /draft.md",
            "system_prompt": writer_prompt,
            "tools": [],
            "model": f"openai:{MODEL_MAIN}",
        },
        {
            "name": "verifier",
            "description": "æª¢æŸ¥å¼•ç”¨è¦†è“‹ä¸¦ä¿®ç¨¿ /draft.mdï¼Œå¯« /review.md",
            "system_prompt": verifier_prompt,
            "tools": [],
            "model": f"openai:{MODEL_MAIN}",
        },
    ]

    if enable_web:
        web_prompt = """
ä½ æ˜¯ç¶²è·¯æœå°‹å°ˆå®¶ï¼ˆåªå…è¨± web_search_summary/get_usageï¼›ä¸å…è¨± doc_*ï¼‰ã€‚
facet å­ä»»å‹™æ ¼å¼åŒ retrieverã€‚

ç¡¬è¦å‰‡ï¼š
- å¯«å…¥ /evidence/web_<facet_slug>.md
- æ¯æ®µè¦ä¿ç•™å¼•ç”¨æ¨™é ­ [WebSearch:<domain> p-]
- ç¦æ­¢æé€ ä¾†æº
- ä¸è¦å¯«ã€ŒBudget exceeded/é¡åº¦ä¸è¶³/å ä½ã€åˆ° evidenceï¼›è‹¥æ²’æœ‰ä¾†æºå°±ä¸è¦å¯«é‚£æ®µ
"""
        subagents.insert(
            1,
            {
                "name": "web-researcher",
                "description": "ç”¨ OpenAI å…§å»º web_search åšå°‘é‡é«˜å“è³ªæœå°‹ï¼Œå¯« /evidence/web_*.md",
                "system_prompt": web_prompt,
                "tools": [tool_web_search_summary, tool_get_usage] if tool_web_search_summary else [tool_get_usage],
                "model": f"openai:{MODEL_MAIN}",
            },
        )

    orchestrator_prompt = f"""
ä½ æ˜¯ Deep Doc Orchestratorï¼ˆæ–‡ä»¶å„ªå…ˆï¼›enable_web={str(enable_web).lower()}ï¼‰ã€‚

å›ºå®šæµç¨‹ï¼ˆå¿…åšï¼‰ï¼š
0) ç«‹åˆ»ç”¨ write_file å»ºç«‹ /workspace/todos.jsonï¼ˆJSON array of stringsï¼›5~9 æ­¥ï¼‰ï¼Œä¸¦åœ¨ç¬¬ä¸€æ­¥èªªæ˜ enable_web èˆ‡ç›®æ¨™ã€‚
1) write_file /evidence/README.md è¨˜éŒ„æœ¬æ¬¡éœ€æ±‚èˆ‡ enable_web
2) æ‹† 2â€“4 å€‹ facetsï¼ˆé¢å‘ï¼Œä¸æ˜¯ç« ç¯€ï¼‰
3) å¹³è¡Œæ´¾å·¥ï¼š
   - æ¯å€‹ facet è‡³å°‘æ´¾ 1 å€‹ retriever
   - enable_web=true ä¸”éœ€è¦å¤–éƒ¨èƒŒæ™¯æ™‚ï¼Œå°åŒ facet å†æ´¾ 1 å€‹ web-researcher
4) å« writer ç”¢ç”Ÿ /draft.md
5) å« verifier ä¿®ç¨¿ï¼ˆæœ€å¤š {DA_MAX_REWRITE_ROUNDS} è¼ªï¼‰
6) read_file /draft.md ä½œç‚ºæœ€çµ‚å›ç­”

å¼•ç”¨èˆ‡éš±ç§è¦å‰‡ï¼š
- /evidence èˆ‡ /draft çµ•å°ä¸èƒ½å‡ºç¾ chunk_id
- å¼•ç”¨åªèƒ½ç”¨ [å ±å‘Šåç¨± pé ] æˆ– [WebSearch:<domain> p-]
- å ±å‘Šåç¨±ä¸å¾—ä½¿ç”¨ /evidence/*.md ç•¶ä½œä¾†æºåç¨±
- å¤šå¼•ç”¨è«‹ç”¨ [A p2][B p8]ï¼Œä¸è¦åœ¨åŒä¸€å° [] å…§å¡å¤šç­†
- æˆå“ä¸å¾—æåŠå…§éƒ¨æµç¨‹/æª”å/é¡åº¦ä¸è¶³ï¼›åªä¿ç•™æœ‰å¼•ç”¨æ”¯æ’çš„å…§å®¹
"""

    llm = _make_langchain_llm(model_name=f"openai:{MODEL_MAIN}", temperature=0.0, reasoning_effort=REASONING_EFFORT)

    agent = create_deep_agent(
        model=llm,
        tools=tools,
        system_prompt=orchestrator_prompt,
        subagents=subagents,
        debug=False,
        name="deep-doc-agent",
    ).with_config({"recursion_limit": int(st.session_state.get("langgraph_recursion_limit", DEFAULT_RECURSION_LIMIT))})

    st.session_state.deep_agent = agent
    st.session_state.deep_agent_web_flag = bool(enable_web)
    return agent


# =========================
# Fallback (RAG)
# =========================
def fallback_answer_from_store(
    client: OpenAI,
    store: Optional[FaissStore],
    question: str,
    *,
    k: int = 10,
) -> str:
    q = (question or "").strip()
    if not q:
        return "ï¼ˆç³»çµ±ï¼šå•é¡Œç‚ºç©ºï¼Œç„¡æ³•ç”¢ç”Ÿå›ç­”ï¼‰"

    if store is None or getattr(store, "index", None) is None or store.index.ntotal == 0:
        system = "ä½ æ˜¯åŠ©ç†ã€‚ç”¨ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰å›ç­”ï¼Œçµæ§‹æ¸…æ¥šã€‚"
        ans, _ = call_gpt(client, model=MODEL_MAIN, system=system, user=q, reasoning_effort=None, tools=None)
        return ans or "ï¼ˆç³»çµ±ï¼šç„¡ç´¢å¼•ä¸”æ¨¡å‹æœªç”¢å‡ºå…§å®¹ï¼‰"

    qvec = embed_texts(client, [q])
    hits = store.search(qvec, k=max(4, min(12, int(k))))
    chunks = [ch for _, ch in hits]
    ctx = render_chunks_for_model(chunks, max_chars_each=900)

    system = (
        "ä½ æ˜¯åš´è¬¹çš„ç ”ç©¶åŠ©ç†ï¼Œåªèƒ½æ ¹æ“šæˆ‘æä¾›çš„è³‡æ–™å›ç­”ï¼Œä¸å¯è…¦è£œã€‚\n"
        "è¼¸å‡ºè¦æ±‚ï¼š\n"
        "1) ç´” bulletï¼Œæ¯è¡Œä»¥ - é–‹é ­ã€‚\n"
        "2) æ¯å€‹ bullet å¥å°¾å¿…é ˆé™„å¼•ç”¨ï¼Œæ ¼å¼å›ºå®šï¼š[å ±å‘Šåç¨± pé ]ã€‚\n"
        "3) å¼•ç”¨ä¸­çš„å ±å‘Šåç¨±å¿…é ˆä¾†è‡ªæˆ‘æä¾›çš„è³‡æ–™ç‰‡æ®µæ¨™é ­ï¼ˆä¾‹å¦‚ [XXX p12]ï¼‰ã€‚\n"
        "4) ä¸å¯ä½¿ç”¨ /evidence/*.md ç•¶ä½œå ±å‘Šåç¨±ã€‚\n"
        "5) è‹¥åŒä¸€å¥éœ€è¦å¤šå€‹å¼•ç”¨ï¼Œè«‹ç”¨å¤šå€‹æ–¹æ‹¬è™Ÿé€£çºŒé™„åœ¨å¥å°¾ï¼Œä¾‹å¦‚ï¼š[A p2][B p8]ã€‚\n"
    )
    user = f"å•é¡Œï¼š{q}\n\nè³‡æ–™ï¼š\n{ctx}\n"

    out, _ = call_gpt(
        client,
        model=MODEL_MAIN,
        system=system,
        user=user,
        reasoning_effort=REASONING_EFFORT,
        tools=None,
        include_sources=False,
    )
    out = (out or "").strip()
    out = strip_internal_process_lines(out)
    return out or "ï¼ˆç³»çµ±ï¼šfallback RAG æœªç”¢å‡ºå…§å®¹ï¼‰"


# =========================
# DeepAgent run + stall detect
# =========================
def deep_agent_run_with_live_status(agent, user_text: str) -> Tuple[str, Optional[dict]]:
    final_state = None
    todos_preview_written = False

    st.session_state["last_run_forced_end"] = None

    recursion_limit = int(st.session_state.get("langgraph_recursion_limit", DEFAULT_RECURSION_LIMIT))
    stall_steps = int(st.session_state.get("citation_stall_steps", DEFAULT_CITATION_STALL_STEPS))
    stall_min_chars = int(st.session_state.get("citation_stall_min_chars", DEFAULT_CITATION_STALL_MIN_CHARS))

    draft_unchanged_streak = 0
    draft_no_citation_streak = 0
    last_draft_hash: Optional[str] = None

    def set_phase(s, phase: str):
        mapping = {
            "start": ("DeepAgentï¼šå•Ÿå‹•ä¸­â€¦", "running"),
            "plan": ("DeepAgentï¼šè¦åŠƒä¸­â€¦", "running"),
            "evidence": ("DeepAgentï¼šè’è­‰ä¸­â€¦", "running"),
            "draft": ("DeepAgentï¼šå¯«ä½œä¸­â€¦", "running"),
            "review": ("DeepAgentï¼šå¯©ç¨¿/è£œå¼•ç”¨ä¸­â€¦", "running"),
            "done": ("DeepAgentï¼šå®Œæˆ", "complete"),
            "error": ("DeepAgentï¼šç™¼ç”ŸéŒ¯èª¤", "error"),
        }
        label, state = mapping.get(phase, ("DeepAgentï¼šåŸ·è¡Œä¸­â€¦", "running"))
        s.update(label=label, state=state, expanded=False)

    with st.status("DeepAgentï¼šå•Ÿå‹•ä¸­â€¦", expanded=False) as s:
        set_phase(s, "start")
        set_phase(s, "plan")

        try:
            for state in agent.stream(
                {"messages": [{"role": "user", "content": user_text}]},
                stream_mode="values",
                config={"recursion_limit": recursion_limit},
            ):
                final_state = state
                files = state.get("files") or {}
                file_keys = set(files.keys()) if isinstance(files, dict) else set()

                if (not todos_preview_written) and isinstance(files, dict) and "/workspace/todos.json" in files:
                    todos_txt = get_files_text(files, "/workspace/todos.json")
                    if todos_txt:
                        s.write("### æœ¬æ¬¡ Todoï¼ˆè¦åŠƒçµæœé è¦½ï¼‰")
                        s.code(todos_txt[:4000], language="json")
                        todos_preview_written = True

                if any(k.startswith("/evidence/") for k in file_keys):
                    set_phase(s, "evidence")
                if "/draft.md" in file_keys:
                    set_phase(s, "draft")
                if "/review.md" in file_keys:
                    set_phase(s, "review")

                # âœ… å¡ä½åˆ¤å®šï¼šdraft å¤ é•·å¾Œæ‰é–‹å§‹ï¼ˆä¸è®Š + ç„¡å¼•ç”¨ï¼‰é€£çºŒ N æ­¥
                if isinstance(files, dict) and "/draft.md" in files:
                    draft_txt = get_files_text(files, "/draft.md")
                    draft_norm = norm_space(draft_txt)
                    if len(draft_norm) >= stall_min_chars:
                        h = _hash_norm_text(draft_norm)
                        if last_draft_hash == h:
                            draft_unchanged_streak += 1
                        else:
                            draft_unchanged_streak = 0
                            last_draft_hash = h

                        if has_visible_citations(draft_norm):
                            draft_no_citation_streak = 0
                        else:
                            draft_no_citation_streak += 1

                        if (draft_unchanged_streak >= stall_steps) and (draft_no_citation_streak >= stall_steps):
                            set_phase(s, "error")
                            st.session_state["last_run_forced_end"] = "citation_stall"
                            s.warning(
                                f"åˆ¤å®šå¡ä½ï¼š/draft.md å…§å®¹é€£çºŒ {draft_unchanged_streak} æ­¥æœªè®Šã€ä¸”é€£çºŒ {draft_no_citation_streak} æ­¥ç„¡å¼•ç”¨ã€‚"
                                "å·²å¼·åˆ¶çµæŸ DeepAgentï¼Œæ”¹ç”¨ fallback ç”¢å‡ºç­”æ¡ˆã€‚"
                            )
                            answer = fallback_answer_from_store(client, st.session_state.get("store", None), user_text, k=10)
                            return answer, files if isinstance(files, dict) and files else None

        except GraphRecursionError:
            set_phase(s, "error")
            st.session_state["last_run_forced_end"] = "recursion_limit"

            files = (final_state or {}).get("files") or {}
            draft = get_files_text(files, "/draft.md") if isinstance(files, dict) else ""
            draft = strip_internal_process_lines(draft)
            if draft.strip():
                s.warning(f"å·²é”æ­¥æ•¸ä¸Šé™ï¼ˆrecursion_limit={recursion_limit}ï¼‰ï¼Œå›å‚³ç›®å‰ /draft.mdã€‚")
                return draft.strip(), (files if isinstance(files, dict) and files else None)

            s.warning(f"å·²é”æ­¥æ•¸ä¸Šé™ï¼ˆrecursion_limit={recursion_limit}ï¼‰ï¼Œæ”¹ç”¨ fallback ç”¢ç”Ÿå›ç­”ã€‚")
            answer = fallback_answer_from_store(client, st.session_state.get("store", None), user_text, k=10)
            return answer, (files if isinstance(files, dict) and files else None)

        except Exception as e:
            msg = str(e)
            if "Budget exceeded" in msg:
                set_phase(s, "evidence")
                s.update(label="DeepAgentï¼šå·²é”å·¥å…·é ç®—ä¸Šé™ï¼ˆåœæ­¢åŠ æœè­‰ï¼‰", state="running", expanded=False)
            else:
                set_phase(s, "error")
                raise

        files = (final_state or {}).get("files") or {}
        final_text = get_files_text(files, "/draft.md")

        if not final_text:
            msgs = (final_state or {}).get("messages") or []
            if msgs:
                last = msgs[-1]
                content = getattr(last, "content", None)
                final_text = (file_to_text(content) or file_to_text(last)).strip()

        if final_text and CHUNK_ID_LEAK_PAT.search(final_text):
            final_text = CHUNK_ID_LEAK_PAT.sub("", final_text)

        final_text = strip_internal_process_lines(final_text)

        set_phase(s, "done")

    return final_text or "ï¼ˆDeepAgent æ²’æœ‰ç”¢å‡ºå…§å®¹ï¼‰", files if isinstance(files, dict) and files else None


# =========================
# need_todo decision
# =========================
def decide_need_todo(client: OpenAI, question: str) -> Tuple[bool, str]:
    system = (
        "ä½ æ˜¯è·¯ç”±å™¨ã€‚è«‹åˆ¤æ–·é€™å€‹å•é¡Œæ˜¯å¦éœ€è¦åšã€Todo + æ–‡ä»¶/ç¶²è·¯æª¢ç´¢ã€ã€‚\n"
        "è¦å‰‡ï¼š\n"
        "- éœ€è¦ï¼šæ¶‰åŠå…·é«”äº‹å¯¦ã€æ•¸æ“šã€æ”¿ç­–ã€ç‰ˆæœ¬ã€å¼•ç”¨ã€ç ”ç©¶ã€æ¯”è¼ƒã€å‡ºè™•ï¼›æˆ–éœ€è¦å¼•ç”¨ä¸Šå‚³æ–‡ä»¶ã€‚\n"
        "- ä¸éœ€è¦ï¼šç´”æ„è¦‹/å¯«ä½œ/æ”¹å¯«/è…¦åŠ›æ¿€ç›ª/ä¸è¦æ±‚å¼•ç”¨çš„æ³›æ³›è§£é‡‹ã€‚\n"
        "è«‹è¼¸å‡º JSONï¼š{\"need_todo\": true/false, \"reason\": \"...\"}ï¼ˆåªè¼¸å‡º JSONï¼‰"
    )
    out, _ = call_gpt(
        client,
        model=MODEL_MAIN,
        system=system,
        user=question,
        reasoning_effort=REASONING_EFFORT,
        tools=None,
        include_sources=False,
    )
    data = _try_parse_json_or_py_literal(out) or {}
    need = bool(data.get("need_todo", False))
    reason = str(data.get("reason", "")).strip() or "ï¼ˆæœªæä¾›åŸå› ï¼‰"
    return need, reason


# =========================
# UI badges
# =========================
def render_run_badges(
    *,
    mode: str,
    need_todo: bool,
    reason: str,
    usage: dict,
    enable_web: bool,
    todo_file_present: Optional[bool] = None,
    forced_end: Optional[str] = None,
) -> None:
    badges: List[str] = []
    badges.append(_badge_directive(f"Mode:{mode}", "gray"))

    # âœ… ä½ è¦æ±‚ï¼šä¸ç®¡æ¨¡å¼éƒ½è¦ Todo åˆ†æï¼ˆæ‰€ä»¥é€™è£¡æ°¸é é¡¯ç¤º Todo:éœ€è¦ï¼‰
    badges.append(_badge_directive("Todo:éœ€è¦", "blue"))

    if todo_file_present is True:
        badges.append(_badge_directive("Todos.json:æœ‰", "blue"))
    elif todo_file_present is False:
        badges.append(_badge_directive("Todos.json:ç„¡(æµç¨‹ç•°å¸¸)", "orange"))

    if forced_end:
        mapping = {
            "citation_stall": "ForcedStop:å¡ä½(å¼•ç”¨æœªç”Ÿæˆ)",
            "recursion_limit": "ForcedStop:æ­¥æ•¸ä¸Šé™",
        }
        label = mapping.get(forced_end, f"ForcedStop:{forced_end}")
        badges.append(_badge_directive(label, "orange"))
        badges.append(_badge_directive("Fallback:RAG", "orange"))

    doc_calls = int((usage or {}).get("doc_search_calls", 0) or 0)
    web_calls = int((usage or {}).get("web_search_calls", 0) or 0)

    badges.append(_badge_directive(f"DB:{'used' if doc_calls else 'unused'}({doc_calls})" if doc_calls else "DB:unused", "green" if doc_calls else "gray"))
    if enable_web:
        badges.append(_badge_directive(f"Web:used({web_calls})" if web_calls else "Web:unused", "violet" if web_calls else "gray"))
    else:
        badges.append(_badge_directive("Web:disabled", "gray"))

    st.markdown(" ".join(badges))


def get_forced_end() -> Optional[str]:
    return st.session_state.get("last_run_forced_end", None)


# =========================
# Debug panel (selectable)
# =========================
def render_debug_panel(files: Optional[dict]) -> None:
    if not files or not isinstance(files, dict):
        st.write("ï¼ˆæ²’æœ‰ filesï¼‰")
        return

    all_keys = sorted([k for k in files.keys() if isinstance(k, str)])
    evidence_keys = [k for k in all_keys if k.startswith("/evidence/")]
    doc_evidence_keys = [k for k in evidence_keys if k.startswith("/evidence/doc_")]
    web_evidence_keys = [k for k in evidence_keys if k.startswith("/evidence/web_")]

    todos = get_files_text(files, "/workspace/todos.json") if "/workspace/todos.json" in files else ""
    readme = get_files_text(files, "/evidence/README.md") if "/evidence/README.md" in files else ""
    draft = get_files_text(files, "/draft.md") if "/draft.md" in files else ""
    review = get_files_text(files, "/review.md") if "/review.md" in files else ""

    tab_overview, tab_orch, tab_retr, tab_web, tab_writer, tab_verifier, tab_browser = st.tabs(
        ["ç¸½è¦½", "Orchestrator", "Retriever(evidence)", "Web(evidence)", "Writer(draft)", "Verifier(review)", "Files browser"]
    )

    with tab_overview:
        st.write(f"files keysï¼š{len(all_keys)}")
        st.write(f"evidenceï¼š{len(evidence_keys)}")
        st.code("\n".join(all_keys[:800]), language="text")

    with tab_orch:
        st.markdown("### /workspace/todos.json")
        st.code((todos or "ï¼ˆç„¡ï¼‰")[:20000], language="json")
        st.divider()
        st.markdown("### /evidence/README.md")
        st.code((readme or "ï¼ˆç„¡ï¼‰")[:20000], language="markdown")

    with tab_retr:
        if not doc_evidence_keys:
            st.write("ï¼ˆæ²’æœ‰ /evidence/doc_*.mdï¼‰")
        else:
            pick = st.selectbox("é¸æ“‡ retriever evidence", doc_evidence_keys, index=0)
            st.code(get_files_text(files, pick)[:60000], language="markdown")

    with tab_web:
        if not web_evidence_keys:
            st.write("ï¼ˆæ²’æœ‰ /evidence/web_*.mdï¼‰")
        else:
            pick = st.selectbox("é¸æ“‡ web evidence", web_evidence_keys, index=0)
            st.code(get_files_text(files, pick)[:60000], language="markdown")

    with tab_writer:
        st.code((draft or "ï¼ˆæ²’æœ‰ /draft.mdï¼‰")[:60000], language="markdown")

    with tab_verifier:
        st.code((review or "ï¼ˆæ²’æœ‰ /review.mdï¼‰")[:60000], language="markdown")

    with tab_browser:
        if not all_keys:
            st.write("ï¼ˆfiles ç‚ºç©ºï¼‰")
        else:
            pick = st.selectbox("é¸æ“‡ä»»ä¸€æª”æ¡ˆï¼ˆfilesï¼‰", all_keys, index=0)
            st.code(get_files_text(files, pick)[:60000], language="text")


# =========================
# Session init
# =========================
OPENAI_API_KEY = get_openai_api_key()
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
os.environ.setdefault("OPENAI_KEY", OPENAI_API_KEY)

client = get_client(OPENAI_API_KEY)

st.session_state.setdefault("file_rows", [])
st.session_state.setdefault("file_bytes", {})
st.session_state.setdefault("store", None)
st.session_state.setdefault("processed_keys", set())
st.session_state.setdefault("default_outputs", None)
st.session_state.setdefault("chat_history", [])

# Popover åªç•™ web é–‹é—œ
st.session_state.setdefault("enable_web_search_agent", False)

# å…¶é¤˜å›ºå®šé è¨­ï¼ˆä¸æä¾› UIï¼‰
st.session_state.setdefault("langgraph_recursion_limit", DEFAULT_RECURSION_LIMIT)
st.session_state.setdefault("citation_stall_steps", DEFAULT_CITATION_STALL_STEPS)
st.session_state.setdefault("citation_stall_min_chars", DEFAULT_CITATION_STALL_MIN_CHARS)
st.session_state.setdefault("last_run_forced_end", None)

st.session_state.setdefault("enable_output_formatter", True)
st.session_state.setdefault("sources_badge_max_titles_inline", DEFAULT_SOURCES_BADGE_MAX_TITLES_INLINE)
st.session_state.setdefault("sources_badge_max_pages_per_title", DEFAULT_SOURCES_BADGE_MAX_PAGES_PER_TITLE)

ENABLE_FORMATTER_FOR_DIRECT = True
ENABLE_FORMATTER_FOR_DEEPAGENT = False

# =========================
# File table helpers
# =========================
def file_rows_to_df(rows: list[FileRow]) -> pd.DataFrame:
    recs = []
    for r in rows:
        if r.ext == ".pdf":
            pages_str = "-" if r.pages is None else str(r.pages)
            text_pages_str = "-" if r.text_pages is None else str(r.text_pages)
            text_ratio_str = "-" if r.text_pages_ratio is None else f"{r.text_pages_ratio:.0%}"
        else:
            pages_str = "-" if r.pages is None else str(r.pages)
            text_pages_str = "-"
            text_ratio_str = "-"

        if r.ext == ".pdf" and r.likely_scanned:
            suggest = "å»ºè­° OCR"
        elif r.ext in (".png", ".jpg", ".jpeg"):
            suggest = "å¿… OCR"
        else:
            suggest = ""

        recs.append({
            "_file_id": r.file_id,
            "ä½¿ç”¨OCR": bool(r.use_ocr),
            "æª”å": truncate_filename(r.name, 52),
            "æ ¼å¼": r.ext.replace(".", ""),
            "é æ•¸": pages_str,
            "æ–‡å­—é ": text_pages_str,
            "æ–‡å­—%": text_ratio_str,
            "tokenä¼°ç®—": int(r.token_est),
            "å»ºè­°": suggest,
        })
    return pd.DataFrame(recs)


def sync_df_to_file_rows(df: pd.DataFrame, rows: list[FileRow]) -> None:
    id_to_row_idx = {r.file_id: i for i, r in enumerate(rows)}
    for _, rec in df.iterrows():
        fid = rec.get("_file_id")
        if fid not in id_to_row_idx:
            continue
        i = id_to_row_idx[fid]

        ext = rows[i].ext
        if ext in (".png", ".jpg", ".jpeg"):
            rows[i].use_ocr = True
        elif ext == ".txt":
            rows[i].use_ocr = False
        else:
            rows[i].use_ocr = bool(rec.get("ä½¿ç”¨OCR", rows[i].use_ocr))


# =========================
# Popoverï¼šæ–‡ä»¶ç®¡ç†ï¼ˆåªç•™ç¶²æœé–‹é—œï¼›å…¶é¤˜ UI ä¸é¡¯ç¤ºï¼‰
# =========================
with st.popover("ğŸ“¦ æ–‡ä»¶ç®¡ç†ï¼ˆä¸Šå‚³ / OCR / å»ºç´¢å¼• / DeepAgentè¨­å®šï¼‰"):
    st.caption("æ”¯æ´ PDF/TXT/PNG/JPGã€‚PDF è‹¥æ–‡å­—æŠ½å–åå°‘æœƒå»ºè­° OCRï¼ˆé€æª”å¯å‹¾é¸ï¼‰ã€‚")
    st.caption("âœ… ä¸ä¸Šå‚³æ–‡ä»¶ä¹Ÿèƒ½èŠå¤©ï¼›åªæœ‰ä½ éœ€è¦å¼•ç”¨æ–‡ä»¶æ™‚æ‰éœ€è¦å»ºç«‹ç´¢å¼•ã€‚")

    has_index = (
        st.session_state.store is not None
        and getattr(st.session_state.store, "index", None) is not None
        and st.session_state.store.index.ntotal > 0
    )
    if has_index:
        st.success(f"å·²å»ºç«‹ç´¢å¼•ï¼šæª”æ¡ˆæ•¸={len(st.session_state.file_rows)} / chunks={len(st.session_state.store.chunks)}")
        st.caption("ä¾†æºä»¥ badge é¡¯ç¤ºï¼ˆæ–‡ä»¶ï¼šæª”å + é ç¢¼ï¼›ç¶²è·¯ï¼šdomain + p-ï¼‰ã€‚")
    else:
        st.info("ç›®å‰æ²’æœ‰ç´¢å¼•ï¼šä½ ä»å¯ç›´æ¥èŠå¤©ï¼ˆç´” LLMï¼‰ã€‚è‹¥éœ€è¦å¼•ç”¨æ–‡ä»¶ï¼Œå†åœ¨æ­¤è™•ä¸Šå‚³ä¸¦å»ºç«‹ç´¢å¼•ã€‚")

    # âœ… åªç•™é€™å€‹
    st.session_state.enable_web_search_agent = st.checkbox(
        "å•Ÿç”¨ç¶²è·¯æœå°‹ï¼ˆæœƒå¢åŠ æˆæœ¬ï¼‰",
        value=bool(st.session_state.enable_web_search_agent),
    )

    uploaded = st.file_uploader(
        "ä¸Šå‚³æ–‡ä»¶",
        type=["pdf", "txt", "png", "jpg", "jpeg"],
        accept_multiple_files=True,
    )

    if uploaded:
        existing = {(r.name, r.bytes_len) for r in st.session_state.file_rows}
        for f in uploaded:
            data = f.read()
            if (f.name, len(data)) in existing:
                continue

            ext = os.path.splitext(f.name)[1].lower()
            fid = str(uuid.uuid4())[:10]
            sig = sha1_bytes(data)
            st.session_state.file_bytes[fid] = data

            pages = None
            extracted_chars = 0
            blank_pages = None
            blank_ratio = None
            text_pages = None
            text_pages_ratio = None

            if ext == ".pdf":
                pdf_pages = extract_pdf_text_pages(data)
                pages = len(pdf_pages)
                extracted_chars, blank_pages, blank_ratio, text_pages, text_pages_ratio = analyze_pdf_text_quality(pdf_pages)
            elif ext == ".txt":
                extracted_chars = len(norm_space(data.decode("utf-8", errors="ignore")))

            token_est = estimate_tokens_from_chars(extracted_chars)
            likely_scanned = should_suggest_ocr(ext, pages, extracted_chars, blank_ratio)

            if ext in (".png", ".jpg", ".jpeg"):
                use_ocr = True
            elif ext == ".txt":
                use_ocr = False
            else:
                use_ocr = bool(likely_scanned)

            st.session_state.file_rows.append(
                FileRow(
                    file_id=fid,
                    file_sig=sig,
                    name=f.name,
                    ext=ext,
                    bytes_len=len(data),
                    pages=pages,
                    extracted_chars=extracted_chars,
                    token_est=token_est,
                    text_pages=text_pages,
                    text_pages_ratio=text_pages_ratio,
                    blank_pages=blank_pages,
                    blank_ratio=blank_ratio,
                    likely_scanned=likely_scanned,
                    use_ocr=use_ocr,
                )
            )

    st.markdown("### æ–‡ä»¶æ¸…å–®ï¼ˆå¯é€æª”å‹¾é¸ OCRï¼‰")

    if not st.session_state.file_rows:
        st.info("å°šæœªä¸Šå‚³æ–‡ä»¶ã€‚")
    else:
        df = file_rows_to_df(st.session_state.file_rows)
        edited = st.data_editor(
            df.drop(columns=["_file_id"]),
            key="file_table_editor",
            width="stretch",
            hide_index=True,
            disabled=["æª”å", "æ ¼å¼", "é æ•¸", "æ–‡å­—é ", "æ–‡å­—%", "tokenä¼°ç®—", "å»ºè­°"],
            column_config={
                "ä½¿ç”¨OCR": st.column_config.CheckboxColumn(
                    "ä½¿ç”¨OCR",
                    help="é€æª”é¸æ“‡æ˜¯å¦å•Ÿç”¨ OCRï¼ˆPDF å¯é¸ï¼›åœ–æª”å›ºå®šOCRï¼›TXTå›ºå®šä¸OCRï¼‰",
                ),
            },
        )

        df_for_sync = df.copy()
        df_for_sync["ä½¿ç”¨OCR"] = edited["ä½¿ç”¨OCR"].values
        sync_df_to_file_rows(df_for_sync, st.session_state.file_rows)

    st.divider()
    col1, col2, col3 = st.columns([1, 1, 1])
    build_btn = col1.button("ğŸš€ å»ºç«‹ç´¢å¼•", type="primary", width="stretch")
    default_btn = col2.button("ğŸ§¾ ç”¢ç”Ÿé è¨­è¼¸å‡º", width="stretch")
    clear_btn = col3.button("ğŸ§¹ æ¸…ç©ºå…¨éƒ¨", width="stretch")

    if clear_btn:
        st.session_state.file_rows = []
        st.session_state.file_bytes = {}
        st.session_state.store = None
        st.session_state.processed_keys = set()
        st.session_state.default_outputs = None
        st.session_state.chat_history = []
        st.session_state.deep_agent = None
        st.session_state.deep_agent_web_flag = None
        st.session_state.da_usage = {"doc_search_calls": 0, "web_search_calls": 0}
        st.session_state["last_run_forced_end"] = None
        st.rerun()

    if build_btn:
        need_ocr = any(r.ext == ".pdf" and r.use_ocr for r in st.session_state.file_rows)
        if need_ocr and not HAS_PYMUPDF:
            st.error("ä½ æœ‰å‹¾é¸ PDF OCRï¼Œä½†ç’°å¢ƒæœªå®‰è£ pymupdfã€‚è«‹å…ˆ pip install pymupdfã€‚")
            st.stop()

        with st.status("å»ºç´¢å¼•ä¸­ï¼ˆOCR + embeddingsï¼‰...", expanded=True) as s:
            t0 = time.perf_counter()
            store, stats, processed_keys = build_indices_incremental_no_kg(
                client,
                st.session_state.file_rows,
                st.session_state.file_bytes,
                st.session_state.store,
                st.session_state.processed_keys,
            )
            st.session_state.store = store
            st.session_state.processed_keys = processed_keys
            s.write(f"æ–°å¢å ±å‘Šæ•¸ï¼š{stats['new_reports']}")
            s.write(f"æ–°å¢ chunksï¼š{stats['new_chunks']}")
            s.write(f"è€—æ™‚ï¼š{time.perf_counter() - t0:.2f}s")
            s.update(state="complete")

        st.session_state.deep_agent = None
        st.session_state.deep_agent_web_flag = None
        st.rerun()

    if default_btn:
        if st.session_state.store is None or st.session_state.store.index.ntotal == 0:
            st.warning("å°šæœªå»ºç«‹ç´¢å¼•æˆ–æ²’æœ‰ chunksï¼Œè«‹å…ˆæŒ‰ã€Œå»ºç«‹ç´¢å¼•ã€ã€‚")
        else:
            with st.status("ç”¢ç”Ÿé è¨­è¼¸å‡ºï¼ˆæ‘˜è¦/ä¸»å¼µ/æ¨è«–éˆï¼‰...", expanded=True) as s2:
                chosen = pick_corpus_chunks_for_default(st.session_state.store.chunks)
                ctx = render_chunks_for_model(chosen)
                bundle = generate_default_outputs_bundle(client, "æ•´é«”èåˆï¼ˆå…¨éƒ¨ä¸Šå‚³å ±å‘Šï¼‰", ctx, max_retries=2)
                st.session_state.default_outputs = bundle
                s2.update(state="complete")

            st.session_state.chat_history.append({
                "role": "assistant",
                "kind": "default",
                "title": "æ•´é«”èåˆï¼ˆå…¨éƒ¨ä¸Šå‚³å ±å‘Šï¼‰",
                **(st.session_state.default_outputs or {}),
            })
            st.rerun()


# =========================
# ä¸»ç•«é¢ï¼šChat
# =========================
has_index = (
    st.session_state.store is not None
    and getattr(st.session_state.store, "index", None) is not None
    and st.session_state.store.index.ntotal > 0
)

st.divider()
st.subheader("Chatï¼ˆDeepAgent + Sources Badges + Todo decisionï¼‰")


# é¡¯ç¤ºæ­·å²ï¼ˆâœ… user ä¸é¡¯ç¤º badgeï¼‰
for msg in st.session_state.chat_history:
    role = msg.get("role", "assistant")
    with st.chat_message(role):
        if role == "user":
            st.markdown(msg.get("content", ""))
            continue

        if msg.get("kind") == "default":
            st.markdown(f"## é è¨­è¼¸å‡ºï¼š{msg.get('title','')}")
            st.markdown("### 1) å ±å‘Šæ‘˜è¦")
            st.code((msg.get("summary", "") or "")[:20000], language="markdown")
            st.markdown("### 2) æ ¸å¿ƒä¸»å¼µ")
            st.code((msg.get("claims", "") or "")[:20000], language="markdown")
            st.markdown("### 3) æ¨è«–éˆ")
            st.code((msg.get("chain", "") or "")[:20000], language="markdown")
        else:
            meta = msg.get("meta", {}) or {}
            render_run_badges(
                mode=meta.get("mode", "unknown"),
                need_todo=True,
                reason=str(meta.get("reason", "") or ""),
                usage=meta.get("usage", {}) or {},
                enable_web=bool(meta.get("enable_web", False)),
                todo_file_present=meta.get("todo_file_present", None),
                forced_end=meta.get("forced_end", None),
            )
            render_markdown_answer_with_sources_badges(msg.get("content", ""))
            render_web_sources_list(meta.get("web_sources", {}) or {})


prompt = st.chat_input("è«‹è¼¸å…¥å•é¡Œï¼ˆä¹Ÿå¯è²¼è‰ç¨¿è¦æˆ‘æŸ¥æ ¸/é™¤éŒ¯ï¼‰ã€‚")
if prompt:
    st.session_state.chat_history.append({"role": "user", "kind": "text", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        enable_web = bool(st.session_state.enable_web_search_agent)

        # âœ… ä¸ç®¡ direct/deepagentï¼Œéƒ½åš todo åˆ¤æ–·ï¼ˆä½ è¦æ±‚ï¼‰
        need_todo, reason = decide_need_todo(client, prompt)

        planned_mode = "deepagent" if (has_index and need_todo) else "direct"
        todos_json_text = build_todos_json_for_question(
            client,
            prompt,
            enable_web=enable_web,
            has_index=has_index,
            planned_mode=planned_mode,
        )

        # direct
        if planned_mode == "direct":
            system = "ä½ æ˜¯åŠ©ç†ã€‚ç”¨ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰å›ç­”ï¼Œçµæ§‹æ¸…æ¥šã€‚"
            answer_text, _ = call_gpt(
                client,
                model=MODEL_MAIN,
                system=system,
                user=prompt,
                reasoning_effort=None,
                tools=None,
            )

            # formatter + å»å…§éƒ¨æµç¨‹
            if ENABLE_FORMATTER_FOR_DIRECT and st.session_state.get("enable_output_formatter", True):
                answer_text = format_markdown_output_preserve_citations(client, answer_text)
            answer_text = strip_internal_process_lines(answer_text)

            # âœ… direct ä¹Ÿé€ å‡º filesï¼ˆä¾› debug / badgesï¼‰
            files = {"/workspace/todos.json": todos_json_text}

            meta = {
                "mode": "direct",
                "need_todo": True,
                "reason": reason,
                "usage": {"doc_search_calls": 0, "web_search_calls": 0},
                "enable_web": enable_web,
                "todo_file_present": True,
                "forced_end": None,
            }

            render_run_badges(
                mode=meta["mode"],
                need_todo=True,
                reason=reason,
                usage=meta["usage"],
                enable_web=enable_web,
                todo_file_present=True,
                forced_end=None,
            )
            render_markdown_answer_with_sources_badges(answer_text)

            with st.expander("Debug", expanded=False):
                st.markdown("### æœ¬æ¬¡ Todoï¼ˆdirect ç”¢ç”Ÿï¼‰")
                st.code(todos_json_text[:20000], language="json")

            st.session_state.chat_history.append({"role": "assistant", "kind": "text", "content": answer_text, "meta": meta})
            st.stop()

        # deepagent
        agent = ensure_deep_agent(client=client, store=st.session_state.store, enable_web=enable_web)
        answer_text, files = deep_agent_run_with_live_status(agent, prompt)

        if ENABLE_FORMATTER_FOR_DEEPAGENT and st.session_state.get("enable_output_formatter", True):
            answer_text = format_markdown_output_preserve_citations(client, answer_text)
        answer_text = strip_internal_process_lines(answer_text)

        # deepagentï¼šè‹¥æ²’ç”¢ todos.jsonï¼Œç”¨ direct è¨ˆç•«è£œä¸Šï¼ˆä¿è­‰æœ‰ï¼‰
        if not isinstance(files, dict):
            files = {}
        if "/workspace/todos.json" not in files:
            files["/workspace/todos.json"] = todos_json_text

        todo_file_present = isinstance(files, dict) and ("/workspace/todos.json" in files)

        meta = {
            "mode": "deepagent",
            "need_todo": True,
            "reason": reason,
            "usage": dict(st.session_state.get("da_usage", {"doc_search_calls": 0, "web_search_calls": 0})),
            "enable_web": enable_web,
            "todo_file_present": bool(todo_file_present),
            "forced_end": get_forced_end(),
        }

        render_run_badges(
            mode=meta["mode"],
            need_todo=True,
            reason=reason,
            usage=meta["usage"],
            enable_web=enable_web,
            todo_file_present=meta["todo_file_present"],
            forced_end=meta.get("forced_end"),
        )
        render_markdown_answer_with_sources_badges(answer_text)

        with st.expander("Debug", expanded=False):
            todos_txt = get_files_text(files, "/workspace/todos.json") if isinstance(files, dict) else ""
            if todos_txt:
                st.markdown("### æœ¬æ¬¡ Todoï¼ˆå®Œæ•´ï¼‰")
                st.code(todos_txt[:20000], language="json")
                st.divider()
            render_debug_panel(files)

    st.session_state.chat_history.append({"role": "assistant", "kind": "text", "content": answer_text, "meta": meta})
