# pages/deep_agents.py
# -*- coding: utf-8 -*-
from __future__ import annotations

import os
import re
import io
import uuid
import math
import time
import json
import base64
import hashlib
import threading
import ast
from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple, List
from concurrent.futures import ThreadPoolExecutor, as_completed

import streamlit as st
import numpy as np
import pandas as pd
import faiss
from pypdf import PdfReader

from openai import OpenAI

try:
    import fitz  # pymupdf
    HAS_PYMUPDF = True
except Exception:
    HAS_PYMUPDF = False


# =========================
# Streamlit configï¼ˆåªå‘¼å«ä¸€æ¬¡ï¼‰
# =========================
st.set_page_config(page_title="ç ”ç©¶å ±å‘ŠåŠ©æ‰‹ï¼ˆDeepAgent + Badgesï¼‰", layout="wide")
st.title("ç ”ç©¶å ±å‘ŠåŠ©æ‰‹ï¼ˆDeepAgent + Badgesï¼‰")

# âœ… Markdown é¡¯ç¤ºå¾®èª¿ï¼šå­—é«”/è¡Œè·/æ¨™é¡Œå¤§å°/åˆ—è¡¨é–“è·
st.markdown(
    """
<style>
/* è®“æ–‡ç« å¯è®€æ€§æ›´å¥½ï¼ˆä½ è¦ºå¾—å…§å®¹ OKï¼Œä½†å‘ˆç¾éœ€è¦èª¿æ•´ï¼‰ */
.block-container { padding-top: 1.5rem; padding-bottom: 3rem; max-width: 1200px; }
.stMarkdown { line-height: 1.7; }
.stMarkdown h1 { font-size: 1.8rem; margin: 0.8rem 0 0.7rem; }
.stMarkdown h2 { font-size: 1.35rem; margin: 0.9rem 0 0.55rem; }
.stMarkdown h3 { font-size: 1.15rem; margin: 0.8rem 0 0.45rem; }
.stMarkdown p { margin: 0.35rem 0 0.6rem; }
.stMarkdown ul, .stMarkdown ol { margin: 0.35rem 0 0.75rem 1.2rem; }
.stMarkdown li { margin: 0.15rem 0; }
.stMarkdown blockquote { padding: 0.4rem 0.8rem; border-left: 0.25rem solid rgba(60, 60, 60, 0.25); }
</style>
""",
    unsafe_allow_html=True,
)


# =========================
# DeepAgents / LangChain importsï¼ˆå¯è¨ºæ–·ç‰ˆï¼‰
# =========================
HAS_DEEPAGENTS = False
DEEPAGENTS_IMPORT_ERRORS: list[str] = []

create_deep_agent = None
init_chat_model = None
ChatOpenAI = None

try:
    from deepagents import create_deep_agent as _create_deep_agent
    create_deep_agent = _create_deep_agent
except Exception as e:
    DEEPAGENTS_IMPORT_ERRORS.append(f"deepagents import failed: {repr(e)}")

try:
    from langchain.chat_models import init_chat_model as _init_chat_model
    init_chat_model = _init_chat_model
except Exception as e:
    DEEPAGENTS_IMPORT_ERRORS.append(f"langchain.chat_models.init_chat_model import failed: {repr(e)}")

try:
    from langchain_openai import ChatOpenAI as _ChatOpenAI
    ChatOpenAI = _ChatOpenAI
except Exception as e:
    DEEPAGENTS_IMPORT_ERRORS.append(f"langchain_openai.ChatOpenAI import failed: {repr(e)}")

HAS_DEEPAGENTS = (create_deep_agent is not None) and ((init_chat_model is not None) or (ChatOpenAI is not None))


def _require_deepagents():
    if HAS_DEEPAGENTS:
        return
    st.error("DeepAgent ä¾è³´è¼‰å…¥å¤±æ•—ï¼ˆä¸ä¸€å®šæ˜¯æ²’å®‰è£ï¼Œå¯èƒ½æ˜¯ç‰ˆæœ¬/ä¾è³´ä¸ç›¸å®¹ï¼‰ã€‚")
    if DEEPAGENTS_IMPORT_ERRORS:
        st.markdown("### ä¾è³´éŒ¯èª¤ç´°ç¯€ï¼ˆè«‹æŠŠé€™æ®µè²¼çµ¦æˆ‘ï¼Œæˆ‘å°±èƒ½ç²¾æº–æŒ‡ä½ è©²è£å“ªå€‹ç‰ˆæœ¬ï¼‰")
        for msg in DEEPAGENTS_IMPORT_ERRORS:
            st.code(msg)
    else:
        st.info("ï¼ˆæ²’æœ‰æ•æ‰åˆ°éŒ¯èª¤ç´°ç¯€ï¼Œè«‹ç¢ºèª deep_agents.py æ˜¯å¦å·²æ•´æª”è¦†è“‹ç‚ºæœ€æ–°ç‰ˆï¼‰")
    st.stop()


def _make_langchain_llm(model_name: str, temperature: float = 0.0, reasoning_effort: Optional[str] = None):
    """
    å›å‚³ LangChain çš„ chat model instanceï¼š
    - å„ªå…ˆ init_chat_model
    - fallback ChatOpenAI

    ä¾ä½ éœ€æ±‚ï¼š
    - æ¨ç†éœ€æ±‚é«˜ï¼šæ‰åŠ  reasoning={"effort":"medium"}
    - å…¶é¤˜ï¼šä¸è¨­å®š reasoning
    """
    if init_chat_model is not None:
        if model_name.startswith("openai:"):
            return init_chat_model(model=model_name, temperature=temperature)
        return init_chat_model(model=f"openai:{model_name}", temperature=temperature)

    if ChatOpenAI is not None:
        if model_name.startswith("openai:"):
            model_name = model_name.split("openai:", 1)[1]
        kwargs = dict(
            model=model_name,
            temperature=temperature,
            use_responses_api=True,
            max_completion_tokens=None,
        )
        if reasoning_effort in ("low", "medium", "high"):
            kwargs["reasoning"] = {"effort": reasoning_effort}
        return ChatOpenAI(**kwargs)

    raise RuntimeError("No LangChain LLM factory available.")


# =========================
# å›ºå®šæ¨¡å‹è¨­å®šï¼ˆä¾ä½ è¦æ±‚ï¼šéƒ½ç”¨ gpt-5.2ï¼‰
# =========================
EMBEDDING_MODEL = "text-embedding-3-small"

MODEL_MAIN = "gpt-5.2"
MODEL_GRADER = "gpt-5.2"
MODEL_WEB = "gpt-5.2"

REASONING_EFFORT = "medium"  # âœ… æ¨ç†éœ€æ±‚é«˜æ‰ä½¿ç”¨


# =========================
# æ•ˆèƒ½åƒæ•¸
# =========================
EMBED_BATCH_SIZE = 256
OCR_MAX_WORKERS = 2

CORPUS_DEFAULT_MAX_CHUNKS = 24
CORPUS_PER_REPORT_QUOTA = 6

# DeepAgent budgets
DA_MAX_DOC_SEARCH_CALLS = 14
DA_MAX_WEB_SEARCH_CALLS = 4
DA_MAX_REWRITE_ROUNDS = 2
DA_MAX_CLAIMS = 10

CHUNK_ID_LEAK_PAT = re.compile(r"(chunk_id\s*=\s*|_p(?:na|\d+)_c\d+)", re.IGNORECASE)

# âœ… é‡è¦ï¼šä½ çš„å…§éƒ¨ evidence æª”åä¸æ‡‰å‡ºç¾åœ¨å¼•ç”¨ badge
EVIDENCE_PATH_IN_CIT_RE = re.compile(r"\[(?:/)?evidence/[^ \]]+?\s+p(\d+|-)\s*\]", re.IGNORECASE)


# =========================
# å°å·¥å…·
# =========================
def norm_space(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())


def estimate_tokens_from_chars(n_chars: int) -> int:
    if n_chars <= 0:
        return 0
    return max(1, int(math.ceil(n_chars / 3.6)))


def chunk_text(text: str, chunk_size: int = 900, overlap: int = 150) -> list[str]:
    text = norm_space(text)
    if not text:
        return []
    out = []
    i = 0
    while i < len(text):
        j = min(len(text), i + chunk_size)
        out.append(text[i:j])
        if j == len(text):
            break
        i = max(0, j - overlap)
    return out


def sha1_bytes(data: bytes) -> str:
    return hashlib.sha1(data).hexdigest()


def truncate_filename(name: str, max_len: int = 44) -> str:
    if len(name) <= max_len:
        return name
    base, ext = os.path.splitext(name)
    keep = max(10, max_len - len(ext) - 1)
    return f"{base[:keep]}â€¦{ext}"


def _dedup_keep_order(items: list[str]) -> list[str]:
    seen = set()
    out = []
    for x in items:
        if x in seen:
            continue
        seen.add(x)
        out.append(x)
    return out


# =========================
# OpenAI client + wrappers
# =========================
def get_openai_api_key() -> str:
    if "OPENAI_KEY" in st.secrets and st.secrets["OPENAI_KEY"]:
        return st.secrets["OPENAI_KEY"]
    if os.environ.get("OPENAI_API_KEY"):
        return os.environ["OPENAI_API_KEY"]
    if os.environ.get("OPENAI_KEY"):
        return os.environ["OPENAI_KEY"]
    raise RuntimeError("Missing OpenAI API key. Set st.secrets['OPENAI_KEY'] or env OPENAI_API_KEY.")


def get_client(api_key: str) -> OpenAI:
    return OpenAI(api_key=api_key)


def embed_texts(client: OpenAI, texts: list[str]) -> np.ndarray:
    resp = client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=texts,
        encoding_format="float",
    )
    vecs = np.array([d.embedding for d in resp.data], dtype=np.float32)
    norms = np.linalg.norm(vecs, axis=1, keepdims=True)
    norms = np.where(norms == 0, 1.0, norms)
    return vecs / norms


def _to_messages(system: str, user: Any) -> list[Dict[str, Any]]:
    return [{"role": "system", "content": system}, {"role": "user", "content": user}]


def call_gpt(
    client: OpenAI,
    *,
    model: str,
    system: str,
    user: Any,
    reasoning_effort: Optional[str] = None,
    tools: Optional[list] = None,
    include_sources: bool = False,
) -> Tuple[str, Optional[list[Dict[str, Any]]]]:
    messages = _to_messages(system, user)
    resp = client.responses.create(
        model=model,
        input=messages,
        tools=tools,
        tool_choice="auto" if tools else "none",
        parallel_tool_calls=True if tools else None,
        reasoning={"effort": reasoning_effort} if reasoning_effort in ("low", "medium", "high") else None,
        include=["web_search_call.action.sources"] if (tools and include_sources) else None,
        truncation="auto",
    )
    out_text = resp.output_text
    sources = None
    if tools and include_sources:
        try:
            sources_list = []
            if hasattr(resp, "output") and resp.output:
                for item in resp.output:
                    d = item if isinstance(item, dict) else getattr(item, "__dict__", {})
                    if isinstance(d, dict) and d.get("type") == "web_search_call":
                        action = d.get("action", {}) or {}
                        if isinstance(action, dict) and action.get("sources"):
                            sources_list.extend(action["sources"])
            sources = sources_list if sources_list else None
        except Exception:
            sources = None
    return out_text, sources


# =========================
# æª”æ¡ˆ / OCR
# =========================
@dataclass
class FileRow:
    file_id: str
    file_sig: str
    name: str
    ext: str
    bytes_len: int
    pages: Optional[int]
    extracted_chars: int
    token_est: int

    text_pages: Optional[int]
    text_pages_ratio: Optional[float]

    blank_pages: Optional[int]
    blank_ratio: Optional[float]
    likely_scanned: bool
    use_ocr: bool


def extract_pdf_text_pages_pypdf(pdf_bytes: bytes) -> list[Tuple[int, str]]:
    reader = PdfReader(io.BytesIO(pdf_bytes))
    out = []
    for i, p in enumerate(reader.pages):
        try:
            t = p.extract_text() or ""
        except Exception:
            t = ""
        out.append((i + 1, norm_space(t)))
    return out


def extract_pdf_text_pages_pymupdf(pdf_bytes: bytes) -> list[Tuple[int, str]]:
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    out = []
    for i in range(doc.page_count):
        page = doc.load_page(i)
        t = page.get_text("text") or ""
        out.append((i + 1, norm_space(t)))
    return out


def extract_pdf_text_pages(pdf_bytes: bytes) -> list[Tuple[int, str]]:
    if HAS_PYMUPDF:
        try:
            return extract_pdf_text_pages_pymupdf(pdf_bytes)
        except Exception:
            return extract_pdf_text_pages_pypdf(pdf_bytes)
    return extract_pdf_text_pages_pypdf(pdf_bytes)


def analyze_pdf_text_quality(
    pdf_pages: list[Tuple[int, str]],
    min_chars_per_page: int = 40,
) -> Tuple[int, int, float, int, float]:
    if not pdf_pages:
        return 0, 0, 1.0, 0, 0.0
    lens = [len(t) for _, t in pdf_pages]
    blank = sum(1 for L in lens if L <= min_chars_per_page)
    total_pages = max(1, len(lens))
    blank_ratio = blank / total_pages
    text_pages = total_pages - blank
    text_pages_ratio = text_pages / total_pages
    return sum(lens), blank, blank_ratio, text_pages, text_pages_ratio


def should_suggest_ocr(ext: str, pages: Optional[int], extracted_chars: int, blank_ratio: Optional[float]) -> bool:
    if ext != ".pdf":
        return False
    if pages is None or pages <= 0:
        return True
    if blank_ratio is not None and blank_ratio >= 0.6:
        return True
    avg = extracted_chars / max(1, pages)
    return avg < 120


def _img_bytes_to_data_url(img_bytes: bytes, mime: str = "image/png") -> str:
    b64 = base64.b64encode(img_bytes).decode("utf-8")
    return f"data:{mime};base64,{b64}"


def ocr_image_bytes(client: OpenAI, image_bytes: bytes, mime: str = "image/png") -> str:
    system = "ä½ æ˜¯ä¸€å€‹OCRå·¥å…·ã€‚åªè¼¸å‡ºå¯è¦‹æ–‡å­—èˆ‡è¡¨æ ¼å…§å®¹ï¼ˆè‹¥æœ‰è¡¨æ ¼ç”¨ Markdown è¡¨æ ¼ï¼‰ã€‚ä¸­æ–‡è«‹ç”¨ç¹é«”ä¸­æ–‡ã€‚ä¸è¦åŠ è©•è«–ã€‚"
    user_content = [
        {"type": "input_text", "text": "è«‹æ“·å–åœ–ç‰‡ä¸­æ‰€æœ‰å¯è¦‹æ–‡å­—ï¼ˆåŒ…å«å°å­—/è¨»è…³ï¼‰ã€‚è‹¥ç„¡æ³•è¾¨è­˜è«‹æ¨™è¨˜[ç„¡æ³•è¾¨è­˜]ã€‚"},
        {"type": "input_image", "image_url": _img_bytes_to_data_url(image_bytes, mime=mime)},
    ]
    text, _ = call_gpt(client, model=MODEL_GRADER, system=system, user=user_content, reasoning_effort=None)
    return text


def ocr_pdf_pages_parallel(client: OpenAI, pdf_bytes: bytes, dpi: int = 180) -> list[Tuple[int, str]]:
    if not HAS_PYMUPDF:
        raise RuntimeError("æœªå®‰è£ pymupdfï¼ˆfitzï¼‰ï¼Œç„¡æ³•åš PDF OCRã€‚è«‹ pip install pymupdf")

    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    zoom = dpi / 72.0
    mat = fitz.Matrix(zoom, zoom)

    def render_page(i: int) -> Tuple[int, bytes]:
        page = doc.load_page(i)
        pix = page.get_pixmap(matrix=mat, alpha=False)
        return i + 1, pix.tobytes("png")

    page_imgs = [render_page(i) for i in range(doc.page_count)]
    results: Dict[int, str] = {}

    with ThreadPoolExecutor(max_workers=OCR_MAX_WORKERS) as ex:
        futs = {ex.submit(ocr_image_bytes, client, img_bytes, "image/png"): page_no for page_no, img_bytes in page_imgs}
        for fut in as_completed(futs):
            page_no = futs[fut]
            try:
                results[page_no] = norm_space(fut.result())
            except Exception:
                results[page_no] = ""

    return [(p, results.get(p, "")) for p, _ in page_imgs]


# =========================
# FAISS store
# =========================
@dataclass
class Chunk:
    chunk_id: str
    report_id: str
    title: str
    page: Optional[int]
    text: str


class FaissStore:
    def __init__(self, dim: int):
        self.index = faiss.IndexFlatIP(dim)
        self.chunks: list[Chunk] = []

    def add(self, vecs: np.ndarray, chunks: list[Chunk]) -> None:
        self.index.add(vecs)
        self.chunks.extend(chunks)

    def search(self, qvec: np.ndarray, k: int = 8) -> list[Tuple[float, Chunk]]:
        if self.index.ntotal == 0:
            return []
        scores, idx = self.index.search(qvec.astype(np.float32), k)
        out = []
        for s, i in zip(scores[0], idx[0]):
            if i < 0 or i >= len(self.chunks):
                continue
            out.append((float(s), self.chunks[i]))
        return out


# =========================
# badges / citations / file-to-text
# =========================
CIT_RE = re.compile(r"\[[^\]]+?\s+p(\d+|-)\s*\]")
BULLET_RE = re.compile(r"^\s*(?:[-â€¢*]|\d+\.)\s+")
CIT_PARSE_RE = re.compile(r"\[([^\]]+?)\s+p(\d+|-)\s*\]")


def file_to_text(file_obj: Any) -> str:
    """
    âœ… ä¿®ä½ æœ€é—œéµçš„é»ï¼šå¾ dict(content=[...]) å–å‡ºçœŸæ­£ markdown æ–‡å­—
    """
    if file_obj is None:
        return ""

    if isinstance(file_obj, dict):
        if "data" in file_obj:
            return file_to_text(file_obj.get("data"))
        if "content" in file_obj:
            return file_to_text(file_obj.get("content"))
        for k in ("text", "answer", "final", "output", "message"):
            if k in file_obj:
                return file_to_text(file_obj.get(k))
        try:
            return json.dumps(file_obj, ensure_ascii=False, indent=2)
        except Exception:
            return str(file_obj)

    if isinstance(file_obj, (bytes, bytearray)):
        return file_obj.decode("utf-8", errors="ignore")

    if isinstance(file_obj, str):
        return file_obj

    if isinstance(file_obj, (list, tuple)):
        parts: list[str] = []
        for x in file_obj:
            t = file_to_text(x).strip()
            if t:
                parts.append(t)
        return "\n".join(parts)

    return str(file_obj)


def get_files_text(files: Optional[dict], key: str) -> str:
    if not isinstance(files, dict) or key not in files:
        return ""
    return file_to_text(files.get(key)).strip()


def _parse_citations(cits: list[str]) -> list[Dict[str, str]]:
    parsed = []
    for c in cits:
        m = CIT_PARSE_RE.search(c)
        if m:
            parsed.append({"title": m.group(1).strip(), "page": m.group(2).strip()})
    return parsed


def _badge_directive(label: str, color: str) -> str:
    safe = label.replace("[", "(").replace("]", ")")
    return f":{color}-badge[{safe}]"


def _dedup_keep_order(items: list[str]) -> list[str]:
    seen = set()
    out = []
    for x in items:
        if x in seen:
            continue
        seen.add(x)
        out.append(x)
    return out


def _try_parse_json_or_py_literal(text: str) -> Optional[Any]:
    t = (text or "").strip()
    if not t:
        return None
    if t.startswith("{") or t.startswith("["):
        try:
            return json.loads(t)
        except Exception:
            pass
    if t.startswith("{") and t.endswith("}"):
        try:
            return ast.literal_eval(t)
        except Exception:
            return None
    return None


def _extract_main_text_from_payload(payload: Any) -> Optional[str]:
    if isinstance(payload, dict):
        for k in ("content", "answer", "final", "output", "text", "message"):
            if k not in payload:
                continue
            v = payload.get(k)
            if isinstance(v, str) and v.strip():
                return v
            if isinstance(v, (list, tuple)):
                joined = file_to_text(v).strip()
                if joined:
                    return joined

        msgs = payload.get("messages")
        if isinstance(msgs, list) and msgs:
            last = msgs[-1]
            if isinstance(last, dict):
                c = last.get("content")
                if isinstance(c, (str, list, tuple, dict)):
                    out = file_to_text(c).strip()
                    if out:
                        return out
            out = file_to_text(last).strip()
            return out or None

        return None

    if isinstance(payload, list):
        out = file_to_text(payload).strip()
        return out or None

    return None


def _extract_citation_strings(text: str) -> list[str]:
    if not text:
        return []
    return [m.group(0) for m in re.finditer(r"\[[^\]]+?\s+p(\d+|-)\s*\]", text)]


def _strip_citations_from_text(text: str) -> str:
    if not text:
        return ""
    return re.sub(r"\s*\[[^\]]+?\s+p(\d+|-)\s*\]\s*", " ", text).strip()


def _group_citations_for_badges(cits: list[str]) -> dict[str, list[str]]:
    grouped: dict[str, list[str]] = {}
    for c in cits:
        m = CIT_PARSE_RE.search(c)
        if not m:
            continue
        title = m.group(1).strip()
        page = m.group(2).strip()
        grouped.setdefault(title, []).append(page)

    for title, pages in grouped.items():
        pages = _dedup_keep_order(pages)

        def _key(p: str):
            if p.isdigit():
                return (0, int(p))
            if p == "-":
                return (1, 10**9)
            return (2, p)

        grouped[title] = sorted(pages, key=_key)

    return grouped


def render_markdown_answer_with_source_badges(answer_text: str, badge_color: str = "green"):
    """
    - æ­£æ–‡ä¸é¡¯ç¤ºå¼•ç”¨
    - ä¾†æºç”¨ badge é¡¯ç¤ºã€Œè³‡æ–™æª”å + é ç¢¼ã€
    - âœ… è‡ªå‹•å¿½ç•¥ /evidence/*.md é€™ç¨®å…§éƒ¨è·¯å¾‘å¼•ç”¨ï¼ˆä½ èªªä¸è©²é¡¯ç¤ºï¼‰
    """
    raw = (answer_text or "").strip()

    if raw and CHUNK_ID_LEAK_PAT.search(raw):
        raw = CHUNK_ID_LEAK_PAT.sub("", raw)

    payload = _try_parse_json_or_py_literal(raw)
    if payload is not None:
        extracted = _extract_main_text_from_payload(payload)
        if extracted is not None:
            raw = extracted.strip()

    cits = _dedup_keep_order(_extract_citation_strings(raw))
    # âœ… æ¿¾æ‰ evidence è·¯å¾‘å¼•ç”¨
    cits = [c for c in cits if not EVIDENCE_PATH_IN_CIT_RE.search(c)]

    clean = _strip_citations_from_text(raw)
    st.markdown(clean if clean else "ï¼ˆç„¡å…§å®¹ï¼‰")

    if not cits:
        return

    grouped = _group_citations_for_badges(cits)

    doc_badges: list[str] = []
    web_badges: list[str] = []

    def _pages_str(pages: list[str], max_keep: int = 6) -> str:
        pages = pages or ["-"]
        if len(pages) <= max_keep:
            return "p" + ",".join(pages)
        kept = pages[:max_keep]
        return "p" + ",".join(kept) + "â€¦"

    for title in sorted(grouped.keys()):
        pages = grouped[title]
        pages_part = _pages_str(pages, max_keep=6)

        is_web = title.strip().lower().startswith("websearch:")
        label = f"{title} {pages_part}"

        if is_web:
            web_badges.append(_badge_directive(label, "violet"))
        else:
            doc_badges.append(_badge_directive(label, badge_color))

    badges_line = []
    if doc_badges:
        badges_line.append(" ".join(doc_badges))
    if web_badges:
        badges_line.append(" ".join(web_badges))

    if badges_line:
        st.markdown(" ".join(badges_line))


def render_bullets_inline_badges(md_bullets: str, badge_color: str = "green"):
    lines = [l.rstrip() for l in (md_bullets or "").splitlines() if l.strip()]
    for line in lines:
        if not BULLET_RE.match(line):
            continue
        full_cits = [m.group(0) for m in re.finditer(r"\[[^\]]+?\s+p(\d+|-)\s*\]", line)]
        full_cits = [c for c in full_cits if not EVIDENCE_PATH_IN_CIT_RE.search(c)]
        clean = re.sub(r"\[[^\]]+?\s+p(\d+|-)\s*\]", "", line).strip()
        parsed = _parse_citations(full_cits)
        badges = [_badge_directive(f"{it['title']} p{it['page']}", badge_color) for it in parsed]
        st.markdown(clean + (" " + " ".join(badges) if badges else ""))


def bullets_all_have_citations(md: str) -> bool:
    lines = (md or "").splitlines()
    if not any(BULLET_RE.match(l) for l in lines):
        return False
    for line in lines:
        if BULLET_RE.match(line):
            # âœ… ä¸€æ¨£å¿½ç•¥ /evidence/*.md é€™ç¨®ä¸ç®—æœ‰æ•ˆå¼•ç”¨
            cits = [m.group(0) for m in re.finditer(r"\[[^\]]+?\s+p(\d+|-)\s*\]", line)]
            cits = [c for c in cits if not EVIDENCE_PATH_IN_CIT_RE.search(c)]
            if not cits:
                return False
    return True


# =========================
# Debug panelï¼ˆä¸€å®šè¦å…ˆå®šç¾©ï¼Œé¿å… NameErrorï¼‰
# =========================
def render_debug_panel(files: Optional[dict]):
    """
    âœ… Tabs å…¨åŒ…åœ¨é€™è£¡ï¼Œä¸¦ä¸”åªæœƒåœ¨ä¸»ç¨‹å¼çš„ Debug expander å…§å‘¼å«
    """
    if not files or not isinstance(files, dict):
        st.write("ï¼ˆæ²’æœ‰ filesï¼‰")
        return

    all_keys = sorted([k for k in files.keys() if isinstance(k, str)])
    evidence_keys = [k for k in all_keys if k.startswith("/evidence/")]

    draft = get_files_text(files, "/draft.md") if "/draft.md" in files else ""
    review = get_files_text(files, "/review.md") if "/review.md" in files else ""
    todos = get_files_text(files, "/workspace/todos.json") if "/workspace/todos.json" in files else ""

    tab1, tab2, tab3, tab4, tab5 = st.tabs(["ç¸½è¦½", "todos.json", "draft.md", "review.md", "evidence"])

    with tab1:
        st.write(f"files keysï¼š{len(all_keys)}")
        st.write(f"evidenceï¼š{len(evidence_keys)}")
        st.code("\n".join(all_keys[:500]), language="text")

    with tab2:
        if todos:
            st.code(todos[:20000], language="json")
        else:
            st.write("ï¼ˆæ²’æœ‰ /workspace/todos.jsonï¼‰")

    with tab3:
        if draft:
            st.code(draft[:20000], language="markdown")
        else:
            st.write("ï¼ˆæ²’æœ‰ /draft.mdï¼‰")

    with tab4:
        if review:
            st.code(review[:20000], language="markdown")
        else:
            st.write("ï¼ˆæ²’æœ‰ /review.mdï¼‰")

    with tab5:
        if evidence_keys:
            # é¡¯ç¤º evidence æª”åæ¸…å–®ï¼ˆä¸æŠŠå…§å®¹å…¨å¡çˆ†ï¼‰
            st.code("\n".join(evidence_keys[:800]), language="text")
        else:
            st.write("ï¼ˆæ²’æœ‰ /evidence/ æª”æ¡ˆï¼‰")


# =========================
# Indexingï¼ˆå¢é‡ï¼šOCR + embeddingsï¼‰
# =========================
def build_indices_incremental_no_kg(
    client: OpenAI,
    file_rows: list[FileRow],
    file_bytes_map: Dict[str, bytes],
    store: Optional[FaissStore],
    processed_keys: set,
    chunk_size: int = 900,
    overlap: int = 150,
) -> Tuple[FaissStore, Dict[str, Any], set]:
    if store is None:
        dim = embed_texts(client, ["dim_probe"]).shape[1]
        store = FaissStore(dim)

    stats = {"new_reports": 0, "new_chunks": 0}
    new_chunks: list[Chunk] = []
    new_texts: list[str] = []

    to_process = []
    for r in file_rows:
        key = (r.file_sig, bool(r.use_ocr))
        if key not in processed_keys:
            to_process.append(r)

    for row in to_process:
        data = file_bytes_map[row.file_id]
        report_id = row.file_id
        title = os.path.splitext(row.name)[0]  # âœ… ä¾†æºé¡¯ç¤ºç”¨ã€Œæª”åï¼ˆä¸å«å‰¯æª”åï¼‰ã€å¾ˆåˆç†
        stats["new_reports"] += 1

        if row.ext == ".pdf":
            pages = ocr_pdf_pages_parallel(client, data) if row.use_ocr else extract_pdf_text_pages(data)
        elif row.ext == ".txt":
            pages = [(None, norm_space(data.decode("utf-8", errors="ignore")))]
        elif row.ext in (".png", ".jpg", ".jpeg"):
            mime = "image/jpeg" if row.ext in (".jpg", ".jpeg") else "image/png"
            txt = norm_space(ocr_image_bytes(client, data, mime=mime))
            pages = [(None, txt)]
        else:
            pages = [(None, "")]

        for page_no, page_text in pages:
            if not page_text:
                continue
            for i, ch in enumerate(chunk_text(page_text, chunk_size=chunk_size, overlap=overlap)):
                cid = f"{report_id}_p{page_no if page_no else 'na'}_c{i}"
                new_chunks.append(Chunk(cid, report_id, title, page_no if isinstance(page_no, int) else None, ch))
                new_texts.append(ch)

        processed_keys.add((row.file_sig, bool(row.use_ocr)))

    if new_texts:
        vecs_list = []
        for i in range(0, len(new_texts), EMBED_BATCH_SIZE):
            vecs_list.append(embed_texts(client, new_texts[i:i + EMBED_BATCH_SIZE]))
        vecs = np.vstack(vecs_list)
        store.add(vecs, new_chunks)

    stats["new_chunks"] = len(new_chunks)
    return store, stats, processed_keys


# =========================
# é è¨­è¼¸å‡ºï¼ˆä¸‰ä»½ bullets + citationsï¼‰
# =========================
def _split_default_bundle(text: str) -> Dict[str, str]:
    t = (text or "").strip()
    pattern = re.compile(
        r"###\s*SUMMARY\s*(.*?)###\s*CLAIMS\s*(.*?)###\s*CHAIN\s*(.*)$",
        re.IGNORECASE | re.DOTALL,
    )
    m = pattern.search(t)
    if not m:
        return {"summary": "", "claims": "", "chain": ""}
    return {"summary": m.group(1).strip(), "claims": m.group(2).strip(), "chain": m.group(3).strip()}


def pick_corpus_chunks_for_default(all_chunks: list[Chunk]) -> list[Chunk]:
    by_title: Dict[str, list[Chunk]] = {}
    for c in all_chunks:
        by_title.setdefault(c.title, []).append(c)

    kw = re.compile(
        r"(outlook|risk|implication|forecast|scenario|inflation|rate|credit|spread|cap rate|vacancy|supply|demand|rental|office|retail|residential|logistics|hotel|reits)",
        re.I,
    )

    def score(c: Chunk) -> float:
        s = 0.0
        if kw.search(c.text or ""):
            s += 6.0
        if c.page is not None:
            s += max(0.0, 2.0 - min(2.0, float(c.page) / 6.0))
        s += min(2.0, len(c.text) / 1400.0)
        return s

    chosen: list[Chunk] = []
    for title, chunks in sorted(by_title.items(), key=lambda x: x[0]):
        by_page: Dict[int, list[Chunk]] = {}
        for c in chunks:
            p = c.page if c.page is not None else 0
            by_page.setdefault(p, []).append(c)

        page_best = []
        for p, cs in by_page.items():
            cs = sorted(cs, key=score, reverse=True)
            page_best.append(cs[0])

        page_best = sorted(page_best, key=score, reverse=True)
        chosen.extend(page_best[:CORPUS_PER_REPORT_QUOTA])

    chosen = sorted(chosen, key=score, reverse=True)[:CORPUS_DEFAULT_MAX_CHUNKS]
    return chosen


def render_chunks_for_model(chunks: list[Chunk], max_chars_each: int = 900) -> str:
    parts = []
    for c in chunks:
        head = f"[{c.title} p{c.page if c.page is not None else '-'}]"
        parts.append(head + "\n" + c.text[:max_chars_each])
    return "\n\n".join(parts)


def generate_default_outputs_bundle(client: OpenAI, title: str, ctx: str, max_retries: int = 2) -> Dict[str, str]:
    system = (
        "ä½ æ˜¯åš´è¬¹çš„ç ”ç©¶åŠ©ç†ï¼Œåªèƒ½æ ¹æ“šæˆ‘æä¾›çš„è³‡æ–™å›ç­”ï¼Œä¸å¯è…¦è£œã€‚\n"
        "ç¡¬æ€§è¦å‰‡ï¼š\n"
        "1) ä½ å¿…é ˆè¼¸å‡ºä¸‰å€‹å€å¡Šï¼Œä¸”é †åº/æ¨™é¡Œå›ºå®šï¼š### SUMMARYã€### CLAIMSã€### CHAINã€‚\n"
        "2) æ¯å€‹å€å¡Šéƒ½å¿…é ˆæ˜¯ç´” bulletï¼ˆæ¯è¡Œä»¥ - é–‹é ­ï¼‰ï¼Œä¸è¦æ®µè½ã€‚\n"
        "3) æ¯å€‹ bullet å¥å°¾å¿…é ˆé™„å¼•ç”¨ï¼Œæ ¼å¼å›ºå®šï¼š[å ±å‘Šåç¨± pé ]\n"
        "4) å¼•ç”¨ä¸­çš„ã€å ±å‘Šåç¨±ã€å¿…é ˆæ˜¯è³‡æ–™ç‰‡æ®µæ–¹æ‹¬è™Ÿå…§çš„é‚£å€‹åç¨±ã€‚\n"
        "5) ä¸å¯ä½¿ç”¨ /evidence/*.md ç•¶ä½œå ±å‘Šåç¨±ã€‚\n"
    )
    user = (
        f"è«‹é‡å°ã€Š{title}ã€‹ä¸€æ¬¡è¼¸å‡ºä¸‰ä»½å…§å®¹ï¼ˆèåˆå¤šä»½å ±å‘Šï¼‰ï¼š\n"
        f"- SUMMARYï¼š8~14 bullets\n"
        f"- CLAIMSï¼š8~14 bullets\n"
        f"- CHAINï¼š6~12 bullets\n\n"
        f"è³‡æ–™ï¼š\n{ctx}\n"
    )

    last = ""
    for _ in range(max_retries + 1):
        out, _ = call_gpt(client, model=MODEL_MAIN, system=system, user=user, reasoning_effort=REASONING_EFFORT)
        parts = _split_default_bundle(out)
        ok = bullets_all_have_citations(parts["summary"]) and bullets_all_have_citations(parts["claims"]) and bullets_all_have_citations(parts["chain"])
        if ok:
            return parts
        last = out
        user += "\n\nã€å¼·åˆ¶ä¿®æ­£ã€‘æ•´ä»½é‡å¯«ï¼šä¸‰å€å¡Šçš†ç‚ºç´” bulletï¼Œä¸”æ¯å€‹ bullet å¥å°¾éƒ½æœ‰ [å ±å‘Šåç¨± pé ]ï¼›ä¸å¾—å‡ºç¾ /evidence/*.mdã€‚"
    return _split_default_bundle(last)


# =========================
# DeepAgent
# =========================
def ensure_deep_agent(client: OpenAI, store: FaissStore, enable_web: bool):
    _require_deepagents()
    from langchain_core.tools import BaseTool, StructuredTool

    st.session_state.setdefault("deep_agent", None)
    st.session_state.setdefault("deep_agent_web_flag", None)
    st.session_state.setdefault("da_usage", {"doc_search_calls": 0, "web_search_calls": 0})

    if (st.session_state.deep_agent is not None) and (st.session_state.deep_agent_web_flag == bool(enable_web)):
        return st.session_state.deep_agent

    lock = threading.Lock()
    usage = {"doc_search_calls": 0, "web_search_calls": 0}
    st.session_state["da_usage"] = usage

    def _inc(name: str, limit: int) -> bool:
        with lock:
            usage[name] += 1
            st.session_state["da_usage"] = usage
            return usage[name] <= limit

    def _get_usage_fn() -> str:
        with lock:
            return json.dumps(usage, ensure_ascii=False)

    def _doc_list_fn() -> str:
        by_title: Dict[str, int] = {}
        for c in store.chunks:
            by_title[c.title] = by_title.get(c.title, 0) + 1
        lines = [f"- {t} (chunks={n})" for t, n in sorted(by_title.items(), key=lambda x: x[0])]
        return "\n".join(lines) if lines else "ï¼ˆç›®å‰æ²’æœ‰ä»»ä½•å·²ç´¢å¼•æ–‡ä»¶ï¼‰"

    def _doc_search_fn(query: str, k: int = 8) -> str:
        if not _inc("doc_search_calls", DA_MAX_DOC_SEARCH_CALLS):
            return json.dumps({"hits": [], "error": f"Budget exceeded: doc_search_calls > {DA_MAX_DOC_SEARCH_CALLS}"}, ensure_ascii=False)

        q = (query or "").strip()
        if not q:
            return json.dumps({"hits": []}, ensure_ascii=False)

        qvec = embed_texts(client, [q])
        hits = store.search(qvec, k=max(1, min(12, int(k))))

        payload = {"hits": []}
        for score, ch in hits:
            payload["hits"].append({
                "title": ch.title,  # âœ… é€™å€‹ title å°±æ˜¯ä½ è¦é¡¯ç¤ºåœ¨ badge çš„ã€Œè³‡æ–™æª”åã€
                "page": str(ch.page) if ch.page is not None else "-",
                "chunk_id": ch.chunk_id,  # internal only
                "text": (ch.text or "")[:1200],
                "score": float(score),
            })
        return json.dumps(payload, ensure_ascii=False)

    def _doc_get_chunk_fn(chunk_id: str, max_chars: int = 2600) -> str:
        cid = (chunk_id or "").strip()
        if not cid:
            return ""
        for c in store.chunks:
            if c.chunk_id == cid:
                return (c.text or "")[:max_chars]
        return ""

    def _mk_tool(fn, name: str, description: str) -> BaseTool:
        return StructuredTool.from_function(fn, name=name, description=description)

    tool_get_usage = _mk_tool(_get_usage_fn, "get_usage", "Get current tool usage counters as JSON (budget/debug).")
    tool_doc_list = _mk_tool(_doc_list_fn, "doc_list", "List indexed documents and chunk counts.")
    tool_doc_search = _mk_tool(_doc_search_fn, "doc_search", "Semantic search over indexed chunks. Returns JSON hits with title/page/chunk_id/text.")
    tool_doc_get_chunk = _mk_tool(_doc_get_chunk_fn, "doc_get_chunk", "Fetch full text for a given chunk_id for close reading. Returns text only.")

    tools: list[BaseTool] = [tool_get_usage, tool_doc_list, tool_doc_search, tool_doc_get_chunk]

    tool_web_search_summary: Optional[BaseTool] = None
    if enable_web:
        def _web_search_summary_fn(query: str) -> str:
            if not _inc("web_search_calls", DA_MAX_WEB_SEARCH_CALLS):
                return f"[WebSearch:{(query or '')[:30]} p-]\nBudget exceeded: web_search_calls > {DA_MAX_WEB_SEARCH_CALLS}"

            q = (query or "").strip()
            if not q:
                return "[WebSearch:empty p-]\nï¼ˆquery ç‚ºç©ºï¼‰"

            system = (
                "ä½ æ˜¯ç ”ç©¶åŠ©ç†ã€‚ç”¨ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰æ•´ç† web_search çµæœæˆ 2-4 æ®µæ‘˜è¦ï¼Œä¿ç•™æ—¥æœŸ/åè©ã€‚"
                "è‹¥çŸ›ç›¾è¦æŒ‡å‡ºã€‚æœ€å¾Œç”¨ Sources: åˆ—å‡º title + urlã€‚"
            )
            user = f"Search term: {q}"
            text, sources = call_gpt(
                client,
                model=MODEL_WEB,
                system=system,
                user=user,
                reasoning_effort=None,
                tools=[{"type": "web_search"}],
                include_sources=True,
            )
            src_lines = []
            for s in (sources or [])[:8]:
                if isinstance(s, dict):
                    t = s.get("title") or s.get("source") or "source"
                    u = s.get("url") or ""
                    if u:
                        src_lines.append(f"- {t} {u}".strip())

            out_text = (text or "").strip()
            if src_lines:
                out_text = (out_text + "\n\nSources:\n" + "\n".join(src_lines)).strip()

            return f"[WebSearch:{q[:30]} p-]\n" + out_text[:2400]

        tool_web_search_summary = _mk_tool(_web_search_summary_fn, "web_search_summary", "Run web_search and return a short Traditional Chinese summary with sources.")
        tools.append(tool_web_search_summary)

    # prompts
    retriever_prompt = f"""
ä½ æ˜¯æ–‡ä»¶æª¢ç´¢å°ˆå®¶ï¼ˆåªå…è¨±ä½¿ç”¨ doc_list/doc_search/doc_get_chunk/get_usageï¼‰ã€‚

facet å­ä»»å‹™æ ¼å¼ï¼š
facet_slug: <è‹±æ–‡å°å¯«_åº•ç·š>
facet_goal: <é€™å€‹é¢å‘è¦å›ç­”ä»€éº¼>
hints: <å¯èƒ½çš„é—œéµå­—/æŒ‡æ¨™/åè©ï¼ˆå¯ç©ºï¼‰>

ç¡¬è¦å‰‡ï¼š
- ä½ è¦å¯«å…¥ /evidence/doc_<facet_slug>.md
- evidence å…§å®¹åªèƒ½åŒ…å«ï¼š
  1) å¼•ç”¨æ¨™é ­ï¼š[å ±å‘Šåç¨± pé ]ï¼ˆä¸å¾—åŒ…å« /evidence/ è·¯å¾‘ï¼›ä¸å¾—ç”¨ doc_*.md ç•¶å ±å‘Šåç¨±ï¼‰
  2) åŸæ–‡ç‰‡æ®µï¼ˆå¯æˆªæ–·ï¼‰
  3) ä¸€è¡Œèªªæ˜ã€Œé€™æ®µæ”¯æŒä»€éº¼ã€
- ä½ å¯ä»¥ç”¨ doc_search æ‹¿åˆ° chunk_idï¼Œç„¶å¾Œç”¨ doc_get_chunk(chunk_id=...) ç²¾è®€ï¼Œ
  ä½† chunk_id çµ•å°ä¸èƒ½å¯«é€² evidenceã€‚

è‹¥é‡åˆ° Budget exceededï¼šç«‹åˆ»åœæ­¢ä¸¦åœ¨ evidence æ˜è¬›ã€Œè­‰æ“šä¸è¶³ã€ã€‚
æœ€å¾Œå›è¦† orchestratorï¼šâ‰¤150 å­—æ‘˜è¦ï¼ˆæ‰¾åˆ°ä»€éº¼ + æœ€å¤§ç¼ºå£ï¼‰
"""

    writer_prompt = f"""
ä½ æ˜¯å¯«ä½œ/æ•´ç†å°ˆå®¶ï¼ˆç”¨ read_file/glob/grep/write_file/edit_file/lsï¼‰ã€‚
ä½ å¿…é ˆæ•´åˆ /evidence/ åº•ä¸‹æ‰€æœ‰æª”æ¡ˆï¼ˆdoc_*.md èˆ‡å¯é¸ web_*.mdï¼‰ã€‚

ä»»å‹™é¡å‹åˆ¤æ–·ï¼š
- å®Œæ•´å ±å‘Š/ç« ç¯€ â†’ REPORT
- å–®é¡Œå›ç­” â†’ QA
- æ•´ç†çŸ¥è­˜è„ˆçµ¡ â†’ KNOWLEDGE
- ä½¿ç”¨è€…è²¼è‰ç¨¿è¦æŸ¥æ ¸/é™¤éŒ¯ â†’ VERIFY_DRAFTï¼ˆæœ€å¤š {DA_MAX_CLAIMS} æ¢ä¸»å¼µï¼‰

å¼•ç”¨è¦å‰‡ï¼ˆåš´æ ¼ï¼‰ï¼š
- QAï¼šç´” bulletï¼ˆæ¯è¡Œ -ï¼‰ï¼Œæ¯å€‹ bullet å¥å°¾å¿…æœ‰å¼•ç”¨ [å ±å‘Šåç¨± pé ] æˆ– [WebSearch:* p-]
- REPORT/KNOWLEDGE/VERIFYï¼šMarkdownï¼›æ¯å€‹éæ¨™é¡Œæ®µè½è‡³å°‘ 1 å€‹å¼•ç”¨
- enable_web=falseï¼šä¸å¾—å‡ºç¾ WebSearch
- draft çµ•å°ä¸èƒ½å‡ºç¾ chunk_id
- å ±å‘Šåç¨±ä¸å¾—æ˜¯ /evidence/*.md

æŠŠçµæœå¯«åˆ° /draft.md
"""

    verifier_prompt = f"""
ä½ æ˜¯å¯©ç¨¿æŸ¥æ ¸å°ˆå®¶ï¼ˆç”¨ read_file/edit_file/grepï¼‰ã€‚
ä»»å‹™ï¼šæª¢æŸ¥ /draft.md æ˜¯å¦ç¬¦åˆå¼•ç”¨è¦†è“‹ï¼Œä¸¦åšã€æœ€å°‘æ”¹å‹•ã€ä¿®æ­£ã€‚

è¦å‰‡ï¼š
- QAï¼šæ¯å€‹ bullet å¥å°¾å¿…æœ‰ [.. p..]
- å…¶ä»–ï¼šæ¯å€‹éæ¨™é¡Œæ®µè½è‡³å°‘ 1 å€‹å¼•ç”¨ [.. p..]
- enable_web=falseï¼šä¸å¾—å‡ºç¾ WebSearch
- è‹¥ /draft.md å‡ºç¾ chunk_id ç—•è·¡ï¼ˆchunk_id= æˆ– _p*_c*ï¼‰ï¼Œå¿…é ˆç§»é™¤ã€‚
- å¼•ç”¨æ¨™é ­ä¸å¾—ä½¿ç”¨ /evidence/*.md

æœ€å¤šä¿®æ­£ {DA_MAX_REWRITE_ROUNDS} è¼ªï¼š
- æ¯è¼ªï¼šread /draft.md â†’ edit_file ä¿®æ­£ â†’ write /review.md è¨˜éŒ„
"""

    subagents = [
        {
            "name": "retriever",
            "description": "å¾ä¸Šå‚³æ–‡ä»¶å‘é‡åº«æ‰¾è­‰æ“šï¼Œå¯« /evidence/doc_*.mdï¼ˆä¸å« chunk_idï¼‰",
            "system_prompt": retriever_prompt,
            "tools": [tool_get_usage, tool_doc_list, tool_doc_search, tool_doc_get_chunk],
            "model": f"openai:{MODEL_MAIN}",
        },
        {
            "name": "writer",
            "description": "æ•´åˆ /evidence/ â†’ ç”¢ç”Ÿ /draft.md",
            "system_prompt": writer_prompt,
            "tools": [],
            "model": f"openai:{MODEL_MAIN}",
        },
        {
            "name": "verifier",
            "description": "æª¢æŸ¥å¼•ç”¨è¦†è“‹ä¸¦ä¿®ç¨¿ /draft.mdï¼Œå¯« /review.md",
            "system_prompt": verifier_prompt,
            "tools": [],
            "model": f"openai:{MODEL_MAIN}",
        },
    ]

    if enable_web:
        web_prompt = """
ä½ æ˜¯ç¶²è·¯æœå°‹å°ˆå®¶ï¼ˆåªå…è¨± web_search_summary/get_usageï¼›ä¸å…è¨± doc_*ï¼‰ã€‚
facet å­ä»»å‹™æ ¼å¼åŒ retrieverã€‚

ç¡¬è¦å‰‡ï¼š
- å¯«å…¥ /evidence/web_<facet_slug>.md
- æ¯æ®µè¦ä¿ç•™å¼•ç”¨æ¨™é ­ [WebSearch:... p-]
- ç¦æ­¢æé€ ä¾†æºï¼›è‹¥ Budget exceeded å°±åœæ­¢ä¸¦å¯«æ˜ç¼ºå£
"""
        subagents.insert(
            1,
            {
                "name": "web-researcher",
                "description": "ç”¨ OpenAI å…§å»º web_search åšå°‘é‡é«˜å“è³ªæœå°‹ï¼Œå¯« /evidence/web_*.md",
                "system_prompt": web_prompt,
                "tools": [tool_web_search_summary, tool_get_usage] if tool_web_search_summary else [tool_get_usage],
                "model": f"openai:{MODEL_MAIN}",
            },
        )

    # âœ… é€™è£¡æ”¹æœ€é‡è¦ï¼šä¸è¦ç”¨ã€Œwrite_todosã€é€™ç¨®ä¸å­˜åœ¨çš„è©ï¼›
    #    æ˜ç¢ºè¦æ±‚ç”¨ write_file å¯« /workspace/todos.json
    orchestrator_prompt = f"""
ä½ æ˜¯ Deep Doc Orchestratorï¼ˆæ–‡ä»¶å„ªå…ˆï¼›enable_web={str(enable_web).lower()}ï¼‰ã€‚

å›ºå®šæµç¨‹ï¼ˆå¿…åšï¼‰ï¼š
0) ç«‹åˆ»ç”¨ write_file å»ºç«‹ /workspace/todos.jsonï¼ˆJSON array of stringsï¼›5~9 æ­¥ï¼‰ï¼Œä¸¦åœ¨ç¬¬ä¸€æ­¥èªªæ˜ enable_web èˆ‡ç›®æ¨™ã€‚
1) write_file /evidence/README.md è¨˜éŒ„æœ¬æ¬¡éœ€æ±‚èˆ‡ enable_web
2) æ‹† 2â€“4 å€‹ facetsï¼ˆé¢å‘ï¼Œä¸æ˜¯ç« ç¯€ï¼‰
3) å¹³è¡Œæ´¾å·¥ï¼š
   - æ¯å€‹ facet è‡³å°‘æ´¾ 1 å€‹ retriever
   - enable_web=true ä¸”éœ€è¦å¤–éƒ¨èƒŒæ™¯æ™‚ï¼Œå°åŒ facet å†æ´¾ 1 å€‹ web-researcher
4) å« writer ç”¢ç”Ÿ /draft.md
5) å« verifier ä¿®ç¨¿ï¼ˆæœ€å¤š {DA_MAX_REWRITE_ROUNDS} è¼ªï¼‰
6) read_file /draft.md ä½œç‚ºæœ€çµ‚å›ç­”

å¼•ç”¨èˆ‡éš±ç§è¦å‰‡ï¼š
- /evidence èˆ‡ /draft çµ•å°ä¸èƒ½å‡ºç¾ chunk_id
- å¼•ç”¨åªèƒ½ç”¨ [å ±å‘Šåç¨± pé ] æˆ– [WebSearch:* p-]
- å ±å‘Šåç¨±ä¸å¾—ä½¿ç”¨ /evidence/*.md ç•¶ä½œä¾†æºåç¨±
"""

    llm = _make_langchain_llm(model_name=f"openai:{MODEL_MAIN}", temperature=0.0, reasoning_effort=REASONING_EFFORT)

    agent = create_deep_agent(
        model=llm,
        tools=tools,
        system_prompt=orchestrator_prompt,
        subagents=subagents,
        debug=False,
        name="deep-doc-agent",
    ).with_config({"recursion_limit": 90})

    st.session_state.deep_agent = agent
    st.session_state.deep_agent_web_flag = bool(enable_web)
    return agent


# =========================
# DeepAgent runï¼ˆstatus ä¸å±•é–‹ + planning å¾Œé¡¯ç¤º todosï¼‰
# =========================
def deep_agent_run_with_live_status(agent, user_text: str) -> Tuple[str, Optional[dict]]:
    final_state = None
    todos_preview_written = False

    def set_phase(s, phase: str):
        mapping = {
            "start": ("DeepAgentï¼šå•Ÿå‹•ä¸­â€¦", "running"),
            "plan": ("DeepAgentï¼šè¦åŠƒä¸­â€¦", "running"),
            "evidence": ("DeepAgentï¼šè’è­‰ä¸­â€¦", "running"),
            "draft": ("DeepAgentï¼šå¯«ä½œä¸­â€¦", "running"),
            "review": ("DeepAgentï¼šå¯©ç¨¿/è£œå¼•ç”¨ä¸­â€¦", "running"),
            "done": ("DeepAgentï¼šå®Œæˆ", "complete"),
            "error": ("DeepAgentï¼šç™¼ç”ŸéŒ¯èª¤", "error"),
        }
        label, state = mapping.get(phase, ("DeepAgentï¼šåŸ·è¡Œä¸­â€¦", "running"))
        s.update(label=label, state=state, expanded=False)

    with st.status("DeepAgentï¼šå•Ÿå‹•ä¸­â€¦", expanded=False) as s:
        set_phase(s, "start")
        set_phase(s, "plan")

        try:
            for state in agent.stream(
                {"messages": [{"role": "user", "content": user_text}]},
                stream_mode="values",
            ):
                final_state = state
                files = state.get("files") or {}
                file_keys = set(files.keys()) if isinstance(files, dict) else set()

                # âœ… è¦åŠƒå¾Œå¦‚æœ todos.json å‡ºç¾ï¼Œç«‹åˆ»é¡¯ç¤ºé è¦½åœ¨ status è£¡
                if (not todos_preview_written) and isinstance(files, dict) and "/workspace/todos.json" in files:
                    todos_txt = get_files_text(files, "/workspace/todos.json")
                    if todos_txt:
                        s.write("### æœ¬æ¬¡ Todoï¼ˆè¦åŠƒçµæœé è¦½ï¼‰")
                        s.code(todos_txt[:4000], language="json")
                        todos_preview_written = True

                if any(k.startswith("/evidence/") for k in file_keys):
                    set_phase(s, "evidence")
                if "/draft.md" in file_keys:
                    set_phase(s, "draft")
                if "/review.md" in file_keys:
                    set_phase(s, "review")

        except Exception as e:
            msg = str(e)
            if "Budget exceeded" in msg:
                set_phase(s, "evidence")
                s.update(label="DeepAgentï¼šå·²é”å·¥å…·é ç®—ä¸Šé™ï¼ˆåœæ­¢åŠ æœè­‰ï¼‰", state="running", expanded=False)
            else:
                try:
                    final_state = agent.invoke({"messages": [{"role": "user", "content": user_text}]})
                except Exception:
                    set_phase(s, "error")
                    raise

        files = (final_state or {}).get("files") or {}

        # âœ… æœ€çµ‚ç­”æ¡ˆï¼šä¸€å¾‹å– /draft.md çš„ content
        final_text = get_files_text(files, "/draft.md")

        if not final_text:
            msgs = (final_state or {}).get("messages") or []
            if msgs:
                last = msgs[-1]
                content = getattr(last, "content", None)
                final_text = (file_to_text(content) or file_to_text(last)).strip()

        if final_text and CHUNK_ID_LEAK_PAT.search(final_text):
            final_text = CHUNK_ID_LEAK_PAT.sub("", final_text)

        set_phase(s, "done")

    return final_text or "ï¼ˆDeepAgent æ²’æœ‰ç”¢å‡ºå…§å®¹ï¼‰", files if isinstance(files, dict) and files else None


# =========================
# need_todo åˆ¤æ–·
# =========================
def decide_need_todo(client: OpenAI, question: str) -> Tuple[bool, str]:
    system = (
        "ä½ æ˜¯è·¯ç”±å™¨ã€‚è«‹åˆ¤æ–·é€™å€‹å•é¡Œæ˜¯å¦éœ€è¦åšã€Todo + æ–‡ä»¶/ç¶²è·¯æª¢ç´¢ã€ã€‚\n"
        "è¦å‰‡ï¼š\n"
        "- éœ€è¦ï¼šæ¶‰åŠå…·é«”äº‹å¯¦ã€æ•¸æ“šã€æ”¿ç­–ã€ç‰ˆæœ¬ã€å¼•ç”¨ã€ç ”ç©¶ã€æ¯”è¼ƒã€å‡ºè™•ï¼›æˆ–éœ€è¦å¼•ç”¨ä¸Šå‚³æ–‡ä»¶ã€‚\n"
        "- ä¸éœ€è¦ï¼šç´”æ„è¦‹/å¯«ä½œ/æ”¹å¯«/è…¦åŠ›æ¿€ç›ª/ä¸è¦æ±‚å¼•ç”¨çš„æ³›æ³›è§£é‡‹ã€‚\n"
        "è«‹è¼¸å‡º JSONï¼š{\"need_todo\": true/false, \"reason\": \"...\"}ï¼ˆåªè¼¸å‡º JSONï¼‰"
    )
    out, _ = call_gpt(
        client,
        model=MODEL_MAIN,
        system=system,
        user=question,
        reasoning_effort=REASONING_EFFORT,
        tools=None,
        include_sources=False,
    )
    data = _try_parse_json_or_py_literal(out) or {}
    need = bool(data.get("need_todo", False))
    reason = str(data.get("reason", "")).strip() or "ï¼ˆæœªæä¾›åŸå› ï¼‰"
    return need, reason


def render_run_badges(*, mode: str, need_todo: bool, reason: str, usage: dict, enable_web: bool, todo_file_present: Optional[bool] = None):
    badges: List[str] = []
    badges.append(_badge_directive(f"Mode:{mode}", "gray"))

    if need_todo:
        badges.append(_badge_directive("Todo:éœ€è¦", "blue"))
    else:
        badges.append(_badge_directive("Todo:ä¸éœ€è¦", "blue"))
        short_reason = reason if len(reason) <= 40 else reason[:40] + "â€¦"
        badges.append(_badge_directive(f"ç†ç”±:{short_reason}", "gray"))

    if todo_file_present is True:
        badges.append(_badge_directive("Todos.json:æœ‰", "blue"))
    elif todo_file_present is False and need_todo:
        badges.append(_badge_directive("Todos.json:ç„¡(æµç¨‹ç•°å¸¸)", "orange"))

    doc_calls = int((usage or {}).get("doc_search_calls", 0) or 0)
    web_calls = int((usage or {}).get("web_search_calls", 0) or 0)

    if doc_calls > 0:
        badges.append(_badge_directive(f"DB:used({doc_calls})", "green"))
    else:
        badges.append(_badge_directive("DB:unused", "gray"))

    if enable_web:
        if web_calls > 0:
            badges.append(_badge_directive(f"Web:used({web_calls})", "violet"))
        else:
            badges.append(_badge_directive("Web:unused", "gray"))
    else:
        badges.append(_badge_directive("Web:disabled", "gray"))

    st.markdown(" ".join(badges))


# =========================
# Session init
# =========================
OPENAI_API_KEY = get_openai_api_key()
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
os.environ.setdefault("OPENAI_KEY", OPENAI_API_KEY)

client = get_client(OPENAI_API_KEY)

st.session_state.setdefault("file_rows", [])
st.session_state.setdefault("file_bytes", {})
st.session_state.setdefault("store", None)
st.session_state.setdefault("processed_keys", set())
st.session_state.setdefault("default_outputs", None)
st.session_state.setdefault("chat_history", [])
st.session_state.setdefault("enable_web_search_agent", False)


# =========================
# File table helpers
# =========================
def file_rows_to_df(rows: list[FileRow]) -> pd.DataFrame:
    recs = []
    for r in rows:
        if r.ext == ".pdf":
            pages_str = "-" if r.pages is None else str(r.pages)
            text_pages_str = "-" if r.text_pages is None else str(r.text_pages)
            text_ratio_str = "-" if r.text_pages_ratio is None else f"{r.text_pages_ratio:.0%}"
        else:
            pages_str = "-" if r.pages is None else str(r.pages)
            text_pages_str = "-"
            text_ratio_str = "-"

        if r.ext == ".pdf" and r.likely_scanned:
            suggest = "å»ºè­° OCR"
        elif r.ext in (".png", ".jpg", ".jpeg"):
            suggest = "å¿… OCR"
        else:
            suggest = ""

        recs.append({
            "_file_id": r.file_id,
            "ä½¿ç”¨OCR": bool(r.use_ocr),
            "æª”å": truncate_filename(r.name, 52),
            "æ ¼å¼": r.ext.replace(".", ""),
            "é æ•¸": pages_str,
            "æ–‡å­—é ": text_pages_str,
            "æ–‡å­—%": text_ratio_str,
            "tokenä¼°ç®—": int(r.token_est),
            "å»ºè­°": suggest,
        })
    return pd.DataFrame(recs)


def sync_df_to_file_rows(df: pd.DataFrame, rows: list[FileRow]) -> None:
    id_to_row_idx = {r.file_id: i for i, r in enumerate(rows)}
    for _, rec in df.iterrows():
        fid = rec.get("_file_id")
        if fid not in id_to_row_idx:
            continue
        i = id_to_row_idx[fid]

        ext = rows[i].ext
        if ext in (".png", ".jpg", ".jpeg"):
            rows[i].use_ocr = True
        elif ext == ".txt":
            rows[i].use_ocr = False
        else:
            rows[i].use_ocr = bool(rec.get("ä½¿ç”¨OCR", rows[i].use_ocr))


# =========================
# Popoverï¼šæ–‡ä»¶ç®¡ç† + DeepAgent è¨­å®š
# =========================
with st.popover("ğŸ“¦ æ–‡ä»¶ç®¡ç†ï¼ˆä¸Šå‚³ / OCR / å»ºç´¢å¼• / DeepAgentè¨­å®šï¼‰", use_container_width=True):
    st.caption("æ”¯æ´ PDF/TXT/PNG/JPGã€‚PDF è‹¥æ–‡å­—æŠ½å–åå°‘æœƒå»ºè­° OCRï¼ˆé€æª”å¯å‹¾é¸ï¼‰ã€‚")
    st.caption("âœ… ä¸ä¸Šå‚³æ–‡ä»¶ä¹Ÿèƒ½èŠå¤©ï¼›åªæœ‰ä½ éœ€è¦å¼•ç”¨æ–‡ä»¶æ™‚æ‰éœ€è¦å»ºç«‹ç´¢å¼•ã€‚")

    st.session_state.enable_web_search_agent = st.checkbox(
        "å•Ÿç”¨ç¶²è·¯æœå°‹ï¼ˆæœƒå¢åŠ æˆæœ¬ï¼‰",
        value=bool(st.session_state.enable_web_search_agent),
    )

    uploaded = st.file_uploader(
        "ä¸Šå‚³æ–‡ä»¶",
        type=["pdf", "txt", "png", "jpg", "jpeg"],
        accept_multiple_files=True,
    )

    if uploaded:
        existing = {(r.name, r.bytes_len) for r in st.session_state.file_rows}
        for f in uploaded:
            data = f.read()
            if (f.name, len(data)) in existing:
                continue

            ext = os.path.splitext(f.name)[1].lower()
            fid = str(uuid.uuid4())[:10]
            sig = sha1_bytes(data)
            st.session_state.file_bytes[fid] = data

            pages = None
            extracted_chars = 0
            blank_pages = None
            blank_ratio = None
            text_pages = None
            text_pages_ratio = None

            if ext == ".pdf":
                pdf_pages = extract_pdf_text_pages(data)
                pages = len(pdf_pages)
                extracted_chars, blank_pages, blank_ratio, text_pages, text_pages_ratio = analyze_pdf_text_quality(pdf_pages)
            elif ext == ".txt":
                extracted_chars = len(norm_space(data.decode("utf-8", errors="ignore")))

            token_est = estimate_tokens_from_chars(extracted_chars)
            likely_scanned = should_suggest_ocr(ext, pages, extracted_chars, blank_ratio)

            if ext in (".png", ".jpg", ".jpeg"):
                use_ocr = True
            elif ext == ".txt":
                use_ocr = False
            else:
                use_ocr = bool(likely_scanned)

            st.session_state.file_rows.append(
                FileRow(
                    file_id=fid,
                    file_sig=sig,
                    name=f.name,
                    ext=ext,
                    bytes_len=len(data),
                    pages=pages,
                    extracted_chars=extracted_chars,
                    token_est=token_est,
                    text_pages=text_pages,
                    text_pages_ratio=text_pages_ratio,
                    blank_pages=blank_pages,
                    blank_ratio=blank_ratio,
                    likely_scanned=likely_scanned,
                    use_ocr=use_ocr,
                )
            )

    st.markdown("### æ–‡ä»¶æ¸…å–®ï¼ˆå¯é€æª”å‹¾é¸ OCRï¼‰")

    if not st.session_state.file_rows:
        st.info("å°šæœªä¸Šå‚³æ–‡ä»¶ã€‚")
    else:
        df = file_rows_to_df(st.session_state.file_rows)

        edited = st.data_editor(
            df.drop(columns=["_file_id"]),
            key="file_table_editor",
            use_container_width=True,
            hide_index=True,
            disabled=["æª”å", "æ ¼å¼", "é æ•¸", "æ–‡å­—é ", "æ–‡å­—%", "tokenä¼°ç®—", "å»ºè­°"],
            column_config={
                "ä½¿ç”¨OCR": st.column_config.CheckboxColumn("ä½¿ç”¨OCR", help="é€æª”é¸æ“‡æ˜¯å¦å•Ÿç”¨ OCRï¼ˆPDF å¯é¸ï¼›åœ–æª”å›ºå®šOCRï¼›TXTå›ºå®šä¸OCRï¼‰"),
            },
        )

        df_for_sync = df.copy()
        df_for_sync["ä½¿ç”¨OCR"] = edited["ä½¿ç”¨OCR"].values
        sync_df_to_file_rows(df_for_sync, st.session_state.file_rows)

        st.divider()
        col1, col2, col3 = st.columns([1, 1, 1])
        build_btn = col1.button("ğŸš€ å»ºç«‹ç´¢å¼•", type="primary", use_container_width=True)
        default_btn = col2.button("ğŸ§¾ ç”¢ç”Ÿé è¨­è¼¸å‡º", use_container_width=True)
        clear_btn = col3.button("ğŸ§¹ æ¸…ç©ºå…¨éƒ¨", use_container_width=True)

        if clear_btn:
            st.session_state.file_rows = []
            st.session_state.file_bytes = {}
            st.session_state.store = None
            st.session_state.processed_keys = set()
            st.session_state.default_outputs = None
            st.session_state.chat_history = []
            st.session_state.deep_agent = None
            st.session_state.deep_agent_web_flag = None
            st.session_state.da_usage = {"doc_search_calls": 0, "web_search_calls": 0}
            st.rerun()

        if build_btn:
            need_ocr = any(r.ext == ".pdf" and r.use_ocr for r in st.session_state.file_rows)
            if need_ocr and not HAS_PYMUPDF:
                st.error("ä½ æœ‰å‹¾é¸ PDF OCRï¼Œä½†ç’°å¢ƒæœªå®‰è£ pymupdfã€‚è«‹å…ˆ pip install pymupdfã€‚")
                st.stop()

            with st.status("å»ºç´¢å¼•ä¸­ï¼ˆOCR + embeddingsï¼‰...", expanded=True) as s:
                t0 = time.perf_counter()
                store, stats, processed_keys = build_indices_incremental_no_kg(
                    client,
                    st.session_state.file_rows,
                    st.session_state.file_bytes,
                    st.session_state.store,
                    st.session_state.processed_keys,
                )
                st.session_state.store = store
                st.session_state.processed_keys = processed_keys
                s.write(f"æ–°å¢å ±å‘Šæ•¸ï¼š{stats['new_reports']}")
                s.write(f"æ–°å¢ chunksï¼š{stats['new_chunks']}")
                s.write(f"è€—æ™‚ï¼š{time.perf_counter() - t0:.2f}s")
                s.update(state="complete")

            st.session_state.deep_agent = None
            st.session_state.deep_agent_web_flag = None
            st.rerun()

        if default_btn:
            if st.session_state.store is None or st.session_state.store.index.ntotal == 0:
                st.warning("å°šæœªå»ºç«‹ç´¢å¼•æˆ–æ²’æœ‰ chunksï¼Œè«‹å…ˆæŒ‰ã€Œå»ºç«‹ç´¢å¼•ã€ã€‚")
            else:
                with st.status("ç”¢ç”Ÿé è¨­è¼¸å‡ºï¼ˆæ‘˜è¦/ä¸»å¼µ/æ¨è«–éˆï¼‰...", expanded=True) as s2:
                    chosen = pick_corpus_chunks_for_default(st.session_state.store.chunks)
                    ctx = render_chunks_for_model(chosen)
                    bundle = generate_default_outputs_bundle(client, "æ•´é«”èåˆï¼ˆå…¨éƒ¨ä¸Šå‚³å ±å‘Šï¼‰", ctx, max_retries=2)
                    st.session_state.default_outputs = bundle
                    s2.update(state="complete")

                st.session_state.chat_history.append({
                    "role": "assistant",
                    "kind": "default",
                    "title": "æ•´é«”èåˆï¼ˆå…¨éƒ¨ä¸Šå‚³å ±å‘Šï¼‰",
                    **(st.session_state.default_outputs or {}),
                })
                st.rerun()


# =========================
# ä¸»ç•«é¢ï¼šç‹€æ…‹ + Chat
# =========================
has_index = (
    st.session_state.store is not None
    and getattr(st.session_state.store, "index", None) is not None
    and st.session_state.store.index.ntotal > 0
)

if has_index:
    st.success(f"å·²å»ºç«‹ç´¢å¼•ï¼šæª”æ¡ˆæ•¸={len(st.session_state.file_rows)} / chunks={len(st.session_state.store.chunks)}")
    st.caption("å¼•ç”¨ badge é¡¯ç¤ºã€è³‡æ–™æª”å + é ç¢¼ã€ï¼›chunk_id åªåœ¨ç³»çµ±å…§éƒ¨ç”¨ä¾†ç²¾è®€èˆ‡æ ¡å°ã€‚")
else:
    st.info("ç›®å‰æ²’æœ‰ç´¢å¼•ï¼šä½ ä»å¯ç›´æ¥èŠå¤©ï¼ˆç´” LLMï¼‰ã€‚è‹¥éœ€è¦å¼•ç”¨æ–‡ä»¶ï¼Œå†å»ã€Œæ–‡ä»¶ç®¡ç†ã€å»ºç«‹ç´¢å¼•ã€‚")

st.divider()
st.subheader("Chatï¼ˆDeepAgent + Badges + Todo decisionï¼‰")

for msg in st.session_state.chat_history:
    with st.chat_message(msg.get("role", "assistant")):
        if msg.get("kind") == "default":
            st.markdown(f"## é è¨­è¼¸å‡ºï¼š{msg.get('title','')}")
            st.markdown("### 1) å ±å‘Šæ‘˜è¦")
            render_bullets_inline_badges(msg.get("summary", ""), badge_color="green")
            st.markdown("### 2) æ ¸å¿ƒä¸»å¼µ")
            render_bullets_inline_badges(msg.get("claims", ""), badge_color="violet")
            st.markdown("### 3) æ¨è«–éˆ")
            render_bullets_inline_badges(msg.get("chain", ""), badge_color="orange")
        else:
            meta = msg.get("meta", {}) or {}
            render_run_badges(
                mode=meta.get("mode", "unknown"),
                need_todo=bool(meta.get("need_todo", False)),
                reason=str(meta.get("reason", "") or ""),
                usage=meta.get("usage", {}) or {},
                enable_web=bool(meta.get("enable_web", False)),
                todo_file_present=meta.get("todo_file_present", None),
            )
            render_markdown_answer_with_source_badges(msg.get("content", ""), badge_color="green")

prompt = st.chat_input("è«‹è¼¸å…¥å•é¡Œï¼ˆä¹Ÿå¯è²¼è‰ç¨¿è¦æˆ‘æŸ¥æ ¸/é™¤éŒ¯ï¼‰ã€‚")
if prompt:
    st.session_state.chat_history.append({"role": "user", "kind": "text", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        enable_web = bool(st.session_state.enable_web_search_agent)
        need_todo, reason = decide_need_todo(client, prompt)

        if (not has_index) or (not need_todo):
            system = "ä½ æ˜¯åŠ©ç†ã€‚ç”¨ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰å›ç­”ï¼Œçµæ§‹æ¸…æ¥šã€‚"
            answer_text, _ = call_gpt(
                client,
                model=MODEL_MAIN,
                system=system,
                user=prompt,
                reasoning_effort=None,
                tools=None,
            )
            meta = {
                "mode": "direct",
                "need_todo": bool(need_todo),
                "reason": reason,
                "usage": {"doc_search_calls": 0, "web_search_calls": 0},
                "enable_web": enable_web,
                "todo_file_present": None,
            }
            render_run_badges(
                mode=meta["mode"],
                need_todo=bool(need_todo),
                reason=reason,
                usage=meta["usage"],
                enable_web=enable_web,
                todo_file_present=None,
            )
            render_markdown_answer_with_source_badges(answer_text, badge_color="green")
            st.session_state.chat_history.append({"role": "assistant", "kind": "text", "content": answer_text, "meta": meta})
            st.stop()

        agent = ensure_deep_agent(client=client, store=st.session_state.store, enable_web=enable_web)
        answer_text, files = deep_agent_run_with_live_status(agent, prompt)

        todo_file_present = isinstance(files, dict) and ("/workspace/todos.json" in files)

        meta = {
            "mode": "deepagent",
            "need_todo": True,
            "reason": reason,
            "usage": dict(st.session_state.get("da_usage", {"doc_search_calls": 0, "web_search_calls": 0})),
            "enable_web": enable_web,
            "todo_file_present": bool(todo_file_present),
        }

        render_run_badges(
            mode=meta["mode"],
            need_todo=True,
            reason=reason,
            usage=meta["usage"],
            enable_web=enable_web,
            todo_file_present=meta["todo_file_present"],
        )
        render_markdown_answer_with_source_badges(answer_text, badge_color="green")

        # âœ… Debugï¼šTabs å…¨åŒ…åœ¨ Debug expander å…§
        with st.expander("Debug", expanded=False):
            # é¡å¤–ï¼šåœ¨ Debug æœ€ä¸Šæ–¹å…ˆé¡¯ç¤º todos é è¦½ï¼ˆä½ æƒ³è¦ã€Œé»é–‹å¯çœ‹åˆ° todosã€ï¼‰
            todos_txt = get_files_text(files, "/workspace/todos.json") if isinstance(files, dict) else ""
            if todos_txt:
                st.markdown("### æœ¬æ¬¡ Todoï¼ˆå®Œæ•´ï¼‰")
                st.code(todos_txt[:20000], language="json")
                st.divider()

            render_debug_panel(files)

    st.session_state.chat_history.append({"role": "assistant", "kind": "text", "content": answer_text, "meta": meta})
