# -*- coding: utf-8 -*-
# æœƒè­°éŒ„éŸ³ â†’ ç›´æ’­é€å­—ï¼‹æ‘˜è¦ï¼ˆSafe UI Writerï¼šæ‰€æœ‰ UI åƒ…ä¸»åŸ·è¡Œç·’æ›´æ–°ï¼‰
# ç®¡ç·šï¼šSTT Producer â†’ï¼ˆä¸¦è¡Œé‹ç®—ï¼‰RefineBatcher + MapBatcher â†’ ä¸»åŸ·è¡Œç·’è¼ªè©¢å›å¡« â†’ æœ€å¾Œ Reduce

import os, re, json, hashlib, tempfile, multiprocessing, time, concurrent.futures
from typing import List, Dict, Any, Optional

import streamlit as st
from openai import OpenAI
from pydub import AudioSegment, silence
from pydub.utils import which

# ========== åŸºæœ¬è¨­å®š ==========
st.set_page_config(page_title="æœƒè­°éŒ„éŸ³ â†’ ç›´æ’­é€å­—ï¼‹æ‘˜è¦", page_icon="ğŸ“", layout="wide")

st.markdown("""
<style>
:root { --brand:#9c2b2f; --brand-weak:#9c2b2fcc; --bg:#FFF6F6; --border:#f2d9d9; }
.main .block-container{padding-top:2.2rem}
.pink-card{background:var(--bg);border:1px solid var(--border);padding:16px 22px;border-radius:12px;margin-bottom:12px;overflow:visible;}
.header-pill{display:flex;align-items:center;gap:12px;font-size:22px;font-weight:700;color:#2f2f2f;line-height:1.35;min-height:48px;}
.header-pill .emoji{font-size:22px;display:inline-block;transform:translateY(1px);}
.stTabs [data-baseweb="tab-list"]{gap:24px;border-bottom:1px solid #f0e2e2;margin-bottom:8px}
.stTabs [data-baseweb="tab"]{padding:10px 2px;color:var(--brand-weak);font-weight:600}
.stTabs [aria-selected="true"]{color:var(--brand);border-bottom:3px solid var(--brand)}
.stMarkdown p{line-height:1.8}
</style>
""", unsafe_allow_html=True)
st.markdown('<div class="pink-card header-pill"><span class="emoji">âœï¸</span> å®‰å¦®äºé–‹æœƒä¸æ¼æ¥ï¼šé€å­— Ã— æ‘˜è¦</div>', unsafe_allow_html=True)

# æª¢æŸ¥ FFmpeg
AudioSegment.converter = which("ffmpeg")
AudioSegment.ffprobe = which("ffprobe")
if not AudioSegment.converter or not AudioSegment.ffprobe:
    st.error("æ‰¾ä¸åˆ° ffmpeg/ffprobeï¼Œè«‹å…ˆæ–¼ç³»çµ±å®‰è£å¾Œå†è©¦ã€‚")
    st.stop()

# Key
OPENAI_KEY = st.secrets.get("OPENAI_KEY", os.getenv("OPENAI_API_KEY"))
if not OPENAI_KEY:
    st.error("æ‰¾ä¸åˆ° API Keyï¼Œè«‹åœ¨ Streamlit Secrets è¨­å®š OPENAI_KEY æˆ–ç’°å¢ƒè®Šæ•¸ OPENAI_API_KEYã€‚")
    st.stop()
client = OpenAI(api_key=OPENAI_KEY)

# ========== åƒæ•¸ ==========
MODEL_STT    = "gpt-4o-mini-transcribe"
MODEL_MAP    = "gpt-5-mini"
MODEL_REDUCE = "gpt-4.1"

# åˆ‡æ®µåƒæ•¸
MIN_SILENCE_LEN_MS = 700
KEEP_SILENCE_MS    = 300
SILENCE_DB_OFFSET  = 16
OVERLAP_MS         = 1200
MAX_CHUNK_MS       = 30_000
MIN_CHUNK_MS       = 2_000
FALLBACK_WINDOW_MS = 20_000

# ä¸¦è¡Œé‹ç®—æ± ï¼ˆåªåšé‹ç®—ï¼Œä¸ç¢° UIï¼‰
MAX_WORKERS_REFINE = min(2, multiprocessing.cpu_count())
MAX_WORKERS_MAP    = min(2, multiprocessing.cpu_count())

# å¾®æ‰¹æ¬¡ï¼ˆ45~60 åˆ†é˜å»ºè­°å€¼ï¼Œå¯åœ¨ UI èª¿ï¼‰
REFINE_MAX_LINES = 80
REFINE_MAX_CHARS = 6000
REFINE_MAX_WAIT_S = 0.35

MAP_MAX_LINES = 30
MAP_MAX_CHARS = 4000
MAP_MAX_WAIT_S = 0.35

DEFAULT_MAP_CHUNK_SIZE = 40

CACHE_DIR = ".stt_cache"
os.makedirs(CACHE_DIR, exist_ok=True)

# ========== å·¥å…· ==========
def _hash_bytes(b: bytes) -> str:
    return hashlib.md5(b).hexdigest()

def cache_get_text(key: str) -> Optional[str]:
    path = os.path.join(CACHE_DIR, key + ".txt")
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    return None

def cache_set_text(key: str, value: str):
    path = os.path.join(CACHE_DIR, key + ".txt")
    with open(path, "w", encoding="utf-8") as f:
        f.write(value)

def convert_to_wav(input_path: str, output_path: str, target_sr=16000):
    audio = AudioSegment.from_file(input_path)
    audio = audio.set_frame_rate(target_sr).set_channels(1)
    audio.export(output_path, format="wav")
    return output_path

def normalize_loudness(audio: AudioSegment, target_dbfs: float = -20.0) -> AudioSegment:
    gain = target_dbfs - audio.dBFS
    return audio.apply_gain(gain)

def trim_leading_silence(audio: AudioSegment, silence_threshold_db: float = -30.0, chunk_ms: int = 10) -> AudioSegment:
    trim_ms = 0
    while trim_ms < len(audio) and audio[trim_ms:trim_ms+chunk_ms].dBFS < silence_threshold_db:
        trim_ms += chunk_ms
    return audio[trim_ms:]

def apply_filters(audio: AudioSegment, use_high_pass=False, hp_hz=100, use_low_pass=False, lp_hz=9500) -> AudioSegment:
    out = audio
    if use_high_pass: out = out.high_pass_filter(hp_hz)
    if use_low_pass:  out = out.low_pass_filter(lp_hz)
    return out

def split_audio_on_silence_safe(audio: AudioSegment) -> List[AudioSegment]:
    silence_thresh = audio.dBFS - SILENCE_DB_OFFSET
    raw_chunks = silence.split_on_silence(audio, min_silence_len=MIN_SILENCE_LEN_MS,
                                          silence_thresh=silence_thresh, keep_silence=KEEP_SILENCE_MS)
    if not raw_chunks:
        chunks, i = [], 0
        while i < len(audio):
            end = min(i + FALLBACK_WINDOW_MS, len(audio))
            chunks.append(audio[i:end]); i = end
    else:
        filtered = []
        for c in raw_chunks:
            if len(c) < 250:
                if filtered: filtered[-1] = filtered[-1] + c
                else:        filtered.append(c)
            else:
                filtered.append(c)
        if not filtered: filtered = raw_chunks

        chunks = []
        for i, c in enumerate(filtered):
            if i == 0: chunks.append(c)
            else:
                prev = filtered[i-1]
                safe_overlap = min(OVERLAP_MS, len(prev))
                chunks.append((prev[-safe_overlap:] if safe_overlap>0 else AudioSegment.silent(duration=0)) + c)

    normalized = []
    for seg in chunks:
        if len(seg) <= MAX_CHUNK_MS:
            normalized.append(seg)
        else:
            start = 0
            while start < len(seg):
                end = min(start + MAX_CHUNK_MS, len(seg))
                normalized.append(seg[start:end]); start = end

    final_chunks = []
    for seg in normalized:
        if final_chunks and len(seg) < MIN_CHUNK_MS:
            final_chunks[-1] = final_chunks[-1] + seg
        else:
            final_chunks.append(seg)
    return final_chunks

def split_sentences(text: str) -> List[str]:
    parts = re.split(r'([ã€‚ï¼ï¼Ÿï¼›;.!?\n])', text)
    result = []
    for i in range(0, len(parts)-1, 2):
        s = (parts[i] + parts[i+1]).strip()
        if s: result.append(s)
    if len(parts) % 2 != 0:
        tail = parts[-1].strip()
        if tail: result.append(tail)
    return result

# ====== å»é‡ï¼ˆç›¸é„°è¦–çª— + Jaccard trigramï¼‰======
def _norm_for_dedupe(s: str) -> str:
    s = s.strip().lower()
    s = re.sub(r'\s+', '', s)
    s = (s.replace('ï¼Œ', ',').replace('ã€‚', '.')
         .replace('ï¼', '!').replace('ï¼Ÿ', '?')
         .replace('ï¼›', ';').replace('ï¼ˆ', '(').replace('ï¼‰', ')'))
    return s

def _jaccard_trigram(a: str, b: str) -> float:
    n = 3
    if len(a) < n or len(b) < n: return 1.0 if a == b else 0.0
    A = {a[i:i+n] for i in range(len(a)-n+1)}
    B = {b[i:i+n] for i in range(len(b)-n+1)}
    un = len(A | B)
    return (len(A & B) / un) if un else 0.0

def dedupe_against_prev_fast(curr: List[str], prev: List[str], threshold=0.88, max_prev=12) -> List[str]:
    if not curr: return []
    tail = prev[-max_prev:] if prev else []
    tail_norm = [_norm_for_dedupe(p) for p in tail]
    out = []
    for s in curr:
        ns = _norm_for_dedupe(s)
        if ns in tail_norm: continue
        similar = False
        for pn in tail_norm:
            if not pn: continue
            if abs(len(ns)-len(pn)) > int(max(len(ns), len(pn)) * 0.4): continue
            if _jaccard_trigram(ns, pn) >= threshold: similar = True; break
        if not similar: out.append(s)
    return out

def add_cjk_spacing(text: str) -> str:
    text = re.sub(r'([\u4e00-\u9fff])([A-Za-z0-9%#@&])', r'\1 \2', text)
    text = re.sub(r'([A-Za-z0-9%#@&])([\u4e00-\u9fff])', r'\1 \2', text)
    return text

def normalize_symbols(text: str) -> str:
    text = text.replace("ï¼…", "%").replace("ï¼„", "$").replace("â€“", "-").replace("â€”", "-")
    text = text.replace("\u200b", "").replace("\u200c", "")
    return text

def pretty_format_sentences(sentences: List[str]) -> List[str]:
    return [normalize_symbols(add_cjk_spacing(s)) for s in sentences]

# ====== LLM å‘¼å«ï¼ˆæ½¤é£¾ã€Mapã€Reduceï¼‰======
def refine_zh_tw_via_prompt(lines: List[str]) -> List[str]:
    if not lines: return lines
    SEP = "\u241E"; MAX_BATCH_CHARS = 9000; MAX_BATCH_LINES = 120
    def _refine_batch(batch: List[str]) -> List[str]:
        blob = SEP.join(batch)
        dev_msg = (
            "ä½ å°‡æ”¶åˆ°å¤šè¡Œé€å­—ç¨¿ï¼Œè«‹é€è¡Œã€æ½¤é£¾ï¼‹å¿…è¦æ™‚ç¿»è­¯ã€ç‚ºæ­£é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰ã€‚\n"
            "1) ä¸å¯æé€ ï¼›2) è‹±æ–‡æ”¹ç¹é«”ï¼›3) ä¸åˆä½µ/ä¸æ‹†åˆ†ï¼›4) ä¿ç•™æ•¸å­—èˆ‡ç¬¦è™Ÿï¼›"
            "5) ç”¨å°ç£ç”¨èªï¼›6) ä½¿ç”¨ â åˆ†éš”ä¸¦ç¶­æŒè¡Œæ•¸ã€‚åªè¼¸å‡ºæ–‡æœ¬ï¼Œä¸è¦è§£é‡‹ã€‚"
        )
        try:
            resp = client.responses.create(
                model=MODEL_REDUCE,
                input=[
                    {"role":"developer","content":[{"type":"input_text","text":dev_msg}]},
                    {"role":"user","content":[{"type":"input_text","text":blob}]},
                ],
                text={"format":{"type":"text"}},
                tools=[],
            )
            out = (resp.output_text or "").rstrip("\n")
            out_lines = out.split(SEP) if SEP in out else out.split("\n")
            return out_lines if len(out_lines)==len(batch) else batch
        except Exception:
            return batch

    refined_all, batch, size = [], [], 0
    for s in lines:
        if (len(batch) >= MAX_BATCH_LINES) or (size + len(s) + 1 > MAX_BATCH_CHARS):
            refined_all.extend(_refine_batch(batch)); batch, size = [], 0
        batch.append(s); size += len(s) + 1
    if batch: refined_all.extend(_refine_batch(batch))
    return refined_all if refined_all else lines

def map_once_to_bullets(lines: List[str]) -> str:
    part = "\n".join(lines)
    dev_msg = (
        "ä½ æ˜¯ä¸€ä½æœƒè­°è¨˜éŒ„å°å¹«æ‰‹ï¼Œè«‹å°‡ä¸‹åˆ—é€å­—ç¨¿æ•´ç†ç‚ºæ¢åˆ—å¼é‡é»ï¼ˆç¹é«”ä¸­æ–‡ï¼‰ã€‚"
        "è¦æ±‚ï¼šå…·é«”ã€é¿å…ç©ºæ³›ï¼›è‹¥æœ‰æ±ºç­–/é¢¨éšª/æœªæ±º/è¡Œå‹•é …ç›®è«‹æ¨™ç¤ºï¼›åªè¼¸å‡ºæ¢åˆ—ã€‚"
    )
    try:
        resp = client.responses.create(
            model=MODEL_MAP,
            input=[
                {"role":"developer","content":[{"type":"input_text","text":dev_msg}]},
                {"role":"user","content":[{"type":"input_text","text":part}]},
            ],
            text={"format":{"type":"text"}},
            tools=[],
        )
        return (resp.output_text or "").strip()
    except Exception as e:
        return f"- ã€API æ‘˜è¦å¤±æ•—ï¼š{e}ã€‘"

def reduce_finalize_markdown(map_blocks: List[str]) -> str:
    dev_msg = (
        "ä½ æ˜¯æœƒè­°è¨˜éŒ„ç¸½æ•´å°ˆå®¶ã€‚è«‹å°‡å¤šå€‹åˆ†æ®µæ‘˜è¦æ•´ä½µç‚ºã€å–®ä¸€ä»½æœ€çµ‚æœƒè­°ç´€éŒ„ï¼ˆMarkdownï¼‰ã€ã€‚\n"
        "1) åƒ…ä¾ä¾†æºï¼›2) é–‹é ­ 3~6 å¥ç¸½çµï¼›3) å¤šå€‹ ## ä¸»é¡Œï¼›4) ä»¥ã€æ±ºç­–ï¼šã€ã€æœªæ±ºï¼šã€ã€é¢¨éšªï¼šã€æ¨™ç¤ºï¼›"
        "5) åªè¼¸å‡º Markdownã€‚"
        "\n\n=== åˆ†æ®µæ‘˜è¦ ===\n" + "\n\n".join(f"[Part {i+1}]\n{blk}" for i, blk in enumerate(map_blocks))
    )
    try:
        resp = client.responses.create(
            model=MODEL_REDUCE,
            input=[{"role":"developer","content":[{"type":"input_text","text":dev_msg}]}],
            text={"format":{"type":"text"}},
            tools=[],
        )
        return (resp.output_text or "").strip()
    except Exception as e:
        return f"âš ï¸ ç”Ÿæˆæœƒè­°æ‘˜è¦å¤±æ•—ï¼š{e}"

def reduce_finalize_json(map_blocks: List[str]) -> Dict[str, Any]:
    dev_msg = (
        "ä½ æ˜¯æœƒè­°è¨˜éŒ„ç¸½æ•´å°ˆå®¶ã€‚è«‹å°‡å¤šå€‹åˆ†æ®µæ‘˜è¦åˆä½µæˆçµæ§‹åŒ– JSONï¼ŒåŒ…å«ï¼š\n"
        "- metadata, topics[], decisions[], risks[], open_questions[], action_items[], overall_summary\n"
        "è¦æ±‚ï¼šä¸å¯æé€ ï¼ŒæœªçŸ¥ç•™ç©ºï¼Œåˆæ³• JSONã€‚\n\n"
        "=== åˆ†æ®µæ‘˜è¦ ===\n" + "\n\n".join(f"[Part {i+1}]\n{blk}" for i, blk in enumerate(map_blocks))
    )
    try:
        resp = client.responses.create(
            model=MODEL_REDUCE,
            input=[{"role":"developer","content":[{"type":"input_text","text":dev_msg}]}],
            text={"format":{"type":"text"}},
            tools=[],
        )
        s = (resp.output_text or "").strip()
        start, end = s.find("{"), s.rfind("}")
        if start != -1 and end != -1: return json.loads(s[start:end+1])
    except Exception as e:
        return {"overall_summary": f"è§£æ JSON å¤±æ•—ï¼š{e}"}
    return {"overall_summary": "è§£æ JSON å¤±æ•—ï¼ˆæœªçŸ¥åŸå› ï¼‰"}

# ====== æ®µè½åˆ†çµ„ï¼ˆæ¸²æŸ“ç”¨ï¼‰======
def group_into_paragraphs(sentences: List[str], max_chars=260, max_sents=4) -> List[str]:
    paras, cur, length = [], [], 0
    for s in sentences:
        s = s.strip()
        if not s: continue
        if cur and (len(cur) >= max_sents or length + len(s) > max_chars):
            paras.append(" ".join(cur)); cur, length = [s], len(s)
        else:
            cur.append(s); length += len(s)
    if cur: paras.append(" ".join(cur))
    return paras

# ====== æ–°å¢ï¼štopics æ­£è¦åŒ–ï¼Œé¿å… 'str' æ²’æœ‰ get çš„éŒ¯ ======
def normalize_topics(raw) -> List[Dict[str, Any]]:
    if not raw:
        return []
    # çµ±ä¸€æˆ list
    if isinstance(raw, (str, int, float)):
        raw = [raw]
    if isinstance(raw, dict):
        raw = [raw]

    out: List[Dict[str, Any]] = []
    for item in raw:
        if isinstance(item, dict):
            out.append({
                "title": item.get("title") or item.get("topic") or item.get("name") or "ä¸»é¡Œ",
                "key_points": item.get("key_points") or [],
                "decisions": item.get("decisions") or [],
                "risks": item.get("risks") or [],
                "open_questions": item.get("open_questions") or [],
            })
        else:
            out.append({
                "title": str(item),
                "key_points": [],
                "decisions": [],
                "risks": [],
                "open_questions": [],
            })
    return out

# ========== æ‰¹æ¬¡å™¨ï¼ˆåªåšé‹ç®—ï¼Œä¸ç¢° UIï¼›ä¸»åŸ·è¡Œç·’è² è²¬ poll èˆ‡ renderï¼‰==========
class RefineBatcher:
    def __init__(self, executor: concurrent.futures.ThreadPoolExecutor,
                 max_lines=80, max_chars=6000, max_wait_s=0.35):
        self.exec = executor
        self.max_lines, self.max_chars, self.max_wait_s = max_lines, max_chars, max_wait_s
        self.grouped_tail: List[str] = []  # ç›¸é„°è¦–çª—ç”¨
        self.batch: List[str] = []
        self.batch_chars = 0
        self.last_flush = time.monotonic()
        self.pending: Dict[int, concurrent.futures.Future] = {}
        self.buffer: Dict[int, List[str]] = {}
        self.next_emit = 0
        self.seq = 0
        self.refined_lines_all: List[str] = []  # ä¸»åŸ·è¡Œç·’æ”¶é›†å¾Œæ¸²æŸ“
        self.raw_lines_all: List[str] = []      # å¯ä½œç‚ºæ‘˜è¦è¼¸å…¥
        self.done_chunks = 0
        self.total_chunks = 0

    def set_total(self, n: int):
        self.total_chunks = n

    def add_sentences(self, sents: List[str]):
        # ç›¸é„°è¦–çª—å»é‡ â†’ pretty
        unique = sents if not self.grouped_tail else dedupe_against_prev_fast(
            sents, self.grouped_tail, threshold=0.88, max_prev=12
        )
        self.grouped_tail = unique
        pretty = pretty_format_sentences(unique)
        self.raw_lines_all.extend(pretty)

        now = time.monotonic()
        timeup = (now - self.last_flush) >= self.max_wait_s
        for s in pretty:
            if not s.strip(): continue
            if (len(self.batch) >= self.max_lines) or (self.batch_chars + len(s) > self.max_chars) or timeup:
                self._submit_current_batch()
                timeup = False
            self.batch.append(s); self.batch_chars += len(s)
        self.done_chunks += 1

    def _submit_current_batch(self):
        if not self.batch: return
        bid = self.seq; payload = self.batch[:]
        self.buffer[bid] = payload
        self.pending[bid] = self.exec.submit(refine_zh_tw_via_prompt, payload)
        self.seq += 1
        self.batch, self.batch_chars = [], 0
        self.last_flush = time.monotonic()

    def poll_emit(self) -> bool:
        # ä¸»åŸ·è¡Œç·’å‘¼å«ï¼šæŠŠå·²å®Œæˆçš„æ‰¹æ¬¡ä¾åºå–å›
        updated = False
        while self.next_emit in self.pending and self.pending[self.next_emit].done():
            fut = self.pending.pop(self.next_emit)
            try:
                refined = fut.result()
            except Exception:
                refined = self.buffer.get(self.next_emit, [])
            self.refined_lines_all.extend(refined)
            self.buffer.pop(self.next_emit, None)
            self.next_emit += 1
            updated = True
        return updated

    def flush(self):
        # STT çµæŸå¾Œï¼šé€å‡ºæ®˜æ‰¹
        self._submit_current_batch()

    def is_all_done(self) -> bool:
        return (not self.pending) and (not self.batch)

class MapBatcher:
    def __init__(self, executor: concurrent.futures.ThreadPoolExecutor,
                 max_lines=30, max_chars=4000, max_wait_s=0.35):
        self.exec = executor
        self.max_lines, self.max_chars, self.max_wait_s = max_lines, max_chars, max_wait_s
        self.batch: List[str] = []
        self.batch_chars = 0
        self.last_flush = time.monotonic()
        self.pending: Dict[int, concurrent.futures.Future] = {}
        self.buffer: Dict[int, List[str]] = {}
        self.blocks: List[str] = []
        self.next_emit = 0
        self.seq = 0
        self.done_chunks = 0
        self.total_chunks = 0

    def set_total(self, n: int):
        self.total_chunks = n

    def add_sentences(self, sents: List[str]):
        # Map å¯ç›´æ¥ç”¨ prettyï¼ˆä¸ä¸€å®šè¦å»é‡ï¼Œè‹¥è¦çœæˆæœ¬å¯ä¹Ÿé¤µå»é‡å¾Œçš„ï¼‰
        pretty = pretty_format_sentences(sents)
        now = time.monotonic()
        timeup = (now - self.last_flush) >= self.max_wait_s
        for s in pretty:
            if not s.strip(): continue
            if (len(self.batch) >= self.max_lines) or (self.batch_chars + len(s) > self.max_chars) or timeup:
                self._submit_current_batch()
                timeup = False
            self.batch.append(s); self.batch_chars += len(s)
        self.done_chunks += 1

    def _submit_current_batch(self):
        if not self.batch: return
        bid = self.seq; payload = self.batch[:]
        def _task(lines: List[str], idx: int) -> str:
            content = map_once_to_bullets(lines)
            return f"### å³æ™‚é‡é» Part {idx+1}\n\n{content}"
        self.buffer[bid] = payload
        self.pending[bid] = self.exec.submit(_task, payload, bid)
        self.seq += 1
        self.batch, self.batch_chars = [], 0
        self.last_flush = time.monotonic()

    def poll_emit(self) -> bool:
        updated = False
        while self.next_emit in self.pending and self.pending[self.next_emit].done():
            fut = self.pending.pop(self.next_emit)
            try:
                md = fut.result()
            except Exception:
                md = "ï¼ˆæœ¬æ‰¹æ‘˜è¦å›å‚³å¤±æ•—ï¼‰"
            self.blocks.append(md)
            self.buffer.pop(self.next_emit, None)
            self.next_emit += 1
            updated = True
        return updated

    def flush(self):
        self._submit_current_batch()

    def is_all_done(self) -> bool:
        return (not self.pending) and (not self.batch)

# ========== STT ä¸»è¿´åœˆï¼ˆä¸»åŸ·è¡Œç·’è² è²¬æ‰€æœ‰ UIï¼‰==========
def build_prompt(prev_text: str, glossary: str, style_seed: str, max_tokens: int = 220) -> str:
    parts = ["è«‹å…¨ç¨‹ä½¿ç”¨æ­£é«”ä¸­æ–‡ï¼ˆç¹é«”ï¼Œå°ç£ç”¨èªï¼‰ã€‚"]
    if style_seed.strip(): parts.append(style_seed.strip())
    if glossary.strip():
        words = [w.strip() for w in glossary.splitlines() if w.strip()]
        if words: parts.append("Glossary: " + ", ".join(words))
    if prev_text.strip():
        tail = prev_text.strip()[-1200:] if len(prev_text) > 1200 else prev_text.strip()
        parts.append(tail)
    toks = " ".join(parts).split()
    if len(toks) > max_tokens: parts = [" ".join(toks[-max_tokens:])]
    return "\n".join(parts).strip()

def stream_transcribe_all(chunks: List[AudioSegment], container, progress_bar,
                          use_prompting=False, glossary="", style_seed="") -> str:
    all_text, rolling_context = "", ""
    last_flush = 0.0
    FLUSH_INTERVAL = 0.15
    for i, chunk in enumerate(chunks):
        chunk_hash = _hash_bytes(chunk.raw_data)
        cache_key = f"stt_{MODEL_STT}_{chunk_hash}"
        cached = cache_get_text(cache_key)
        if cached:
            all_text += cached + "\n"
            rolling_context = (rolling_context + " " + cached).strip()[-5000:]
            progress_bar.progress((i + 1) / len(chunks))
            container.markdown(all_text)
            yield cached  # æŠŠé€™æ®µæ–‡æœ¬å›å‚³çµ¦ä¸Šå±¤é‚è¼¯
            continue

        full_text = ""
        tmp_path = None
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                tmp_path = tmp.name
                chunk.export(tmp_path, format="wav", parameters=["-ac","1","-ar","16000"])
            with open(tmp_path, "rb") as audio_file:
                extra = {}
                if use_prompting:
                    prompt_str = build_prompt(rolling_context, glossary, style_seed, max_tokens=220)
                    if prompt_str: extra["prompt"] = prompt_str
                try:
                    stream = client.audio.transcriptions.create(
                        model=MODEL_STT,
                        file=audio_file,
                        response_format="text",
                        prompt=("This audio contains a discussion or presentation. Preserve original language per sentence."),
                        stream=True,
                        **extra
                    )
                except Exception:
                    try:
                        stream = client.audio.transcriptions.create(
                            model=MODEL_STT,
                            file=audio_file,
                            response_format="text",
                            prompt=("This audio contains a discussion or presentation. Preserve original language per sentence."),
                            stream=True
                        )
                        container.warning("æ­¤è½‰éŒ„ç«¯é»ä¸æ”¯æ´ promptï¼Œå¼•å°å·²è‡ªå‹•åœç”¨ï¼ˆæœ¬æ¬¡ï¼‰ã€‚")
                    except Exception as e2:
                        container.error(f"API è½‰éŒ„å¤±æ•—ï¼š{e2}")
                        stream = None

                if stream is not None:
                    for event in stream:
                        delta = getattr(event, "delta", None)
                        final_text = getattr(event, "text", None)
                        if delta:
                            full_text += delta
                            now = time.time()
                            if now - last_flush > FLUSH_INTERVAL:
                                container.markdown(all_text + full_text)
                                last_flush = now
                        elif final_text:
                            full_text = final_text
                            container.markdown(all_text + full_text)
        finally:
            if tmp_path:
                try: os.remove(tmp_path)
                except Exception: pass

        txt = full_text.strip()
        cache_set_text(cache_key, txt)
        all_text += txt + "\n"
        rolling_context = (rolling_context + " " + txt).strip()[-5000:]
        progress_bar.progress((i + 1) / len(chunks))
        container.markdown(all_text)
        yield txt  # å›å‚³æœ¬æ®µæ–‡æœ¬
    # è¿´åœˆçµæŸ
    return

# ========== UIï¼šä¸Šå‚³èˆ‡é€²éšèª¿æ•´ ==========
with st.expander("ä¸Šå‚³æœƒè­°éŒ„éŸ³æª”æ¡ˆ", expanded=True):
    f = st.file_uploader("è«‹ä¸Šå‚³éŸ³æª”ï¼ˆ.wav, .mp3, .m4a, .mp4, .webmï¼‰", type=["wav","mp3","m4a","mp4","webm"])
    start_btn = st.button("é–‹å§‹ Streaming è½‰éŒ„èˆ‡æ‘˜è¦")

with st.expander("é€²éšèª¿æ•´ï¼ˆå…¨éƒ¨è¨­å®šï¼Œå¯é¸ï¼‰", expanded=False):
    st.caption("å³æ™‚æ€§â†‘ï¼šæŠŠç­‰å¾…æ™‚é–“/å–®æ‰¹è¡Œæ•¸èª¿å°ï¼›çœæˆæœ¬â†‘ï¼šåä¹‹ã€‚")
    cols = st.columns(2)
    with cols[0]:
        do_trim_leading = st.checkbox("å»å‰å°éœéŸ³ï¼ˆå»ºè­°é–‹ï¼‰", True)
        do_normalize    = st.checkbox("éŸ³é‡æ­£è¦åŒ–åˆ° -20 dBFSï¼ˆå»ºè­°é–‹ï¼‰", True)
        use_high_pass   = st.checkbox("é«˜é€šæ¿¾æ³¢ï¼ˆé™ä½é »å™ªï¼‰", False)
        hp_hz           = st.slider("é«˜é€šæˆªæ­¢é »ç‡ (Hz)", 60, 300, 100, 10, disabled=not use_high_pass)
        use_low_pass    = st.checkbox("ä½é€šæ¿¾æ³¢ï¼ˆé™é«˜é »å™ªï¼‰", False)
        lp_hz           = st.slider("ä½é€šæˆªæ­¢é »ç‡ (Hz)", 4000, 12000, 9500, 100, disabled=not use_low_pass)
    with cols[1]:
        use_prompting = st.checkbox("STT Prompt å¼•å°ï¼ˆå°ˆæœ‰åè©æ‹¼å¯«æ›´ç©©ï¼‰", False)
        glossary_input = st.text_area("å°ˆæœ‰åè©æ‹¼å¯«ï¼ˆæ¯è¡Œä¸€å€‹ï¼‰", height=100, placeholder="Aimee\nShawn\nBBQ")
        style_seed     = st.text_area("é¢¨æ ¼ç¤ºä¾‹ï¼ˆéæŒ‡ä»¤ï¼‰", height=60, placeholder="ä¾‹ï¼šç”¨è©ç²¾ç°¡ï¼Œä¸€è‡´æ¨™é»ã€‚")

    st.markdown("###### ä¸¦è¡Œæ½¤é£¾å¾®æ‰¹æ¬¡ï¼ˆå¡è»Šæ€éº¼èª¿ï¼Ÿï¼‰")
    REFINE_MAX_WAIT_S = st.slider("Refine ç­‰å¾…ç§’æ•¸", 0.10, 0.80, REFINE_MAX_WAIT_S, 0.05,
                                  help="è¶Šå°è¶Šå³æ™‚ï¼ˆAPI æ¬¡æ•¸â†‘ï¼‰ï¼›å¡è»Šè«‹å…ˆèª¿å°é€™å€‹ã€‚")
    REFINE_MAX_LINES  = st.slider("Refine å–®æ‰¹æœ€å¤§è¡Œæ•¸", 20, 140, REFINE_MAX_LINES, 5,
                                  help="è¶Šå°è¶Šå³æ™‚ï¼›çœ‹åˆ°æ’éšŠä¹…â†’èª¿å°ã€‚")
    REFINE_MAX_CHARS  = st.slider("Refine å–®æ‰¹æœ€å¤§å­—æ•¸", 2000, 12000, REFINE_MAX_CHARS, 500)
    MAX_WORKERS_REFINE = st.slider("Refine å·¥äººæ•¸ï¼ˆ1~2ï¼‰", 1, 2, MAX_WORKERS_REFINE, 1,
                                   help="1 èµ·æ­¥ï¼›å¡è»Šå†é–‹åˆ° 2ï¼ˆæˆæœ¬â†‘ï¼‰ã€‚")

    st.markdown("###### å³æ™‚ Map å¾®æ‰¹æ¬¡")
    MAP_MAX_WAIT_S = st.slider("Map ç­‰å¾…ç§’æ•¸", 0.10, 0.80, MAP_MAX_WAIT_S, 0.05)
    MAP_MAX_LINES  = st.slider("Map å–®æ‰¹æœ€å¤§è¡Œæ•¸", 10, 80, MAP_MAX_LINES, 2)
    MAP_MAX_CHARS  = st.slider("Map å–®æ‰¹æœ€å¤§å­—æ•¸", 1000, 10000, MAP_MAX_CHARS, 250)
    MAX_WORKERS_MAP = st.slider("Map å·¥äººæ•¸ï¼ˆ1~2ï¼‰", 1, 2, MAX_WORKERS_MAP, 1)

if not (f and start_btn):
    st.stop()

# ========== ä¸»æµç¨‹ ==========
raw_bytes = f.read()
st.audio(raw_bytes)
tab1, tab2, tab3, tab4 = st.tabs(["è½‰éŒ„çµæœ", "é‡é»æ‘˜è¦", "å…§å®¹è§£æ", "åŸå§‹å…§å®¹"])

with tab2:
    st.markdown("#### å³æ™‚é‡é»ï¼ˆMap streamingï¼‰")
    map_stream_container = st.empty()
    map_progress = st.progress(0.0)
    st.divider()
    final_summary_placeholder = st.empty()

with tab1:
    with st.status("è™•ç†ä¸­...", expanded=True) as status:
        status.update(label="å„²å­˜èˆ‡è½‰æª”...")
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{f.name.split('.')[-1]}") as temp_input:
            temp_input.write(raw_bytes)
            temp_input_path = temp_input.name

        wav_path = temp_input_path
        if not f.name.lower().endswith(".wav"):
            wav_path = temp_input_path + ".wav"
            convert_to_wav(temp_input_path, wav_path)

        status.update(label="è¼‰å…¥éŸ³æª”èˆ‡å‰è™•ç†...")
        audio = AudioSegment.from_file(wav_path, format="wav")
        if do_trim_leading:
            audio = trim_leading_silence(audio, silence_threshold_db=-30.0, chunk_ms=10)
        if do_normalize:
            audio = normalize_loudness(audio, target_dbfs=-20.0)
        if use_high_pass or use_low_pass:
            audio = apply_filters(audio, use_high_pass=use_high_pass, hp_hz=hp_hz,
                                  use_low_pass=use_low_pass, lp_hz=lp_hz)

        status.update(label="éœéŸ³åˆ‡æ®µï¼ˆæœ‰ä¿è­·ï¼›æ‰¾ä¸åˆ°éœéŸ³å‰‡å›é€€å›ºå®šåˆ‡ï¼‰...")
        chunks = split_audio_on_silence_safe(audio)
        if not chunks:
            st.error("ç„¡æ³•åˆ‡å‡ºæœ‰æ•ˆéŸ³è¨Šæ®µï¼Œè«‹æª¢æŸ¥éŸ³æª”æˆ–èª¿æ•´åƒæ•¸ã€‚")
            st.stop()

        st.markdown("#### è½‰éŒ„çµæœ")
        stream_container = st.empty()
        progress_bar = st.progress(0.0)

        # å»ºç«‹é‹ç®—æ± ï¼ˆåªåš API å‘¼å«èˆ‡é‹ç®—ï¼Œä¸æ›´æ–° UIï¼‰
        refine_pool = concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS_REFINE)
        map_pool    = concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS_MAP)
        refine = RefineBatcher(refine_pool, REFINE_MAX_LINES, REFINE_MAX_CHARS, REFINE_MAX_WAIT_S)
        mapper = MapBatcher(map_pool, MAP_MAX_LINES, MAP_MAX_CHARS, MAP_MAX_WAIT_S)
        refine.set_total(len(chunks)); mapper.set_total(len(chunks))

        status.update(label="é€æ®µ Streaming è½‰éŒ„ä¸­ï¼ˆä¸¦è¡Œé‹ç®—ï¼šæ½¤é£¾ï¼‹å³æ™‚æ‘˜è¦ï¼‰...")
        all_text = ""
        # STT ä¸»è¿´åœˆï¼šæ¯æ®µçµæŸâ†’ä¸Ÿé€²å…©å€‹ batcherï¼›æ¯åœˆè¼ªè©¢å·²å®Œæˆæ‰¹æ¬¡â†’ä¸»åŸ·è¡Œç·’æ›´æ–° UI
        for txt in stream_transcribe_all(chunks, stream_container, progress_bar,
                                         use_prompting=use_prompting,
                                         glossary=glossary_input or "",
                                         style_seed=style_seed or ""):
            all_text += txt + "\n"
            sents = split_sentences(txt)
            refine.add_sentences(sents)
            mapper.add_sentences(sents)

            # è¼ªè©¢å·²å®Œæˆæ‰¹æ¬¡ä¸¦æ¸²æŸ“ï¼ˆåªåœ¨ä¸»åŸ·è¡Œç·’ï¼‰
            updated_refine = refine.poll_emit()
            if updated_refine:
                paras = group_into_paragraphs(refine.refined_lines_all, max_chars=280, max_sents=4)
                stream_container.markdown("\n\n".join(paras))
            if mapper.poll_emit():
                map_stream_container.markdown("\n\n".join(mapper.blocks))

            # é€²åº¦ï¼ˆç”¨è™•ç†æ®µæ•¸è¿‘ä¼¼æ¨é€²ï¼‰
            if refine.total_chunks:
                st.session_state["__refine_prog"] = min(1.0, refine.done_chunks / refine.total_chunks)
            if mapper.total_chunks:
                st.session_state["__map_prog"] = min(1.0, mapper.done_chunks / mapper.total_chunks)
                map_progress.progress(st.session_state["__map_prog"])

        # æ”¶å°¾ï¼šé€å‡ºæ®˜æ‰¹ä¸¦ç­‰å¾…å…¨éƒ¨å®Œæˆï¼ŒæœŸé–“æŒçºŒè¼ªè©¢èˆ‡æ¸²æŸ“
        refine.flush(); mapper.flush()
        t0 = time.monotonic()
        while not (refine.is_all_done() and mapper.is_all_done()):
            any_r = refine.poll_emit()
            any_m = mapper.poll_emit()
            if any_r:
                paras = group_into_paragraphs(refine.refined_lines_all, max_chars=280, max_sents=4)
                stream_container.markdown("\n\n".join(paras))
            if any_m:
                map_stream_container.markdown("\n\n".join(mapper.blocks))
            if not (any_r or any_m):
                time.sleep(0.05)
            # ç°¡å–®è¶…æ™‚ä¿è­·ï¼šé¿å…æ¥µç«¯é˜»å¡ï¼ˆç™¼ç”Ÿæ™‚ä¿ç•™å·²å®Œæˆå…§å®¹ï¼‰
            if time.monotonic() - t0 > 120:
                st.warning("èƒŒæ™¯æ‰¹æ¬¡ç­‰å¾…é€¾æ™‚ï¼Œå·²é¡¯ç¤ºç›®å‰å®Œæˆçš„å…§å®¹ã€‚")
                break

        refine_pool.shutdown(wait=False)
        map_pool.shutdown(wait=False)

        # æœ€çµ‚å¯è®€å…§å®¹
        refined_lines = refine.refined_lines_all if refine.refined_lines_all else refine.raw_lines_all
        paras = group_into_paragraphs(refined_lines, max_chars=280, max_sents=4)
        stream_container.markdown("\n\n".join(paras))
        st.success("Transcription + Refine complete!")

        status.update(label="æ•´ä½µé‡é»ï¼ˆReduce ä¸­ï¼‰...")
        map_blocks_text = mapper.blocks[:] if mapper.blocks else [map_once_to_bullets(refine.raw_lines_all)]
        final_md_summary = reduce_finalize_markdown(map_blocks_text)
        final_summary_placeholder.markdown(final_md_summary)
        final_minutes = reduce_finalize_json(map_blocks_text)

        # é¡å‹ä¿è­·ï¼šç¢ºä¿ final_minutes è‡³å°‘æ˜¯ dictï¼Œé¿å…å¾ŒçºŒ .get çˆ†ç‚¸
        if not isinstance(final_minutes, dict):
            final_minutes = {"overall_summary": str(final_minutes)}

        with tab3:
            st.markdown("#### ä¸»é¡Œ")
            # å…ˆæ­£è¦åŒ– topicsï¼Œé¿å… 'str' æ²’æœ‰ get çš„éŒ¯
            topics_raw = (final_minutes or {}).get("topics", [])
            topics = normalize_topics(topics_raw)

            # é™¤éŒ¯é–‹é—œï¼šéœ€è¦æ™‚å¯æª¢è¦–åŸå§‹ JSON çµæ§‹
            if st.checkbox("é¡¯ç¤º final_minutesï¼ˆdebugï¼‰", False):
                st.json(final_minutes)

            for t in topics:
                st.markdown(f"##### {t['title']}")
                if t["key_points"]:     st.markdown("\n".join(f"- {x}" for x in t["key_points"]))
                if t["decisions"]:      st.markdown("æ±ºç­–ï¼š\n" + "\n".join(f"- {x}" for x in t["decisions"]))
                if t["risks"]:          st.markdown("é¢¨éšªï¼š\n" + "\n".join(f"- {x}" for x in t["risks"]))
                if t["open_questions"]: st.markdown("æœªæ±ºå•é¡Œï¼š\n" + "\n".join(f"- {x}" for x in t["open_questions"]))

        with tab4:
            st.markdown("#### åŸå§‹å…§å®¹ï¼ˆæœ€åŸå§‹ä¸²æµè¼¸å‡ºï¼Œæœªåˆ†å¥ï¼æœªå»é‡ï¼‰")
            st.code(all_text.strip(), language="text")

        status.update(label="å…¨éƒ¨å®Œæˆï¼", state="complete", expanded=True)

# æ¸…ç†æš«å­˜
try:
    os.remove(temp_input_path)
    if 'wav_path' in locals() and wav_path != temp_input_path:
        os.remove(wav_path)
except Exception:
    pass
