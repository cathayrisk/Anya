import streamlit as st
import os
import mimetypes
from openai import OpenAI

# ====== åƒæ•¸è¨­å®š ======
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
VECTOR_STORE_NAME = "my_knowledge_base"
ALLOWED_FILE_TYPES = ["txt", "pdf", "jpg", "jpeg", "png", "docx", "pptx", "md"]

# ====== åˆå§‹åŒ– OpenAI ç‰©ä»¶èˆ‡ Vector Store ======
@st.cache_resource
def get_openai_client():
    return OpenAI(api_key=st.secrets["OPENAI_KEY"])

@st.cache_resource
def get_vector_store(_client):
    vector_store = client.vector_stores.create(name=VECTOR_STORE_NAME)
    return vector_store.id

client = get_openai_client()
vector_store_id = get_vector_store(client)

# ====== æª”æ¡ˆä¸Šå‚³åˆ° Vector Store ======
def upload_file_to_vector_store(file, client, vector_store_id):
    # å…ˆå­˜åˆ°æœ¬åœ°æš«å­˜
    temp_path = f"temp_{file.name}"
    with open(temp_path, "wb") as f:
        f.write(file.read())
    # ä¸Šå‚³åˆ° OpenAI
    file_resp = client.files.create(file=open(temp_path, "rb"), purpose="assistants")
    client.vector_stores.files.create(
        vector_store_id=vector_store_id,
        file_id=file_resp.id
    )
    os.remove(temp_path)
    return file_resp.id

# ====== å¤šæ¨¡æ…‹æŸ¥è©¢ ======
def multimodal_query(client, vector_store_id, user_text=None, image_file=None):
    input_content = []
    if user_text:
        input_content.append({"type": "input_text", "text": user_text})
    if image_file:
        # Streamlit çš„ UploadedFile ç›´æ¥å­˜æˆæœ¬åœ°æª”æ¡ˆ
        temp_img_path = f"temp_{image_file.name}"
        with open(temp_img_path, "wb") as f:
            f.write(image_file.read())
        # é€™è£¡ç›´æ¥ç”¨æœ¬åœ°è·¯å¾‘çµ¦ OpenAIï¼ˆå¦‚éœ€ base64 è«‹å†èª¿æ•´ï¼‰
        input_content.append({"type": "input_image", "image_url": temp_img_path})

    params = {
        "model": "gpt-4.1",
        "input": [{"role": "user", "content": input_content}],
        "tools": [{
            "type": "file_search",
            "vector_store_ids": [vector_store_id]
        }]
    }
    response = client.responses.create(**params)
    # æ¸…ç†æš«å­˜åœ–ç‰‡
    if image_file:
        os.remove(temp_img_path)
    return response

# ====== Citation æ ¼å¼åŒ– ======
def format_response(response):
    output = response.output[1].content[0]
    text = output.text
    citations = output.annotations if hasattr(output, "annotations") else []
    for i, cite in enumerate(citations):
        if hasattr(cite, "filename"):
            text += f"\n[ä¾†æº{i+1}: {cite.filename}]"
    return text

# ====== Streamlit UI ======
st.title("ğŸ¦¸â€â™€ï¸ å¤šæ¨¡æ…‹ Responses API Chatbot by å®‰å¦®äº")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Chat UI
for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Chat input
user_input = st.chat_input(
    "è«‹è¼¸å…¥å•é¡Œï¼Œæˆ–ä¸Šå‚³æª”æ¡ˆä¸€èµ·å•æˆ‘å§ï¼",
    accept_file="multiple",
    file_type=ALLOWED_FILE_TYPES,
    key="chat_input"
)

if user_input:
    # 1. è™•ç†æª”æ¡ˆä¸Šå‚³
    uploaded_files = user_input.files if hasattr(user_input, "files") else []
    user_text = user_input.text if hasattr(user_input, "text") else user_input

    # 2. æª”æ¡ˆå…¨éƒ¨ä¸Šå‚³åˆ° Vector Store
    for file in uploaded_files:
        upload_file_to_vector_store(file, client, vector_store_id)

    # 3. å¤šæ¨¡æ…‹æŸ¥è©¢ï¼ˆåªä¸Ÿç¬¬ä¸€å¼µåœ–ç‰‡çµ¦ LLMï¼Œå…¶ä»–éƒ½é€²çŸ¥è­˜åº«ï¼‰
    image_file = None
    for file in uploaded_files:
        mime, _ = mimetypes.guess_type(file.name)
        if mime and mime.startswith("image/"):
            image_file = file
            break

    # 4. å‘¼å« Responses API
    response = multimodal_query(client, vector_store_id, user_text, image_file)

    # 5. é¡¯ç¤ºå°è©±
    st.session_state.chat_history.append({"role": "user", "content": user_text})
    with st.chat_message("user"):
        st.markdown(user_text)

    answer = format_response(response)
    st.session_state.chat_history.append({"role": "assistant", "content": answer})
    with st.chat_message("assistant"):
        st.markdown(answer)
