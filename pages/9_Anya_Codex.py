import streamlit as st
import asyncio
from pydantic import BaseModel
import os
import nest_asyncio
nest_asyncio.apply()

from openai.types.shared.reasoning import Reasoning
from agents import Agent, ModelSettings, WebSearchTool, Runner, handoff

# è¿½åŠ å…¥å¤šæ¨¡æ…‹éœ€è¦çš„å·¥å…·
import base64
from io import BytesIO
from PIL import Image
from openai import OpenAI
import time

# =========================
# åŸºæœ¬ç’°å¢ƒè¨­å®š
# =========================
st.set_page_config(page_title="AI ç ”ç©¶åŠ©ç† Chatï¼ˆå¤šæ¨¡æ…‹ï¼‹æ®µè½æ·¡å…¥ï¼‰", layout="wide", page_icon="ğŸ¤–")
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_KEY"]

def run_async(coro):
    # åœ¨ Streamlit ä¸­å®‰å…¨åœ°è·‘ asyncio å”ç¨‹
    loop = asyncio.get_event_loop()
    return loop.run_until_complete(coro)

# =========================
# æ‰“å­—å‹•ç•«ï¼ˆç„¡åœæ­¢åŠŸèƒ½ã€åªæœ‰ emojiï¼Œç¯€å¥ç¨æ…¢ï¼‰
# =========================
def emoji_token_stream(
    full_text: str,
    emoji: str = "ğŸŒ¸",
    # ç¨å¾®æ”¾æ…¢ä¸€é»é»ï¼ˆåŸæœ¬ 28~140ï¼‰
    min_cps: int = 20,
    max_cps: int = 110,
    short_len: int = 300,
    long_len: int = 1200,
    punctuation_pause: float = 0.50,  # æ¨™é»åœé “ç•¥å¢
    preview_ratio: float = 0.40,      # emoji é è¦½æ¯”é‡ç•¥å¢
    code_speedup: float = 1.8,
    ph=None
):
    """
    åªç”¨ emoji åšçŸ­æš«é è¦½ã€ç„¡æ¸¸æ¨™ã€ç„¡åœæ­¢åŠŸèƒ½ã€‚
    """
    import time

    if not full_text:
        return ""

    # ä½¿ç”¨å­—ç´ å¢é›†åˆ‡åˆ†ï¼Œé¿å…åˆ‡å£ emoji/åˆå­—
    try:
        import regex as re
        tokens = re.findall(r"\X", full_text)
    except Exception:
        tokens = list(full_text)

    n = len(tokens)

    # ä¾é•·åº¦æ’å€¼é€Ÿåº¦
    def lerp(a, b, t): return a + (b - a) * t
    if n <= short_len:
        base_cps = min_cps
    elif n >= long_len:
        base_cps = max_cps
    else:
        t = (n - short_len) / max(1, (long_len - short_len))
        base_cps = lerp(min_cps, max_cps, t)

    per_char_delay = 1.0 / max(1.0, base_cps)

    placeholder = ph or st.empty()

    out = []
    i = 0
    inside_code = False
    punct = set(".!?;:ï¼Œã€‚ï¼ï¼Ÿï¼šã€â€¦\n")

    # å‰æ®µç²¾ç·»ã€å¾Œæ®µåŠ é€Ÿï¼ˆç¨æ…¢ç‰ˆï¼‰
    def chunk_size(idx):
        if inside_code:
            return 8
        if idx < 80:
            return 1
        if idx < 240:
            return 2
        if idx < 900:
            return 3
        return 4

    def render(txt):
        placeholder.markdown(txt)

    while i < n:
        k = min(chunk_size(i), n - i)
        chunk_tokens = tokens[i:i + k]
        chunk_text = "".join(chunk_tokens)
        i += k

        # åµæ¸¬ ``` ç¨‹å¼ç¢¼å€å¡Š
        if "```" in chunk_text:
            flips = chunk_text.count("```")
            if flips % 2 == 1:
                inside_code = not inside_code

        intended = per_char_delay * k
        if inside_code:
            intended = max(intended / code_speedup, 0.002)

        last_char = chunk_tokens[-1]
        if last_char in punct and not inside_code:
            intended += per_char_delay * punctuation_pause

        start_t = time.monotonic()

        # é è¦½ï¼šåªæœ‰ emojiï¼ˆä¸é¡¯ç¤ºæ¸¸æ¨™ï¼‰
        current_text = "".join(out)
        if not inside_code:
            render(current_text + emoji)
            time.sleep(min(intended * preview_ratio, 0.07))

        # æ­£å¼å¯«å…¥
        out.append(chunk_text)
        render("".join(out))

        # å¡«æ»¿å‰©é¤˜æ™‚é–“ï¼Œè®“ç¯€å¥ç©©å®š
        elapsed = time.monotonic() - start_t
        remain = max(0.0, intended - elapsed)
        time.sleep(remain)

    # æ”¶å°¾ï¼Œä¸å¸¶ emoji
    render("".join(out))
    return "".join(out)

# =========================
# æ®µè½æ·¡å…¥ + é€å­—ï¼ˆæ–¹æ¡ˆ2ï¼‰
# =========================
def split_md_paragraphs(md: str):
    parts, buf, in_code = [], [], False
    for line in md.splitlines(keepends=True):
        if line.strip().startswith("```"):
            in_code = not in_code
            buf.append(line)
            continue
        if not in_code and line.strip() == "":
            if buf:
                parts.append("".join(buf).strip("\n")); buf=[]
        else:
            buf.append(line)
    if buf:
        parts.append("".join(buf).strip("\n"))
    return [p for p in parts if p.strip()]

def paragraph_type_with_fade(md_text: str, emoji: str = "ğŸŒ¸", fade_ms: int = 160):
    paragraphs = split_md_paragraphs(md_text)
    for para in paragraphs:
        ph = st.empty()
        # 1) æ®µè½æ·¡å…¥ï¼ˆç°è‰²å¹½éˆï¼‰
        ph.markdown(f":grey[{para}]")
        time.sleep(fade_ms / 1000.0)
        # 2) åŒä¸€å€‹ placeholder é€å­—æ’­æ”¾ï¼ˆæœƒè¦†è“‹ç°è‰²ï¼‰
        emoji_token_stream(para, emoji=emoji, ph=ph)
        st.markdown("")  # æ®µè½é–“è·

# =========================
# è¦åŠƒ Agentï¼ˆPlannerï¼‰
# =========================
planner_agent_PROMPT = (
    "You are a helpful research assistant. Given a query, come up with a set of web searches "
    "to perform to best answer the query. Output between 5 and 20 terms to query for."
)

class WebSearchItem(BaseModel):
    reason: str
    "Your reasoning for why this search is important to the query."
    query: str
    "The search term to use for the web search."

class WebSearchPlan(BaseModel):
    searches: list[WebSearchItem]
    """A list of web searches to perform to best answer the query."""

planner_agent = Agent(
    name="PlannerAgent",
    instructions=planner_agent_PROMPT,
    model="gpt-5",
    model_settings=ModelSettings(reasoning=Reasoning(effort="medium")),
    output_type=WebSearchPlan,
)

# =========================
# æœå°‹ Agentï¼ˆSearchï¼‰
# =========================
INSTRUCTIONS = (
    "You are a research assistant. Given a search term, you search the web for that term and "
    "produce a concise summary of the results. The summary must be 2-3 paragraphs and less than 300 "
    "words. Capture the main points. Write succinctly, no need to have complete sentences or good "
    "grammar. This will be consumed by someone synthesizing a report, so its vital you capture the "
    "essence and ignore any fluff. Do not include any additional commentary other than the summary "
    "itself."
)

search_agent = Agent(
    name="Search agent",
    model="gpt-4.1",
    instructions=INSTRUCTIONS,
    tools=[WebSearchTool()],
    model_settings=ModelSettings(tool_choice="required"),
)

# =========================
# å¯«ä½œ Agentï¼ˆWriterï¼‰
# =========================
writer_agent_PROMPT = (
    "You are a senior researcher tasked with writing a cohesive report for a research query. "
    "You will be provided with the original query, and some initial research done by a research "
    "assistant.\n"
    "You should first come up with an outline for the report that describes the structure and "
    "flow of the report. Then, generate the report and return that as your final output.\n"
    "The final output should be in markdown format, and it should be lengthy and detailed. Aim "
    "for 5-10 pages of content, at least 1000 words."
    "è«‹å‹™å¿…ä»¥æ­£é«”ä¸­æ–‡å›æ‡‰ï¼Œä¸¦éµå¾ªå°ç£ç”¨èªç¿’æ…£"
)

class ReportData(BaseModel):
    short_summary: str
    """A short 2-3 sentence summary of the findings."""
    markdown_report: str
    """The final report"""
    follow_up_questions: list[str]
    """Suggested topics to research further"""

writer_agent = Agent(
    name="WriterAgent",
    instructions=writer_agent_PROMPT,
    model="gpt-5-mini",
    model_settings=ModelSettings(reasoning=Reasoning(effort="medium")),
    output_type=ReportData,
)

# =========================
# Router Agentï¼ˆè‡ªå‹• handoffï¼‰
# =========================
ROUTER_PROMPT = """
ä½ æ˜¯ä¸€å€‹æ™ºæ…§åŠ©ç†ï¼Œæœƒæ ¹æ“šç”¨æˆ¶çš„éœ€æ±‚è‡ªå‹•æ±ºå®šè¦æ€éº¼è™•ç†å•é¡Œã€‚
- å¦‚æœç”¨æˆ¶çš„å•é¡Œæ˜¯ã€Œéœ€è¦ç ”ç©¶ã€æŸ¥è³‡æ–™ã€åˆ†æã€å¯«å ±å‘Šã€æ–‡ç»æ¢è¨ã€ç­‰ï¼Œè«‹ä½¿ç”¨ transfer_to_planner_agent å·¥å…·ï¼ŒæŠŠå•é¡Œäº¤çµ¦ç ”ç©¶è¦åŠƒåŠ©ç†ã€‚
- å¦‚æœåªæ˜¯ä¸€èˆ¬èŠå¤©ã€çŸ¥è­˜å•ç­”ã€é–’èŠï¼Œè«‹ç›´æ¥ç”¨ä½ è‡ªå·±çš„çŸ¥è­˜å›ç­”ã€‚å¿…è¦çš„æ™‚å€™å¯ä»¥ä½¿ç”¨WebSearchToolä¾†æœå°‹ç¶²è·¯è³‡è¨Šã€‚
è«‹æ ¹æ“šç”¨æˆ¶çš„è¼¸å…¥ï¼Œè‡ªè¡Œåˆ¤æ–·è¦ä¸è¦ handoffã€‚
"""

router_agent = Agent(
    name="RouterAgent",
    instructions=ROUTER_PROMPT,
    model="gpt-5",
    tools=[WebSearchTool()],
    model_settings=ModelSettings(
        reasoning=Reasoning(effort="low"),
        verbosity="medium",
    ),
    handoffs=[handoff(planner_agent)]
)

# =========================
# Multimodalï¼šåœ–ç‰‡ç†è§£æ¨¡å¼ï¼ˆæ–°ï¼‰
# =========================
client = OpenAI(api_key=st.secrets["OPENAI_KEY"])

VISION_SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä½å¤šæ¨¡æ…‹åŠ©ç†ã€‚æ”¶åˆ°åœ–ç‰‡èˆ‡ï¼ˆå¯é¸ï¼‰æ–‡å­—æŒ‡ç¤ºæ™‚ï¼š
- å…ˆæè¿°åœ–ç‰‡é—œéµå…§å®¹ï¼ˆç‰©ä»¶ã€æ–‡å­—ã€é—œä¿‚ã€å ´æ™¯ã€ç‰ˆé¢ï¼‰ã€‚
- è‹¥æœ‰å¤šå¼µåœ–ç‰‡ï¼Œè«‹æ¯”è¼ƒå·®ç•°æˆ–å»ºç«‹æ­¥é©Ÿæ¨è«–ã€‚
- é©åº¦çµåˆOCRèˆ‡æ¨ç†ï¼›è‹¥èˆ‡ä½¿ç”¨è€…æå•ç›¸é—œï¼Œæä¾›æ¢åˆ—å¼çµè«–èˆ‡å¯è¡Œå»ºè­°ã€‚
è«‹ä»¥æ­£é«”ä¸­æ–‡ä½œç­”ã€‚
"""

st.title("AI ç ”ç©¶åŠ©ç† Chat ç‰ˆï¼ˆå¤šæ¨¡æ…‹å‡ç´šï¼‰")
st.write("ç”¨å°è©±æ–¹å¼å•ç ”ç©¶å•é¡Œï¼ŒAI æœƒåƒèŠå¤©ä¸€æ¨£å¹«ä½ æŸ¥è³‡æ–™ã€å¯«å ±å‘Šï¼å¦å¤–ä¹Ÿèƒ½ä¸Šå‚³åœ–ç‰‡ï¼Œè«‹ AI å¹«ä½ çœ‹åœ–èªªæ•…äº‹ï½")

with st.expander("ğŸ–¼ï¸ åœ–ç‰‡ç†è§£æ¨¡å¼ï¼ˆå¤šæ¨¡æ…‹ï¼‰", expanded=False):
    col1, col2 = st.columns([3, 2])
    with col1:
        vision_text = st.text_area("ï¼ˆå¯é¸ï¼‰è¼¸å…¥ä½ æƒ³è®“ AI é‡å°åœ–ç‰‡å›ç­”çš„å•é¡Œæˆ–ä»»å‹™", placeholder="ä¾‹å¦‚ï¼šå¹«æˆ‘æ¯”å°é€™å…©å¼µç°¡å ±åœ–çš„å·®ç•°ï¼Œæ•´ç†æˆ3é»é‡é»ã€‚")
    with col2:
        files = st.file_uploader("ä¸Šå‚³ 1ï½6 å¼µåœ–ç‰‡", type=["png", "jpg", "jpeg", "webp"], accept_multiple_files=True)
    if st.button("åˆ†æåœ–ç‰‡", type="primary", use_container_width=True, disabled=not files):
        # æº–å‚™å…§å®¹å€å¡Š
        content_blocks = []
        if vision_text and vision_text.strip():
            content_blocks.append({"type": "input_text", "text": vision_text.strip()})
        imgs_preview = []
        for f in files[:6]:
            imgbytes = f.getvalue()
            mime = f"type" if hasattr(f, "type") and f.type else "image/png"
            b64 = base64.b64encode(imgbytes).decode()
            content_blocks.append({"type": "input_image", "image_url": f"data:{mime};base64,{b64}"})
            imgs_preview.append(imgbytes)

        with st.spinner("å®‰å¦®äºçœ‹åœ–ä¸­â€¦wakuwakuï¼ğŸ¤©"):
            try:
                resp = client.responses.create(
                    model="gpt-5",
                    input=[{"role": "user", "content": content_blocks}],
                    instructions=VISION_SYSTEM_PROMPT,
                    parallel_tool_calls=True,
                    reasoning={"effort": "medium"},
                    text={"verbosity": "medium"},
                    store=False,
                    truncation="auto",
                )
                out_text = ""
                if hasattr(resp, "output") and resp.output:
                    for item in resp.output:
                        if hasattr(item, "content") and item.content:
                            for c in item.content:
                                if getattr(c, "type", None) == "output_text":
                                    out_text += c.text

                if not out_text.strip():
                    out_text = "å®‰å¦®äºçœ‹éäº†ï¼Œä½†æ²’æœ‰è¾¨è­˜åˆ°å¯ä»¥å›ç­”çš„é‡é»ï¼Œèƒ½ä¸èƒ½è£œå……ä¸€ä¸‹ä½ çš„æœŸå¾…å‘¢ï¼Ÿ"

                # é¡¯ç¤ºä¸Šå‚³åœ–ç‰‡é è¦½
                st.markdown("#### åœ–ç‰‡é è¦½")
                st.image([Image.open(BytesIO(x)) for x in imgs_preview], width=260)

                st.markdown("#### è§£æçµæœ")
                paragraph_type_with_fade(out_text, emoji="ğŸŒ¸", fade_ms=140)

            except Exception as e:
                st.error(f"åœ–ç‰‡åˆ†æå¤±æ•—ï¼š{e}")

st.markdown("---")

# =========================
# ä¸»è¦èŠå¤©ä»‹é¢ï¼ˆç ”ç©¶/ä¸€èˆ¬å°è©±ï¼‰
# =========================
# åˆå§‹åŒ–å°è©±æ­·å²
if "messages" not in st.session_state:
    st.session_state.messages = []

# é¡¯ç¤ºæ­·å²è¨Šæ¯
for msg in st.session_state.messages:
    with st.chat_message(msg["role"], avatar=msg.get("avatar")):
        st.markdown(msg["content"])

# èŠå¤©è¼¸å…¥
user_input = st.chat_input("è«‹è¼¸å…¥ä½ æƒ³ç ”ç©¶çš„å•é¡Œæˆ–ç¹¼çºŒè¿½å•...")

if user_input:
    # é¡¯ç¤ºä½¿ç”¨è€…è¨Šæ¯
    st.session_state.messages.append({
        "role": "user",
        "content": user_input,
    })
    with st.chat_message("user"):
        st.markdown(user_input)

    # AI è™•ç†ï¼ˆé¡¯ç¤º spinnerï¼‰
    with st.chat_message("assistant"):
        with st.spinner("AI æ­£åœ¨åŠªåŠ›æ€è€ƒä¸­..."):
            # è®“ Router æ±ºå®šæ˜¯å¦è¦ handoff
            router_result = run_async(Runner.run(router_agent, user_input))

            # è‹¥ handoff åˆ°è¦åŠƒåŠ©ç†ï¼ˆéœ€è¦ç ”ç©¶å·¥ä½œï¼‰
            if isinstance(router_result.final_output, WebSearchPlan):
                # Step 1: è¦åŠƒ
                search_plan = router_result.final_output.searches

                plan_md = "### ğŸ” æœå°‹è¦åŠƒ\n"
                for idx, item in enumerate(search_plan):
                    plan_md += f"**{idx+1}. {item.query}**\n> {item.reason}\n"

                # Step 2: ä¸¦è¡Œæœå°‹
                search_tasks = [
                    Runner.run(search_agent, f"Search term: {item.query}\nReason: {item.reason}")
                    for item in search_plan
                ]
                search_results = run_async(asyncio.gather(*search_tasks))
                summaries = [str(r.final_output) for r in search_results]

                summary_md = "### ğŸ“ å„é …æœå°‹æ‘˜è¦\n"
                for idx, summary in enumerate(summaries):
                    summary_md += f"**{search_plan[idx].query}**\n{summary}\n\n"

                with st.expander("ğŸ” æœå°‹è¦åŠƒèˆ‡å„é …æœå°‹æ‘˜è¦", expanded=True):
                    st.markdown("### æœå°‹è¦åŠƒ")
                    for idx, item in enumerate(search_plan):
                        st.markdown(f"**{idx+1}. {item.query}**\n> {item.reason}")

                    st.markdown("### å„é …æœå°‹æ‘˜è¦")
                    for idx, summary in enumerate(summaries):
                        st.markdown(f"**{search_plan[idx].query}**\n{summary}\n")

                # Step 3: æ•´åˆå¯«ä½œ
                writer_input = f"Original query: {user_input}\nSummarized search results: {summaries}"
                report = run_async(Runner.run(writer_agent, writer_input))

                st.markdown("### ğŸ“‹ Executive Summary")
                # çŸ­æ‘˜è¦ä¿ç•™ç´”é€å­—
                emoji_token_stream(report.final_output.short_summary, emoji="ğŸŒŸ")

                st.markdown("### ğŸ“– å®Œæ•´å ±å‘Š")
                # é•·æ–‡æ”¹ç”¨ã€Œæ®µè½æ·¡å…¥ + é€å­—ã€
                paragraph_type_with_fade(report.final_output.markdown_report, emoji="ğŸŒ¸", fade_ms=160)

                st.markdown("### â“ å¾ŒçºŒå»ºè­°å•é¡Œ")
                for q in report.final_output.follow_up_questions:
                    emoji_token_stream(q, emoji="ğŸ¥œ")

                # æŠŠ AI å›è¦†å­˜é€²æ­·å²
                ai_reply = (
                    plan_md + "\n" +
                    summary_md + "\n" +
                    "#### Executive Summary\n" + report.final_output.short_summary + "\n" +
                    "#### å®Œæ•´å ±å‘Š\n" + report.final_output.markdown_report + "\n" +
                    "#### å¾ŒçºŒå»ºè­°å•é¡Œ\n" + "\n".join([f"- {q}" for q in report.final_output.follow_up_questions])
                )
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": ai_reply,
                })

            else:
                # ä¸€èˆ¬å°è©±ï¼šç›´æ¥ä½¿ç”¨ç¬¬ä¸€æ¬¡çš„ router çµæœï¼Œä¸è¦é‡è·‘
                full_text = str(router_result.final_output)
                emoji_token_stream(full_text, emoji="ğŸŒ¸")  # é€Ÿåº¦å·²èª¿æ…¢
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": full_text,
                })
