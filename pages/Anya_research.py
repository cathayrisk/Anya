# app.py
# -*- coding: utf-8 -*-
"""
ç ”ç©¶å ±å‘ŠåŠ©æ‰‹ï¼ˆFAISS + OpenAI embeddings + Lazy LangExtract KG + Chat + Workflow UIï¼‰
é ˜åŸŸï¼šç¸½ç¶“ / é‡‘è / è²¡å‹™ / æ°£å€™é¢¨éšª / æ°¸çºŒé‡‘è

âœ… é€™ç‰ˆæ ¸å¿ƒï¼šLangExtract/KG åªåœ¨ã€Œéœ€è¦ KG æ‰èƒ½å›ç­”ã€æ™‚ï¼Œå° relevant chunks åšæŠ½å–ï¼ˆLazy/on-demandï¼‰
- å»ºç´¢å¼•ï¼šOCRï¼ˆå‹¾äº†æ‰åšï¼‰+ chunk + embeddingsï¼ˆFAISSï¼‰
- é è¨­è¼¸å‡ºï¼šæ‘˜è¦/æ ¸å¿ƒä¸»å¼µ/æ¨è«–éˆï¼ˆä¸€æ¬¡ç”Ÿæˆä¸‰ä»½ï¼›åªå°æ–°/è®Šæ›´å ±å‘Šè·‘ï¼‰
- Chatï¼šWorkflow UI + grading/retry + å¼•ç”¨ badges
- KGï¼šè‡ªå‹•/é—œé–‰/å¼·åˆ¶ æ¨¡å¼ï¼›è‡ªå‹•æ¨¡å¼ç”¨è¦å‰‡åˆ¤æ–·æ˜¯å¦éœ€è¦ KGï¼›éœ€è¦æ™‚åªå° relevant chunks æŠ½å–ï¼Œä¸¦åšå¿«å–

ç’°å¢ƒï¼š
- st.secrets["OPENAI_KEY"] æˆ–ç’°å¢ƒè®Šæ•¸ OPENAI_API_KEY

ä¾è³´ï¼š
streamlit, openai, langextract[openai], numpy, faiss-cpu, networkx, pypdf
ï¼ˆæ¨è–¦ï¼‰pymupdfï¼šPDF å¿«é€ŸæŠ½å­—èˆ‡ OCR rasterize
"""

from __future__ import annotations

import os
import re
import io
import uuid
import math
import time
import hashlib
from dataclasses import dataclass, asdict
from typing import Any, Dict, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

import streamlit as st
import numpy as np
import faiss
import networkx as nx
from pypdf import PdfReader

from openai import OpenAI
import langextract as lx

try:
    import fitz  # pymupdf
    HAS_PYMUPDF = True
except Exception:
    HAS_PYMUPDF = False


# =========================
# å›ºå®šæ¨¡å‹è¨­å®šï¼ˆä¾ä½ éœ€æ±‚ï¼šä¸è®“ä½¿ç”¨è€…è¼¸å…¥ï¼‰
# =========================
EMBEDDING_MODEL = "text-embedding-3-small"
LLM_MODEL = "gpt-5.2"
LX_MODEL = "gpt-5.2"
OCR_MODEL = "gpt-4.1-mini"  # è‹¥ä½ ç¢ºèª gpt-5.2 æ”¯æ´ visionï¼Œå¯æ”¹æˆ gpt-5.2


# =========================
# æ•ˆèƒ½åƒæ•¸ï¼ˆå¯èª¿ï¼‰
# =========================
EMBED_BATCH_SIZE = 256

OCR_MAX_WORKERS = 2               # OCR å¾ˆè²´ã€ä¹Ÿæ˜“è¢« rate limitï¼›å»ºè­°å°
LX_MAX_WORKERS_QUERY = 4          # å•ç­”æ™‚ LangExtract å¹¶è¡Œï¼ˆå¯è¦– rate limit èª¿ï¼‰
LX_MAX_CHUNKS_PER_QUERY = 8       # æœ€å¤šå°å¹¾å€‹ relevant chunks è·‘ LangExtractï¼ˆé¿å…å¤ªæ…¢ï¼‰


# =========================
# å°å·¥å…·
# =========================
def norm_space(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def estimate_tokens_from_chars(n_chars: int) -> int:
    if n_chars <= 0:
        return 0
    return max(1, int(math.ceil(n_chars / 3.6)))

def chunk_text(text: str, chunk_size: int = 900, overlap: int = 150) -> List[str]:
    text = norm_space(text)
    if not text:
        return []
    out = []
    i = 0
    while i < len(text):
        j = min(len(text), i + chunk_size)
        out.append(text[i:j])
        if j == len(text):
            break
        i = max(0, j - overlap)
    return out

def sha1_bytes(data: bytes) -> str:
    return hashlib.sha1(data).hexdigest()

def truncate_filename(name: str, max_len: int = 30) -> str:
    if len(name) <= max_len:
        return name
    base, ext = os.path.splitext(name)
    keep = max(10, max_len - len(ext) - 1)
    return f"{base[:keep]}â€¦{ext}"


# =========================
# OpenAI helpers
# =========================
def get_openai_api_key() -> str:
    if "OPENAI_KEY" in st.secrets and st.secrets["OPENAI_KEY"]:
        return st.secrets["OPENAI_KEY"]
    if os.environ.get("OPENAI_API_KEY"):
        return os.environ["OPENAI_API_KEY"]
    if os.environ.get("OPENAI_KEY"):
        return os.environ["OPENAI_KEY"]
    raise RuntimeError("Missing OpenAI API key. Set st.secrets['OPENAI_KEY'] or env OPENAI_API_KEY.")

def get_client(api_key: str) -> OpenAI:
    return OpenAI(api_key=api_key)

def embed_texts(client: OpenAI, texts: List[str]) -> np.ndarray:
    resp = client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=texts,
        encoding_format="float",
    )
    vecs = np.array([d.embedding for d in resp.data], dtype=np.float32)
    norms = np.linalg.norm(vecs, axis=1, keepdims=True)
    norms = np.where(norms == 0, 1.0, norms)
    return vecs / norms

def gen_text(client: OpenAI, system: str, user: str, model: str = LLM_MODEL) -> str:
    resp = client.responses.create(
        model=model,
        input=[
            {"role": "system", "content": system},
            {"role": "user", "content": user},
        ],
    )
    return resp.output_text if getattr(resp, "output_text", None) else str(resp)

def gen_yesno(client: OpenAI, system: str, user: str) -> str:
    out = gen_text(
        client,
        system=system + "\n\nåªå›è¦† 'yes' æˆ– 'no'ï¼ˆå°å¯«ï¼‰ï¼Œä¸è¦åŠ ä»»ä½•å…¶ä»–æ–‡å­—ã€‚",
        user=user,
        model=LLM_MODEL,
    ).strip().lower()
    if "yes" in out and "no" in out:
        return "yes" if out.find("yes") < out.find("no") else "no"
    if "yes" in out:
        return "yes"
    if "no" in out:
        return "no"
    return "no"


# =========================
# æª”æ¡ˆ / OCR
# =========================
@dataclass
class FileRow:
    file_id: str
    file_sig: str
    name: str
    ext: str
    bytes_len: int
    pages: Optional[int]
    extracted_chars: int
    token_est: int
    blank_pages: Optional[int]
    blank_ratio: Optional[float]
    likely_scanned: bool
    use_ocr: bool

def extract_pdf_text_pages_pypdf(pdf_bytes: bytes) -> List[Tuple[int, str]]:
    reader = PdfReader(io.BytesIO(pdf_bytes))
    out = []
    for i, p in enumerate(reader.pages):
        try:
            t = p.extract_text() or ""
        except Exception:
            t = ""
        out.append((i + 1, norm_space(t)))
    return out

def extract_pdf_text_pages_pymupdf(pdf_bytes: bytes) -> List[Tuple[int, str]]:
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    out = []
    for i in range(doc.page_count):
        page = doc.load_page(i)
        t = page.get_text("text") or ""
        out.append((i + 1, norm_space(t)))
    return out

def extract_pdf_text_pages(pdf_bytes: bytes) -> List[Tuple[int, str]]:
    if HAS_PYMUPDF:
        try:
            return extract_pdf_text_pages_pymupdf(pdf_bytes)
        except Exception:
            return extract_pdf_text_pages_pypdf(pdf_bytes)
    return extract_pdf_text_pages_pypdf(pdf_bytes)

def analyze_pdf_text_quality(pdf_pages: List[Tuple[int, str]], min_chars_per_page: int = 40) -> Tuple[int, int, float]:
    if not pdf_pages:
        return 0, 0, 1.0
    lens = [len(t) for _, t in pdf_pages]
    blank = sum(1 for L in lens if L <= min_chars_per_page)
    ratio = blank / max(1, len(lens))
    return sum(lens), blank, ratio

def should_suggest_ocr(ext: str, pages: Optional[int], extracted_chars: int, blank_ratio: Optional[float]) -> bool:
    if ext != ".pdf":
        return False
    if pages is None or pages <= 0:
        return True
    if blank_ratio is not None and blank_ratio >= 0.6:
        return True
    avg = extracted_chars / max(1, pages)
    return avg < 120

def ocr_image_bytes_with_openai(client: OpenAI, image_bytes: bytes, model: str = OCR_MODEL) -> str:
    system = "ä½ æ˜¯ä¸€å€‹OCRå·¥å…·ã€‚åªè¼¸å‡ºå¯è¦‹æ–‡å­—èˆ‡è¡¨æ ¼å…§å®¹ï¼ˆè‹¥æœ‰è¡¨æ ¼ç”¨ Markdown è¡¨æ ¼ï¼‰ã€‚ä¸­æ–‡è«‹ç”¨ç¹é«”ä¸­æ–‡ã€‚ä¸è¦åŠ è©•è«–ã€‚"
    user_content = [
        {"type": "input_text", "text": "è«‹æ“·å–åœ–ç‰‡ä¸­æ‰€æœ‰å¯è¦‹æ–‡å­—ï¼ˆåŒ…å«å°å­—/è¨»è…³ï¼‰ã€‚è‹¥ç„¡æ³•è¾¨è­˜è«‹æ¨™è¨˜[ç„¡æ³•è¾¨è­˜]ã€‚"},
        {"type": "input_image", "image_bytes": image_bytes},
    ]
    resp = client.responses.create(
        model=model,
        input=[
            {"role": "system", "content": system},
            {"role": "user", "content": user_content},
        ],
    )
    return resp.output_text if getattr(resp, "output_text", None) else str(resp)

def ocr_pdf_pages_with_openai_parallel(client: OpenAI, pdf_bytes: bytes, dpi: int = 180) -> List[Tuple[int, str]]:
    if not HAS_PYMUPDF:
        raise RuntimeError("æœªå®‰è£ pymupdfï¼ˆfitzï¼‰ï¼Œç„¡æ³•åš PDF OCRã€‚è«‹ pip install pymupdf")

    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    zoom = dpi / 72.0
    mat = fitz.Matrix(zoom, zoom)

    def render_page(i: int) -> Tuple[int, bytes]:
        page = doc.load_page(i)
        pix = page.get_pixmap(matrix=mat, alpha=False)
        return i + 1, pix.tobytes("png")

    page_imgs = [render_page(i) for i in range(doc.page_count)]

    results: Dict[int, str] = {}
    with ThreadPoolExecutor(max_workers=OCR_MAX_WORKERS) as ex:
        futs = {ex.submit(ocr_image_bytes_with_openai, client, img_bytes): page_no for page_no, img_bytes in page_imgs}
        for fut in as_completed(futs):
            page_no = futs[fut]
            try:
                results[page_no] = norm_space(fut.result())
            except Exception:
                results[page_no] = ""

    return [(p, results.get(p, "")) for p, _ in page_imgs]


# =========================
# FAISS
# =========================
@dataclass
class Chunk:
    chunk_id: str
    report_id: str
    title: str
    page: Optional[int]
    text: str

class FaissStore:
    def __init__(self, dim: int):
        self.index = faiss.IndexFlatIP(dim)
        self.chunks: List[Chunk] = []

    def add(self, vecs: np.ndarray, chunks: List[Chunk]) -> None:
        self.index.add(vecs)
        self.chunks.extend(chunks)

    def search(self, qvec: np.ndarray, k: int = 8) -> List[Tuple[float, Chunk]]:
        if self.index.ntotal == 0:
            return []
        scores, idx = self.index.search(qvec.astype(np.float32), k)
        out = []
        for s, i in zip(scores[0], idx[0]):
            if i < 0 or i >= len(self.chunks):
                continue
            out.append((float(s), self.chunks[i]))
        return out


# =========================
# KGï¼ˆQuery-timeï¼‰
# =========================
@dataclass
class Prov:
    report_id: str
    title: str
    page: Optional[int]
    char_start: Optional[int]
    char_end: Optional[int]
    snippet: str

ALLOWED_RELATIONS = {
    "CAUSES", "DRIVES", "AFFECTS", "INCREASES", "DECREASES", "CORRELATES_WITH",
    "ANNOUNCES", "TIGHTENS", "EASES",
    "ASSUMES_SCENARIO", "HAS_RISK", "HAS_METRIC", "TARGETS",
    "IS_A", "LOCATED_IN", "HAS_TIME", "HAS_SOURCE",
    "MENTIONS",
}

def norm_rel(r: str) -> str:
    r = norm_space(r).upper().replace(" ", "_")
    r = re.sub(r"[^A-Z0-9_]+", "", r)
    mapping = {
        "IMPACTS": "AFFECTS",
        "IMPACT": "AFFECTS",
        "INCREASE": "INCREASES",
        "DECREASE": "DECREASES",
        "CORRELATES": "CORRELATES_WITH",
        "CORRELATION": "CORRELATES_WITH",
        "SCENARIO": "ASSUMES_SCENARIO",
    }
    return mapping.get(r, r)

class KnowledgeGraph:
    def __init__(self):
        self.g = nx.MultiDiGraph()

    def add_edge(self, s: str, r: str, o: str, prov: Prov, attrs: Optional[Dict[str, Any]] = None):
        s = norm_space(s)
        o = norm_space(o)
        r = norm_rel(r)
        if not s or not o or not r:
            return
        if r not in ALLOWED_RELATIONS:
            return

        if s not in self.g:
            self.g.add_node(s, label=s)
        if o not in self.g:
            self.g.add_node(o, label=o)

        self.g.add_edge(
            s, o, key=str(uuid.uuid4()),
            relation=r,
            prov=asdict(prov),
            attrs=attrs or {},
        )

    def find_nodes_in_query(self, query: str, max_n: int = 2) -> List[str]:
        q = norm_space(query)
        hits = []
        for n in self.g.nodes():
            if len(n) >= 4 and n in q:
                hits.append(n)
        return hits[:max_n]

    def bfs_edges(self, start: str, max_edges: int = 18) -> List[Dict[str, Any]]:
        if start not in self.g:
            return []
        out = []
        for u, v, k, data in nx.edge_bfs(self.g, start):
            out.append({"u": u, "v": v, "rel": data.get("relation"), "prov": data.get("prov")})
            if len(out) >= max_edges:
                break
        return out


# =========================
# LangExtractï¼ˆLazyï¼‰
# =========================
def lx_prompt() -> str:
    return (
        "Extract structured information from macro/finance/climate-risk/sustainable-finance reports.\n"
        "Rules:\n"
        "1) Use exact text spans for extraction_text. Do NOT paraphrase.\n"
        "2) Extract only two classes: claim, relation.\n"
        "3) claim.attributes may include: theme, stance, confidence, time, implication.\n"
        "4) relation.attributes must include: {subject, relation, object}. Optional: {time, polarity, qualifier}.\n"
        "5) Only extract relations explicitly supported by text; if unsure, skip.\n"
    )

def lx_examples() -> List[lx.data.ExampleData]:
    t1 = (
        "We expect US CPI inflation to decelerate in 2025Q2 as energy prices fall. "
        "The Fed is likely to keep policy restrictive through mid-2025."
    )
    ex1 = lx.data.ExampleData(
        text=t1,
        extractions=[
            lx.data.Extraction(
                extraction_class="relation",
                extraction_text="as energy prices fall",
                attributes={"subject": "energy prices", "relation": "DECREASES", "object": "US CPI inflation", "time": "2025Q2"},
            ),
            lx.data.Extraction(
                extraction_class="claim",
                extraction_text="keep policy restrictive through mid-2025",
                attributes={"theme": "policy_outlook"},
            ),
        ],
    )
    return [ex1]

def run_langextract(text: str, api_key: str) -> lx.data.AnnotatedDocument:
    return lx.extract(
        text_or_documents=text,
        prompt_description=lx_prompt(),
        examples=lx_examples(),
        model_id=LX_MODEL,
        api_key=api_key,
        extraction_passes=1,      # Lazy æ¨¡å¼å…ˆç”¨ 1 pass çœæ™‚é–“ï¼ˆéœ€è¦å†åŠ ï¼‰
        max_char_buffer=1200,
        max_workers=8,
        fence_output=True,
        use_schema_constraints=False,
    )


# =========================
# å¼•ç”¨æª¢æŸ¥ + badge å‘ˆç¾
# =========================
CIT_RE = re.compile(r"\[[^\]]+\|\s*[^\]]+\]")  # [title pX | chunk_id]
BULLET_RE = re.compile(r"^\s*(?:[-â€¢*]|\d+\.)\s+")
CIT_PARSE_RE = re.compile(r"\[([^\]]+?)\s+p(\d+|-)\s*\|\s*([A-Za-z0-9_\-]+)\]")

def bullets_all_have_citations(md: str) -> Tuple[bool, List[str]]:
    bad_lines = []
    lines = (md or "").splitlines()
    has_bullet = any(BULLET_RE.match(l) for l in lines)
    for line in lines:
        if BULLET_RE.match(line) and not CIT_RE.search(line):
            bad_lines.append(line)
    if not has_bullet:
        return False, ["ï¼ˆæ²’æœ‰ç”¢å‡ºä»»ä½• bullet æ¢åˆ—ï¼‰"]
    return (len(bad_lines) == 0), bad_lines

def paragraphs_all_have_citations(md: str) -> Tuple[bool, List[str]]:
    paras = [p.strip() for p in re.split(r"\n\s*\n", md or "") if p.strip()]
    bad = []
    if not paras:
        return False, ["ï¼ˆæ²’æœ‰è¼¸å‡ºä»»ä½•æ®µè½ï¼‰"]
    for p in paras:
        if not CIT_RE.search(p):
            bad.append(p[:120])
    return (len(bad) == 0), bad

def generate_with_bullet_citation_guard(client: OpenAI, user: str, max_retries: int = 2) -> str:
    system = (
        "ä½ æ˜¯åš´è¬¹çš„ç ”ç©¶åŠ©ç†ã€‚\n"
        "è¦å‰‡ï¼šåªèƒ½æ ¹æ“šè³‡æ–™å›ç­”ï¼Œä¸å¯è…¦è£œï¼›è¼¸å‡ºç´” bulletï¼ˆæ¯è¡Œ - é–‹é ­ï¼‰ï¼›æ¯å€‹ bullet å¥å°¾å¿…é ˆæœ‰å¼•ç”¨ [å ±å‘Š pé  | chunk_id]ã€‚\n"
    )
    last = ""
    for _ in range(max_retries + 1):
        out = gen_text(client, system, user, model=LLM_MODEL)
        ok, _ = bullets_all_have_citations(out)
        if ok:
            return out
        last = out
        user += "\n\nã€å¼·åˆ¶ä¿®æ­£ã€‘é‡å¯«ï¼šæ¯å€‹ bullet å¥å°¾éƒ½è¦æœ‰ [å ±å‘Š pé  | chunk_id]ã€‚"
    return last

def generate_with_paragraph_citation_guard(client: OpenAI, user: str, max_retries: int = 2) -> str:
    system = (
        "ä½ æ˜¯åš´è¬¹çš„ç ”ç©¶åŠ©ç†ã€‚\n"
        "è¦å‰‡ï¼šåªèƒ½æ ¹æ“š Context å›ç­”ï¼Œä¸å¯è…¦è£œï¼›2~4 æ®µï¼›æ¯æ®µè‡³å°‘ 1 å€‹å¼•ç”¨ [å ±å‘Š pé  | chunk_id]ã€‚\n"
    )
    last = ""
    for _ in range(max_retries + 1):
        out = gen_text(client, system, user, model=LLM_MODEL)
        ok, _ = paragraphs_all_have_citations(out)
        if ok:
            return out
        last = out
        user += "\n\nã€å¼·åˆ¶ä¿®æ­£ã€‘é‡å¯«ï¼šæ¯æ®µè‡³å°‘ 1 å€‹ [å ±å‘Š pé  | chunk_id]ã€‚"
    return last

def _parse_citations(cits: List[str]) -> List[Dict[str, str]]:
    parsed = []
    for c in cits:
        m = CIT_PARSE_RE.search(c)
        if not m:
            parsed.append({"title": "ä¾†æº", "page": "-", "chunk_id": c.strip("[]")})
        else:
            parsed.append({"title": m.group(1).strip(), "page": m.group(2).strip(), "chunk_id": m.group(3).strip()})
    return parsed

def _render_badges(parsed: List[Dict[str, str]], color: str = "blue", icon: Optional[str] = ":material/bookmark:"):
    if not parsed:
        return
    per_row = 4
    for i in range(0, len(parsed), per_row):
        row = parsed[i:i+per_row]
        cols = st.columns(len(row))
        for col, item in zip(cols, row):
            title = item["title"]
            title_short = title if len(title) <= 18 else (title[:18] + "â€¦")
            page = item["page"]
            chunk_id = item["chunk_id"]
            label = f"{title_short} p{page} Â· {chunk_id}"
            with col:
                st.badge(label, icon=icon, color=color, help=f"{title} p{page} | {chunk_id}")

def render_bullets_with_badges(md_bullets: str, badge_color: str = "blue"):
    lines = [l.rstrip() for l in (md_bullets or "").splitlines() if l.strip()]
    for line in lines:
        if not BULLET_RE.match(line):
            continue
        cits = CIT_RE.findall(line)
        clean = CIT_RE.sub("", line).strip()
        st.markdown(clean)
        _render_badges(_parse_citations(cits), color=badge_color)

def render_text_with_badges(md_text: str, badge_color: str = "gray"):
    cits = CIT_RE.findall(md_text or "")
    clean = CIT_RE.sub("", md_text or "").strip()
    st.markdown(clean if clean else "ï¼ˆç„¡å…§å®¹ï¼‰")
    parsed = _parse_citations(sorted(set(cits)))
    if parsed:
        st.markdown("**ä¾†æºï¼š**")
        _render_badges(parsed, color=badge_color, icon=":material/source:")


# =========================
# âœ… ä¸€æ¬¡ç”Ÿæˆä¸‰ä»½é è¨­è¼¸å‡ºï¼ˆSUMMARY/CLAIMS/CHAINï¼‰
# =========================
def _split_default_bundle(text: str) -> Dict[str, str]:
    t = (text or "").strip()
    pattern = re.compile(
        r"###\s*SUMMARY\s*(.*?)###\s*CLAIMS\s*(.*?)###\s*CHAIN\s*(.*)$",
        re.IGNORECASE | re.DOTALL,
    )
    m = pattern.search(t)
    if not m:
        return {"summary": "", "claims": "", "chain": ""}
    return {"summary": m.group(1).strip(), "claims": m.group(2).strip(), "chain": m.group(3).strip()}

def generate_default_outputs_bundle_with_guard(client: OpenAI, title: str, ctx: str, max_retries: int = 2) -> Dict[str, str]:
    system = (
        "ä½ æ˜¯åš´è¬¹çš„ç ”ç©¶åŠ©ç†ï¼Œåªèƒ½æ ¹æ“šæˆ‘æä¾›çš„è³‡æ–™å›ç­”ï¼Œä¸å¯è…¦è£œã€‚\n"
        "ç¡¬æ€§è¦å‰‡ï¼š\n"
        "1) ä½ å¿…é ˆè¼¸å‡ºä¸‰å€‹å€å¡Šï¼Œä¸”é †åº/æ¨™é¡Œå›ºå®šï¼š### SUMMARYã€### CLAIMSã€### CHAINã€‚\n"
        "2) æ¯å€‹å€å¡Šéƒ½å¿…é ˆæ˜¯ç´” bulletï¼ˆæ¯è¡Œä»¥ - é–‹é ­ï¼‰ï¼Œä¸è¦æ®µè½ã€‚\n"
        "3) æ¯å€‹ bullet å¥å°¾å¿…é ˆé™„å¼•ç”¨ï¼Œæ ¼å¼å›ºå®šï¼š[å ±å‘Š pé  | chunk_id]\n"
    )
    base_user = (
        f"è«‹é‡å°å ±å‘Šã€Š{title}ã€‹ä¸€æ¬¡è¼¸å‡ºä¸‰ä»½å…§å®¹ï¼š\n"
        f"- SUMMARYï¼š8~14 bulletsï¼ˆçµè«–/é æ¸¬/å‡è¨­/é¢¨éšª/é™åˆ¶/å¸‚å ´å«æ„ï¼‰\n"
        f"- CLAIMSï¼š8~14 bulletsï¼ˆå¯é©—è­‰ä¸»å¼µï¼‰\n"
        f"- CHAINï¼š6~12 bulletsï¼ˆå‚³å°ï¼šé©…å‹•â†’ä¸­ä»‹â†’çµè«–â†’é¢¨éšªï¼‰\n\n"
        f"è³‡æ–™ï¼š\n{ctx}\n\n"
        f"ç¾åœ¨è«‹è¼¸å‡ºï¼š\n"
        f"### SUMMARY\n(bullets...)\n### CLAIMS\n(bullets...)\n### CHAIN\n(bullets...)\n"
    )

    user = base_user
    for _ in range(max_retries + 1):
        out = gen_text(client, system, user, model=LLM_MODEL)
        parts = _split_default_bundle(out)
        ok_s, _ = bullets_all_have_citations(parts.get("summary", ""))
        ok_c, _ = bullets_all_have_citations(parts.get("claims", ""))
        ok_h, _ = bullets_all_have_citations(parts.get("chain", ""))
        if ok_s and ok_c and ok_h:
            return {"summary": parts["summary"], "claims": parts["claims"], "chain": parts["chain"]}
        user = base_user + "\n\nã€å¼·åˆ¶ä¿®æ­£ã€‘æ•´ä»½é‡å¯«ï¼šä¸‰å€å¡Šçš†ç‚ºç´” bulletï¼Œä¸”æ¯å€‹ bullet å¥å°¾éƒ½æœ‰ [å ±å‘Š pé  | chunk_id]ã€‚"

    return {"summary": "", "claims": "", "chain": ""}


# =========================
# å»ºç´¢å¼•ï¼ˆåªåš OCR + chunk + embeddingsï¼›ä¸åš LangExtractï¼‰
# =========================
def build_indices_incremental_no_kg(
    client: OpenAI,
    file_rows: List[FileRow],
    file_bytes_map: Dict[str, bytes],
    store: Optional[FaissStore],
    processed_keys: set,
    chunk_size: int = 900,
    overlap: int = 150,
) -> Tuple[FaissStore, Dict[str, Any], set, List[str]]:
    if store is None:
        dim = embed_texts(client, ["dim_probe"]).shape[1]
        store = FaissStore(dim)

    stats = {"new_reports": 0, "new_chunks": 0}
    new_titles: List[str] = []

    new_chunks: List[Chunk] = []
    new_texts: List[str] = []

    to_process = []
    for r in file_rows:
        key = (r.file_sig, bool(r.use_ocr))
        if key not in processed_keys:
            to_process.append(r)

    for row in to_process:
        data = file_bytes_map[row.file_id]
        report_id = row.file_id
        title = os.path.splitext(row.name)[0]
        stats["new_reports"] += 1
        new_titles.append(title)

        # pages (OCR only if you checked)
        if row.ext == ".pdf":
            if row.use_ocr:
                pages = ocr_pdf_pages_with_openai_parallel(client, data)
            else:
                pages = extract_pdf_text_pages(data)
        elif row.ext == ".txt":
            pages = [(None, norm_space(data.decode("utf-8", errors="ignore")))]
        elif row.ext in (".png", ".jpg", ".jpeg"):
            txt = norm_space(ocr_image_bytes_with_openai(client, data))
            pages = [(None, txt)]
        else:
            pages = [(None, "")]

        # chunks
        for page_no, page_text in pages:
            if not page_text:
                continue
            for i, ch in enumerate(chunk_text(page_text, chunk_size=chunk_size, overlap=overlap)):
                cid = f"{report_id}_p{page_no if page_no else 'na'}_c{i}"
                new_chunks.append(Chunk(cid, report_id, title, page_no if isinstance(page_no, int) else None, ch))
                new_texts.append(ch)

        processed_keys.add((row.file_sig, bool(row.use_ocr)))

    # embeddingsï¼šå¤§æ‰¹æ¬¡
    if new_texts:
        vecs_list = []
        for i in range(0, len(new_texts), EMBED_BATCH_SIZE):
            vecs_list.append(embed_texts(client, new_texts[i:i+EMBED_BATCH_SIZE]))
        vecs = np.vstack(vecs_list)
        store.add(vecs, new_chunks)

    stats["new_chunks"] = len(new_chunks)
    return store, stats, processed_keys, sorted(set(new_titles))


# =========================
# é è¨­è¼¸å‡ºï¼šæŒ‘ chunk context
# =========================
def pick_chunks_for_report(all_chunks: List[Chunk], title: str, max_n: int = 12) -> List[Chunk]:
    kw = re.compile(r"(conclusion|outlook|risk|implication|forecast|scenario|inflation|rate|credit|spread|emission|transition|physical|policy)", re.I)

    def score(c: Chunk) -> float:
        s = 0.0
        if c.page is not None:
            s += max(0.0, 8.0 - min(8.0, float(c.page)))
        if kw.search(c.text or ""):
            s += 6.0
        s += min(2.0, len(c.text) / 1200.0)
        return s

    cands = [c for c in all_chunks if c.title == title]
    cands = sorted(cands, key=score, reverse=True)
    return cands[:max_n]

def render_chunks_with_ids(chunks: List[Chunk], max_chars_each: int = 900) -> str:
    parts = []
    for c in chunks:
        head = f"[{c.title} p{c.page if c.page else '-'} | {c.chunk_id}]"
        parts.append(head + "\n" + c.text[:max_chars_each])
    return "\n\n".join(parts)


# =========================
# Lazy KGï¼šåˆ¤æ–·æ˜¯å¦éœ€è¦ KG + å° relevant chunks æŠ½å–
# =========================
KG_KEYWORDS = [
    # multi-hop / causal / mechanism
    "å‚³å°", "æ©Ÿåˆ¶", "å› æœ", "é€ æˆ", "å°è‡´", "å½±éŸ¿", "å¦‚ä½•å½±éŸ¿", "ç‚ºä½•", "ç‚ºä»€éº¼",
    "é—œè¯", "ä¹‹é–“é—œä¿‚", "è·¯å¾‘", "éˆ", "interplay", "mechanism",
    # comparison / conflict / versions
    "çŸ›ç›¾", "è¡çª", "ä¸€è‡´", "ä¸ä¸€è‡´", "æ¯”è¼ƒ", "å·®ç•°", "æ–°ç‰ˆ", "èˆŠç‰ˆ", "æ›´æ–°",
    # aggregation / set operations
    "æœ‰å“ªäº›å› ç´ ", "å…±åŒ", "äº¤é›†", "é‡ç–Š", "æ’å", "è¨ˆæ•¸", "å¤šå°‘", "å»é‡",
    # rules / constraints
    "è¦å‰‡", "é™åˆ¶", "ä¾‹å¤–", "æ¢ä»¶", "åˆè¦", "ä¸ç¬¦åˆ", "æ’é™¤",
]

def need_kg_auto(question: str) -> bool:
    q = norm_space(question)
    return any(k in q for k in KG_KEYWORDS)

def build_kg_hints_block_from_edges(kg: KnowledgeGraph, question: str, max_edges: int = 18) -> str:
    starts = kg.find_nodes_in_query(question, max_n=2)
    if not starts:
        # è‹¥ query ä¸­æ²’æœ‰ä»»ä½• node å­—é¢åŒ¹é…ï¼Œå…ˆä¸ç¡¬åš entity linkingï¼ˆé¿å…å¤šä¸€æ¬¡ LLMï¼‰
        return ""
    lines = []
    for s in starts:
        edges = kg.bfs_edges(s, max_edges=max_edges)
        for e in edges:
            prov = e.get("prov") or {}
            title = prov.get("title", "Unknown")
            page = prov.get("page", "-")
            chunk_id = prov.get("chunk_id", prov.get("chunkId", ""))  # å…¼å®¹
            if not chunk_id:
                chunk_id = prov.get("snippet_id", "") or "unknown"
            # ä¿è­‰ citation å½¢å¼ä¸€è‡´
            citation = f"[{title} p{page if page else '-'} | {chunk_id}]"
            lines.append(f"- {e['u']} --[{e['rel']}]--> {e['v']} {citation}")
            if len(lines) >= max_edges:
                break
        if len(lines) >= max_edges:
            break
    return "ã€KG ç·šç´¢ã€‘\n" + "\n".join(lines) if lines else ""

def lazy_langextract_kg_for_chunks(
    api_key: str,
    chunks: List[Chunk],
    lx_cache: Dict[str, Any],
) -> Tuple[KnowledgeGraph, int]:
    """
    å° chunks åš LangExtract æŠ½å–ï¼ˆæœ‰å¿«å–å°±ä¸é‡è·‘ï¼‰ï¼Œå›å‚³ query-time KG èˆ‡æ–°å¢é‚Šæ•¸
    cache æ ¼å¼ï¼šlx_cache[chunk_id] = list of edges {s,r,o,prov,attrs}
    """
    kg = KnowledgeGraph()
    new_edges = 0

    def extract_one(ch: Chunk) -> Tuple[str, List[Dict[str, Any]]]:
        if ch.chunk_id in lx_cache:
            return ch.chunk_id, lx_cache[ch.chunk_id]

        ann = run_langextract(ch.text, api_key=api_key)
        extracted = []

        for e in ann.extractions:
            cls = getattr(e, "extraction_class", "")
            attrs = getattr(e, "attributes", {}) or {}
            etext = getattr(e, "extraction_text", "") or ""
            cstart = getattr(e, "char_start", None)
            cend = getattr(e, "char_end", None)

            snippet = ch.text[:220]
            if cstart is not None and cend is not None and 0 <= cstart < len(ch.text):
                snippet = ch.text[max(0, cstart - 80): min(len(ch.text), cend + 80)]

            prov = Prov(
                report_id=ch.report_id,
                title=ch.title,
                page=ch.page,
                char_start=cstart,
                char_end=cend,
                snippet=snippet,
            )
            prov_dict = asdict(prov)
            prov_dict["chunk_id"] = ch.chunk_id

            if cls == "relation":
                s = attrs.get("subject", "")
                r = attrs.get("relation", "")
                o = attrs.get("object", "")
                extracted.append({"s": s, "r": r, "o": o, "prov": prov_dict, "attrs": attrs})
            elif cls == "claim":
                # claim å…ˆæ›åœ¨å ±å‘Šååº•ä¸‹ï¼ˆå¯ç”¨æ–¼ä¹‹å¾Œåšå°æ¯”/å½™ç¸½ï¼‰
                claim_node = f"CLAIM: {norm_space(etext)}"
                extracted.append({"s": ch.title, "r": "MENTIONS", "o": claim_node, "prov": prov_dict, "attrs": attrs})

        lx_cache[ch.chunk_id] = extracted
        return ch.chunk_id, extracted

    # å¹¶è¡Œè·‘ï¼ˆæ³¨æ„ rate limitï¼Œworker åˆ¥å¤ªå¤§ï¼‰
    with ThreadPoolExecutor(max_workers=LX_MAX_WORKERS_QUERY) as ex:
        futs = {ex.submit(extract_one, ch): ch.chunk_id for ch in chunks}
        for fut in as_completed(futs):
            _cid = futs[fut]
            try:
                _cid, extracted = fut.result()
            except Exception:
                extracted = []

            for item in extracted:
                prov = item.get("prov") or {}
                prov_obj = Prov(
                    report_id=prov.get("report_id", ""),
                    title=prov.get("title", ""),
                    page=prov.get("page", None),
                    char_start=prov.get("char_start", None),
                    char_end=prov.get("char_end", None),
                    snippet=prov.get("snippet", ""),
                )
                # è®“ KG çš„ prov å¸¶ chunk_id
                prov_dict = asdict(prov_obj)
                prov_dict["chunk_id"] = prov.get("chunk_id", "")
                # ç”¨ kg.add_edge å…§éƒ¨ schema filter
                before_edges = kg.g.number_of_edges()
                kg.add_edge(item.get("s", ""), item.get("r", ""), item.get("o", ""), prov_obj, attrs=item.get("attrs"))
                # é‡æ–°è¦†è“‹ provï¼ˆå« chunk_idï¼‰
                # networkx MultiDiGraph add_edge æœƒå­˜ prov=asdict(prov_obj)ï¼›æˆ‘å€‘æŠŠ chunk_id æ”¾åœ¨ snippet æ—é‚Šæœ‰é»æ€ª
                # => é€™è£¡ç”¨ã€Œé¡å¤–åŠ ä¸€å€‹ prov_chunk_idã€å­˜ï¼ˆä¸å½±éŸ¿ BFS é¡¯ç¤ºçš„ citation æˆ‘å€‘è‡ªå·±æ‹¼ chunk_idï¼‰
                # ä½†ç‚ºäº†ç°¡åŒ–ï¼šcitation æ™‚å¾ prov_obj å–ä¸åˆ° chunk_idï¼Œæ‰€ä»¥æˆ‘å€‘åœ¨ build_kg_hints_block_from_edges æ™‚å¾ prov_dict å¡ chunk_id
                after_edges = kg.g.number_of_edges()
                if after_edges > before_edges:
                    new_edges += (after_edges - before_edges)

    return kg, new_edges


# =========================
# Chat workflowï¼ˆUIï¼‰
# =========================
def build_context_from_chunks(items: List[Dict[str, Any]], top_k: int = 8) -> str:
    items = sorted(items, key=lambda x: x["score"], reverse=True)[:top_k]
    parts = []
    for it in items:
        ch: Chunk = it["chunk"]
        parts.append(f"[{ch.title} p{ch.page if ch.page else '-'} | {ch.chunk_id}]\n{ch.text}")
    return "\n\n".join(parts) if parts else "ï¼ˆæ‰¾ä¸åˆ°ä»»ä½•ç›¸é—œå…§å®¹ï¼‰"

def want_bullets(question: str) -> bool:
    return bool(re.search(r"(åˆ—å‡º|æœ‰å“ªäº›|æ‰€æœ‰|æ¸…å–®|å½™ç¸½|æ‘˜è¦|ç¸½çµ)", question))

def generate_answer_from_context(client: OpenAI, question: str, context: str) -> str:
    if want_bullets(question):
        user = f"Context:\n{context}\n\nQuestion:\n{question}"
        return generate_with_bullet_citation_guard(client, user, max_retries=1)
    user = (
        "è«‹å›ç­”å•é¡Œï¼Œä¾åºï¼šçµè«–â†’ä¾æ“šï¼ˆå¼•ç”¨ï¼‰â†’æ¨è«–/è§£é‡‹ã€‚\n"
        "æ¯æ®µè‡³å°‘1å€‹å¼•ç”¨ [å ±å‘Š pé  | chunk_id]ã€‚\n\n"
        f"Context:\n{context}\n\nQuestion:\n{question}"
    )
    return generate_with_paragraph_citation_guard(client, user, max_retries=1)

def grade_doc_relevance(client: OpenAI, question: str, doc_text: str) -> str:
    system = "ä½ æ˜¯æ–‡ä»¶ç›¸é—œæ€§è©•åˆ†è€…ã€‚ç‰‡æ®µèƒ½æ”¯æŒå›ç­”=>yesï¼Œå¦å‰‡=>noã€‚"
    user = f"Question:\n{question}\n\nDocument:\n{doc_text[:2200]}"
    return gen_yesno(client, system, user)

def rewrite_question(client: OpenAI, question: str) -> str:
    system = "ä½ æ˜¯æª¢ç´¢ query æ”¹å¯«è€…ã€‚ä¿ç•™åŸæ„ï¼Œè£œä¸Šå¯æª¢ç´¢é—œéµå­—ã€‚è¼¸å‡ºä¸€è¡Œæ”¹å¯«å¾Œå•é¡Œã€‚"
    return gen_text(client, system, question, model=LLM_MODEL).strip()

def grade_hallucination(client: OpenAI, context: str, answer: str) -> str:
    system = "åˆ¤æ–·å›ç­”æ˜¯å¦è¢« context æ”¯æŒã€‚æ”¯æŒ=>yesï¼Œå¦å‰‡=>noã€‚"
    user = f"Context:\n{context[:9000]}\n\nAnswer:\n{answer[:4500]}"
    return gen_yesno(client, system, user)

def grade_answer(client: OpenAI, question: str, answer: str) -> str:
    system = "åˆ¤æ–·å›ç­”æ˜¯å¦å›æ‡‰å•é¡Œã€‚æ˜¯=>yesï¼Œå¦=>noã€‚"
    user = f"Question:\n{question}\n\nAnswer:\n{answer[:4500]}"
    return gen_yesno(client, system, user)

def build_retrieval_packages(client: OpenAI, store: FaissStore, question: str, top_k: int = 10) -> List[Dict[str, Any]]:
    qvec = embed_texts(client, [question])
    hits = store.search(qvec, k=top_k)
    return [{"chunk": ch, "score": score} for score, ch in hits]

def _step_table(step_state: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:
    return [{"Step": k, "Status": v.get("status"), "Seconds": v.get("seconds"), "Note": v.get("note")} for k, v in step_state.items()]

def run_chat_workflow_with_ui(
    client: OpenAI,
    api_key: str,
    store: FaissStore,
    question: str,
    *,
    kg_mode: str,  # "OFF" / "AUTO" / "FORCE"
    max_query_rewrites: int = 2,
    max_generate_retries: int = 2,
    top_k: int = 10,
) -> Dict[str, Any]:
    query_history: List[str] = [question]
    logs: List[str] = []
    final_context = ""
    final_answer = ""

    step_state = {
        "RETRIEVE": {"status": "PENDING", "seconds": None, "note": ""},
        "GRADE": {"status": "PENDING", "seconds": None, "note": ""},
        "KG_EXTRACT": {"status": "SKIP", "seconds": None, "note": "not triggered"},
        "TRANSFORM": {"status": "PENDING", "seconds": None, "note": ""},
        "GENERATE": {"status": "PENDING", "seconds": None, "note": ""},
        "CHECK": {"status": "PENDING", "seconds": None, "note": ""},
    }

    step_summary_ph = st.empty()
    query_hist_ph = st.empty()

    def update_step_summary():
        step_summary_ph.markdown("#### Step Summaryï¼ˆâœ…/âŒ + è€—æ™‚ï¼‰")
        step_summary_ph.dataframe(_step_table(step_state), width="stretch", hide_index=True)

    def set_step(name: str, status: str, seconds: Optional[float] = None, note: str = ""):
        step_state[name]["status"] = status
        step_state[name]["seconds"] = (round(seconds, 3) if seconds is not None else None)
        step_state[name]["note"] = note
        update_step_summary()

    update_step_summary()
    query_hist_ph.markdown("#### Query history")
    query_hist_ph.code("\n".join([f"{i}. {q}" for i, q in enumerate(query_history)]))

    q = question

    for rewrite_round in range(max_query_rewrites + 1):
        # RETRIEVE
        t0 = time.perf_counter()
        retrieved = build_retrieval_packages(client, store, q, top_k=top_k)
        set_step("RETRIEVE", "âœ… OK", time.perf_counter() - t0, note=f"top_k={top_k}, got={len(retrieved)}")

        st.markdown(f"### RETRIEVEï¼ˆround {rewrite_round}ï¼‰")
        st.dataframe(
            [{
                "score": round(float(it["score"]), 4),
                "å ±å‘Š": it["chunk"].title,
                "é ": it["chunk"].page if it["chunk"].page is not None else "-",
                "chunk_id": it["chunk"].chunk_id,
                "preview": (it["chunk"].text[:140] + "â€¦") if len(it["chunk"].text) > 140 else it["chunk"].text,
            } for it in retrieved],
            width="stretch",
            hide_index=True,
        )

        # GRADE
        t1 = time.perf_counter()
        relevant: List[Dict[str, Any]] = []
        graded_rows = []
        prog = st.progress(0, text="gradingâ€¦")
        for i, it in enumerate(retrieved):
            ch: Chunk = it["chunk"]
            verdict = grade_doc_relevance(client, q, ch.text)
            graded_rows.append({
                "grade": verdict,
                "score": round(float(it["score"]), 4),
                "å ±å‘Š": ch.title,
                "é ": ch.page if ch.page is not None else "-",
                "chunk_id": ch.chunk_id,
                "preview": (ch.text[:140] + "â€¦") if len(ch.text) > 140 else ch.text,
            })
            if verdict == "yes":
                relevant.append(it)
            prog.progress((i + 1) / max(1, len(retrieved)), text=f"gradingâ€¦ {i+1}/{len(retrieved)}")

        set_step("GRADE", "âœ… OK", time.perf_counter() - t1, note=f"relevant={len(relevant)}/{len(retrieved)}")

        st.markdown("### GRADEï¼ˆdoc relevance yes/noï¼‰")
        st.dataframe(graded_rows, width="stretch", hide_index=True)

        st.markdown("### Relevant Chunksï¼ˆYESï¼‰")
        if not relevant:
            st.info("é€™ä¸€è¼ªæ²’æœ‰æ‰¾åˆ°ç›¸é—œ chunksï¼ˆå…¨éƒ¨è¢«åˆ¤å®š noï¼‰ã€‚")
        else:
            rel_sorted = sorted(relevant, key=lambda x: x["score"], reverse=True)[:top_k]
            st.dataframe(
                [{
                    "score": round(float(it["score"]), 4),
                    "å ±å‘Š": it["chunk"].title,
                    "é ": it["chunk"].page if it["chunk"].page is not None else "-",
                    "chunk_id": it["chunk"].chunk_id,
                    "preview": (it["chunk"].text[:180] + "â€¦") if len(it["chunk"].text) > 180 else it["chunk"].text,
                } for it in rel_sorted],
                width="stretch",
                hide_index=True,
            )
            st.markdown("#### å±•é–‹çœ‹å…¨æ–‡")
            for it in rel_sorted:
                ch = it["chunk"]
                with st.expander(f"{ch.title} p{ch.page if ch.page else '-'} | {ch.chunk_id} | score={it['score']:.3f}"):
                    st.text(ch.text)

        # TRANSFORMï¼šæ²’ relevant æ‰æ”¹å¯«
        if not relevant:
            if rewrite_round < max_query_rewrites:
                t2 = time.perf_counter()
                st.markdown("### TRANSFORMï¼ˆrewrite queryï¼‰")
                new_q = rewrite_question(client, q)
                query_history.append(new_q)
                query_hist_ph.code("\n".join([f"{i}. {qq}" for i, qq in enumerate(query_history)]))
                set_step("TRANSFORM", "âœ… OK", time.perf_counter() - t2, note="rewrite applied (no relevant docs)")
                q = new_q
                continue
            else:
                set_step("TRANSFORM", "âŒ SKIP", None, note="rewrite limit reached")
                return {
                    "final_answer": "è³‡æ–™ä¸è¶³ï¼šæª¢ç´¢ä¸åˆ°è¶³å¤ ç›¸é—œå…§å®¹ã€‚ä½ å¯ä»¥æ›å€‹å•æ³•æˆ–ä¸Šå‚³æ›´å¤šå ±å‘Šã€‚",
                    "query_history": query_history,
                    "context": "",
                    "logs": logs,
                    "render_mode": "text",
                }

        # === Lazy KG decide ===
        use_kg = False
        if kg_mode == "FORCE":
            use_kg = True
        elif kg_mode == "AUTO":
            use_kg = need_kg_auto(q)
        else:
            use_kg = False

        # context: chunks + (optional) KG hints
        base_context = build_context_from_chunks(relevant, top_k=8)

        kg_hints = ""
        if use_kg:
            tkg = time.perf_counter()
            try:
                # å– top relevant chunks åšæŠ½å–ï¼ˆé¿å…å¤ªæ…¢ï¼‰
                rel_sorted = sorted(relevant, key=lambda x: x["score"], reverse=True)
                target_chunks = [it["chunk"] for it in rel_sorted[:LX_MAX_CHUNKS_PER_QUERY]]

                # Lazy LangExtractï¼ˆæœ‰å¿«å–æ›´å¿«ï¼‰
                kg, new_edges = lazy_langextract_kg_for_chunks(
                    api_key=api_key,
                    chunks=target_chunks,
                    lx_cache=st.session_state.lx_cache,
                )

                # ç”¢ç”Ÿ KG hintsï¼ˆå¸¶ citationï¼‰
                # æ³¨æ„ï¼šKG å…§éƒ¨ prov æ²’ chunk_idï¼Œcitation ç”¨ prov è£œä¸åˆ°ï¼Œå› æ­¤é€™è£¡ä¿å®ˆï¼šç›´æ¥å¾ cache item prov_dict è®€ chunk_id
                # æˆ‘å€‘ç”¨ã€Œå¾ cache é‡å»ºä¸€ä»½ç°¡åŒ– hintsã€ä¾†ä¿è­‰ citation å®Œæ•´ï¼š
                hint_lines = []
                starts = kg.find_nodes_in_query(q, max_n=2)
                if starts:
                    # åªåš BFSï¼Œä¸åš entity linking
                    for s in starts:
                        for u, v, k, data in nx.edge_bfs(kg.g, s):
                            rel = data.get("relation")
                            prov = data.get("prov") or {}
                            title = prov.get("title", u"Unknown")
                            page = prov.get("page", "-")
                            chunk_id = prov.get("chunk_id", "")  # å¯èƒ½æ²’æœ‰
                            if not chunk_id:
                                # å˜—è©¦å¾ lx_cache æ‰¾åˆ°å« u/r/v çš„é‚£ç­†
                                for ch in target_chunks:
                                    cached = st.session_state.lx_cache.get(ch.chunk_id, [])
                                    for item in cached:
                                        if item.get("s") == u and norm_rel(item.get("r", "")) == rel and item.get("o") == v:
                                            chunk_id = ch.chunk_id
                                            title = ch.title
                                            page = ch.page if ch.page is not None else "-"
                                            break
                                    if chunk_id:
                                        break
                            citation = f"[{title} p{page} | {chunk_id if chunk_id else 'unknown'}]"
                            hint_lines.append(f"- {u} --[{rel}]--> {v} {citation}")
                            if len(hint_lines) >= 18:
                                break
                        if len(hint_lines) >= 18:
                            break
                kg_hints = "ã€KG ç·šç´¢ã€‘\n" + "\n".join(hint_lines) if hint_lines else ""

                set_step("KG_EXTRACT", "âœ… OK", time.perf_counter() - tkg, note=f"chunks={len(target_chunks)}, new_edgesâ‰ˆ{new_edges}")
            except Exception as e:
                set_step("KG_EXTRACT", "âŒ FAIL", time.perf_counter() - tkg, note=str(e))
                kg_hints = ""

        # final context
        context = "\n\n".join([p for p in [kg_hints, base_context] if p.strip()])
        final_context = context

        # GENERATE + CHECK
        for gen_round in range(max_generate_retries + 1):
            t3 = time.perf_counter()
            st.markdown(f"### GENERATEï¼ˆround {gen_round}ï¼‰")
            ans = generate_answer_from_context(client, q, context)
            set_step("GENERATE", "âœ… OK", time.perf_counter() - t3, note=f"gen_round={gen_round}")

            st.markdown("#### Draft answerï¼ˆå‘ˆç¾ç”¨ badgeï¼‰")
            if want_bullets(q):
                render_bullets_with_badges(ans, badge_color="blue")
            else:
                render_text_with_badges(ans, badge_color="gray")

            t4 = time.perf_counter()
            st.markdown("### CHECKï¼ˆhallucination / answerï¼‰")
            hall = grade_hallucination(client, context, ans)
            good = grade_answer(client, q, ans)
            set_step("CHECK", "âœ… OK", time.perf_counter() - t4, note=f"hall={hall}, answer_ok={good}")
            logs.append(f"[CHECK] gen_round={gen_round} hall={hall} answer_ok={good}")

            st.write({"hallucination": hall, "answer_ok": good})

            if hall == "yes" and good == "yes":
                final_answer = ans
                return {
                    "final_answer": final_answer,
                    "query_history": query_history,
                    "context": final_context,
                    "logs": logs,
                    "render_mode": ("bullets" if want_bullets(q) else "text"),
                }

            if hall == "no":
                continue
            if good == "no":
                break

        # ç”Ÿæˆä¸ç†æƒ³ â†’ TRANSFORM
        if rewrite_round < max_query_rewrites:
            t2 = time.perf_counter()
            st.markdown("### TRANSFORMï¼ˆrewrite queryï¼‰")
            new_q = rewrite_question(client, q)
            query_history.append(new_q)
            query_hist_ph.code("\n".join([f"{i}. {qq}" for i, qq in enumerate(query_history)]))
            set_step("TRANSFORM", "âœ… OK", time.perf_counter() - t2, note="rewrite applied (generation not ok)")
            q = new_q
            continue

        set_step("TRANSFORM", "âŒ SKIP", None, note="rewrite limit reached")
        return {
            "final_answer": "è³‡æ–™ä¸è¶³ï¼šå·²å¤šæ¬¡å˜—è©¦ä»ç„¡æ³•ç”¢ç”Ÿå¯è¢«è­‰æ“šæ”¯æŒä¸”å›æ‡‰å•é¡Œçš„ç­”æ¡ˆã€‚å»ºè­°æ›å•æ³•æˆ–å¢åŠ è³‡æ–™ã€‚",
            "query_history": query_history,
            "context": final_context,
            "logs": logs,
            "render_mode": "text",
        }

    return {
        "final_answer": "è³‡æ–™ä¸è¶³ï¼šå·¥ä½œæµæœªèƒ½å®Œæˆã€‚",
        "query_history": query_history,
        "context": final_context,
        "logs": logs,
        "render_mode": "text",
    }


# =========================
# Streamlit UI
# =========================
st.set_page_config(page_title="ç ”ç©¶å ±å‘ŠåŠ©æ‰‹ï¼ˆLazy KGï¼‰", layout="wide")
st.title("ç ”ç©¶å ±å‘ŠåŠ©æ‰‹ï¼ˆLazy KGï¼šåªæœ‰éœ€è¦æ™‚æ‰è·‘ LangExtractï¼‰")

OPENAI_API_KEY = get_openai_api_key()
client = get_client(OPENAI_API_KEY)
api_key = OPENAI_API_KEY  # LangExtract ç”¨åŒä¸€æŠŠ key

# Session State
if "file_rows" not in st.session_state:
    st.session_state.file_rows: List[FileRow] = []
if "file_bytes" not in st.session_state:
    st.session_state.file_bytes: Dict[str, bytes] = {}
if "store" not in st.session_state:
    st.session_state.store: Optional[FaissStore] = None
if "processed_keys" not in st.session_state:
    st.session_state.processed_keys = set()  # {(sha1, use_ocr)}
if "default_outputs_cache" not in st.session_state:
    st.session_state.default_outputs_cache = {}  # {title: {report_key, summary, claims, chain}}
if "report_key_by_title" not in st.session_state:
    st.session_state.report_key_by_title = {}
if "chat_history" not in st.session_state:
    st.session_state.chat_history: List[Dict[str, Any]] = []
if "lx_cache" not in st.session_state:
    st.session_state.lx_cache = {}  # chunk_id -> extracted items (edges/claims)


def push_default_outputs_to_chat(default_outputs: Dict[str, Dict[str, str]]):
    st.session_state.chat_history.append({
        "role": "assistant",
        "kind": "text",
        "content": "æˆ‘å…ˆæŠŠä¸Šå‚³å ±å‘Šçš„é è¨­è¼¸å‡ºæ•´ç†å¥½å›‰ï¼ˆæ‘˜è¦/æ ¸å¿ƒä¸»å¼µ/æ¨è«–éˆï¼›æ¯å€‹ bullet éƒ½æœ‰å¼•ç”¨ï¼‰ã€‚ä½ æ¥ä¸‹ä¾†å¯ä»¥ç›´æ¥å•å•é¡Œã€‚",
    })
    for title, out in default_outputs.items():
        st.session_state.chat_history.append({
            "role": "assistant",
            "kind": "default_outputs",
            "title": title,
            "summary": out["summary"],
            "claims": out["claims"],
            "chain": out["chain"],
        })

def render_chat_message(msg: Dict[str, Any]):
    role = msg.get("role", "assistant")
    with st.chat_message(role):
        kind = msg.get("kind", "text")
        if kind == "default_outputs":
            st.markdown(f"## é è¨­è¼¸å‡ºï¼š{msg['title']}")
            st.markdown("### 1) å ±å‘Šæ‘˜è¦")
            render_bullets_with_badges(msg["summary"], badge_color="green")
            st.markdown("### 2) æ ¸å¿ƒä¸»å¼µ")
            render_bullets_with_badges(msg["claims"], badge_color="violet")
            st.markdown("### 3) æ¨è«–éˆ / å‚³å°æ©Ÿåˆ¶")
            render_bullets_with_badges(msg["chain"], badge_color="orange")
        else:
            st.markdown(msg.get("content", ""))


# =========================
# æ–‡ä»¶ç®¡ç†ï¼ˆå…¨éƒ¨åœ¨ popoverï¼‰
# =========================
with st.popover("ğŸ“¦ æ–‡ä»¶ç®¡ç†ï¼ˆä¸Šå‚³ / OCR / å»ºç´¢å¼•ï¼‰", width="content"):
    st.caption("æ”¯æ´ PDF/TXT/PNG/JPGã€‚PDF è‹¥æ–‡å­—æŠ½å–åå°‘æœƒå»ºè­° OCRï¼ˆé€æª”å¯å‹¾é¸ï¼‰ã€‚")

    kg_mode = st.radio(
        "Knowledge Graphï¼ˆå½±éŸ¿ Chat æ™‚æ˜¯å¦æœƒ Lazy è·‘ LangExtractï¼‰",
        options=["AUTO", "OFF", "FORCE"],
        index=0,
        horizontal=True,
        help="AUTOï¼šåµæ¸¬åˆ°å› æœ/å‚³å°/è¦å‰‡/å½™ç¸½ç­‰å•é¡Œæ‰è·‘ï¼›OFFï¼šæ°¸ä¸è·‘ï¼›FORCEï¼šæ¯æ¬¡å•ç­”éƒ½è·‘ï¼ˆæ…¢ï¼‰",
        key="kg_mode",
    )

    up = st.file_uploader(
        "ä¸Šå‚³æ–‡ä»¶",
        type=["pdf", "txt", "png", "jpg", "jpeg"],
        accept_multiple_files=True,
        key="uploader",
    )

    if up:
        existing = {(r.name, r.bytes_len) for r in st.session_state.file_rows}
        for f in up:
            data = f.read()
            if (f.name, len(data)) in existing:
                continue

            ext = os.path.splitext(f.name)[1].lower()
            fid = str(uuid.uuid4())[:10]
            sig = sha1_bytes(data)
            st.session_state.file_bytes[fid] = data

            pages = None
            extracted_chars = 0
            blank_pages = None
            blank_ratio = None

            if ext == ".pdf":
                pdf_pages = extract_pdf_text_pages(data)
                pages = len(pdf_pages)
                extracted_chars, blank_pages, blank_ratio = analyze_pdf_text_quality(pdf_pages)
            elif ext == ".txt":
                text = norm_space(data.decode("utf-8", errors="ignore"))
                extracted_chars = len(text)
            elif ext in (".png", ".jpg", ".jpeg"):
                extracted_chars = 0

            token_est = estimate_tokens_from_chars(extracted_chars)
            likely_scanned = should_suggest_ocr(ext, pages, extracted_chars, blank_ratio)
            use_ocr = True if ext in (".png", ".jpg", ".jpeg") else bool(likely_scanned)

            st.session_state.file_rows.append(
                FileRow(
                    file_id=fid,
                    file_sig=sig,
                    name=f.name,
                    ext=ext,
                    bytes_len=len(data),
                    pages=pages,
                    extracted_chars=extracted_chars,
                    token_est=token_est,
                    blank_pages=blank_pages,
                    blank_ratio=blank_ratio,
                    likely_scanned=likely_scanned,
                    use_ocr=use_ocr,
                )
            )

    st.markdown("### æ–‡ä»¶æ¸…å–®ï¼ˆé‡é»æ¬„ä½ç½®å‰ï¼‰")

    if not st.session_state.file_rows:
        st.info("å°šæœªä¸Šå‚³æ–‡ä»¶ã€‚")
    else:
        header_cols = st.columns([1, 4, 1, 1, 1, 1])
        header_cols[0].markdown("**OCR**")
        header_cols[1].markdown("**æª”å**")
        header_cols[2].markdown("**é **")
        header_cols[3].markdown("**å­—æ•¸**")
        header_cols[4].markdown("**tok**")
        header_cols[5].markdown("**å»ºè­°**")

        for idx, r in enumerate(st.session_state.file_rows):
            cols = st.columns([1, 4, 1, 1, 1, 1])

            if r.ext in (".png", ".jpg", ".jpeg"):
                st.session_state.file_rows[idx].use_ocr = True
                cols[0].checkbox(" ", value=True, key=f"ocr_{idx}", disabled=True)
            elif r.ext == ".txt":
                st.session_state.file_rows[idx].use_ocr = False
                cols[0].checkbox(" ", value=False, key=f"ocr_{idx}", disabled=True)
            else:
                val = cols[0].checkbox(" ", value=bool(r.use_ocr), key=f"ocr_{idx}")
                st.session_state.file_rows[idx].use_ocr = bool(val)

            short = truncate_filename(r.name, 20)
            with cols[1]:
                st.markdown(short)
                st.badge("Full name", icon=":material/info:", color="gray", help=r.name)

            cols[2].markdown(str(r.pages if r.pages is not None else "-"))
            cols[3].markdown(str(r.extracted_chars))
            cols[4].markdown(str(r.token_est))
            cols[5].markdown("OCR" if r.likely_scanned else "")

        st.divider()
        b1, b2, b3 = st.columns([1, 1, 1])
        build_btn = b1.button("ğŸš€ å»ºç«‹ç´¢å¼• + é è¨­è¼¸å‡º", type="primary", width="stretch")
        clear_btn = b2.button("ğŸ§¹ æ¸…ç©ºå…¨éƒ¨", width="stretch")
        clear_lx_cache_btn = b3.button("ğŸ§½ æ¸… LangExtract å¿«å–", width="stretch")

        if clear_btn:
            st.session_state.file_rows = []
            st.session_state.file_bytes = {}
            st.session_state.store = None
            st.session_state.processed_keys = set()
            st.session_state.default_outputs_cache = {}
            st.session_state.report_key_by_title = {}
            st.session_state.chat_history = []
            st.session_state.lx_cache = {}
            st.rerun()

        if clear_lx_cache_btn:
            st.session_state.lx_cache = {}
            st.success("å·²æ¸…é™¤ LangExtract å¿«å–ã€‚")

        if build_btn:
            need_ocr = any(r.ext == ".pdf" and r.use_ocr for r in st.session_state.file_rows)
            if need_ocr and not HAS_PYMUPDF:
                st.error("ä½ æœ‰å‹¾é¸ PDF OCRï¼Œä½†ç’°å¢ƒæœªå®‰è£ pymupdfã€‚è«‹å…ˆ pip install pymupdfï¼Œå†é‡è©¦ã€‚")
                st.stop()

            # 1) å»ºç´¢å¼•ï¼ˆä¸è·‘ LangExtractï¼‰
            with st.status("å»ºç´¢å¼•ä¸­ï¼ˆå¢é‡ï¼šOCR + embeddingsï¼›ä¸åš LangExtractï¼‰...", expanded=True) as s:
                t0 = time.perf_counter()
                store, stats, processed_keys, new_titles = build_indices_incremental_no_kg(
                    client=client,
                    file_rows=st.session_state.file_rows,
                    file_bytes_map=st.session_state.file_bytes,
                    store=st.session_state.store,
                    processed_keys=st.session_state.processed_keys,
                )
                st.session_state.store = store
                st.session_state.processed_keys = processed_keys
                s.write(f"æ–°å¢å ±å‘Šæ•¸ï¼š{stats['new_reports']}")
                s.write(f"æ–°å¢ chunksï¼š{stats['new_chunks']}")
                s.write(f"è€—æ™‚ï¼š{time.perf_counter() - t0:.2f}s")
                s.update(state="complete")

            # 2) é è¨­è¼¸å‡ºï¼šåªè·‘æ–°/è®Šæ›´ï¼›ä¸€æ¬¡ç”Ÿæˆä¸‰ä»½
            titles_now = []
            title_to_key = {}
            for r in st.session_state.file_rows:
                title = os.path.splitext(r.name)[0]
                titles_now.append(title)
                title_to_key[title] = f"{r.file_sig}:{int(bool(r.use_ocr))}"

            to_regen_titles = []
            for title in sorted(set(titles_now)):
                rk = title_to_key.get(title)
                old_rk = st.session_state.report_key_by_title.get(title)
                if (title not in st.session_state.default_outputs_cache) or (old_rk != rk):
                    to_regen_titles.append(title)

            st.session_state.report_key_by_title.update(title_to_key)

            with st.status("ç”¢ç”Ÿé è¨­è¼¸å‡ºï¼ˆåªè·‘æ–°/è®Šæ›´ï¼›ä¸€æ¬¡ç”Ÿæˆä¸‰ä»½ï¼‰...", expanded=True) as s2:
                if not to_regen_titles:
                    s2.write("æ²’æœ‰åµæ¸¬åˆ°æ–°å ±å‘Šæˆ–è®Šæ›´ï¼ˆæ²¿ç”¨å¿«å–ï¼‰ã€‚")
                    s2.update(state="complete")
                else:
                    s2.write(f"éœ€è¦é‡æ–°ç”¢ç”Ÿï¼š{len(to_regen_titles)} ä»½å ±å‘Š")
                    for i, title in enumerate(to_regen_titles, start=1):
                        s2.write(f"[{i}/{len(to_regen_titles)}] ç”¢ç”Ÿï¼š{title}")
                        reps = pick_chunks_for_report(st.session_state.store.chunks, title, max_n=12)
                        ctx = render_chunks_with_ids(reps)
                        bundle = generate_default_outputs_bundle_with_guard(client, title, ctx, max_retries=2)
                        rk = title_to_key[title]
                        st.session_state.default_outputs_cache[title] = {
                            "report_key": rk,
                            "summary": bundle["summary"],
                            "claims": bundle["claims"],
                            "chain": bundle["chain"],
                        }
                    s2.update(state="complete")

            # 3) æ¨é€åˆ° chatï¼ˆæœ¬æ¬¡å­˜åœ¨çš„å ±å‘Šï¼‰
            default_outputs = {}
            for title in sorted(set(titles_now)):
                cached = st.session_state.default_outputs_cache.get(title)
                if cached:
                    default_outputs[title] = {
                        "summary": cached["summary"],
                        "claims": cached["claims"],
                        "chain": cached["chain"],
                    }

            st.session_state.chat_history = []
            push_default_outputs_to_chat(default_outputs)
            st.rerun()


# popover å¤–ï¼šç‹€æ…‹
if st.session_state.store is None:
    st.info("å°šæœªå»ºç«‹ç´¢å¼•ã€‚è«‹é»ã€ŒğŸ“¦ æ–‡ä»¶ç®¡ç†ï¼ˆä¸Šå‚³ / OCR / å»ºç´¢å¼•ï¼‰ã€é–‹å§‹ã€‚")
else:
    st.success(
        f"å·²å»ºç«‹ç´¢å¼•ï¼šæª”æ¡ˆæ•¸={len(st.session_state.file_rows)} / chunks={len(st.session_state.store.chunks)} / "
        f"LangExtractå¿«å–chunks={len(st.session_state.lx_cache)}"
    )

st.divider()

# Chat ä¸»ç•«é¢
st.subheader("Chatï¼ˆéœ€è¦æ™‚æ‰è·‘ LangExtract/KGï¼‰")

for msg in st.session_state.chat_history:
    render_chat_message(msg)

if st.session_state.store is None:
    st.stop()

prompt = st.chat_input("è¼¸å…¥å•é¡Œï¼šç†è§£å«æ„/ç‚ºä½•é€™æ¨£é™³è¿°/å‚³å°æ©Ÿåˆ¶/é‡çµ„æ–°å ±å‘Š/åˆ—å‡ºæ‰€æœ‰â€¦")
if prompt:
    st.session_state.chat_history.append({"role": "user", "kind": "text", "content": prompt})
    render_chat_message(st.session_state.chat_history[-1])

    with st.chat_message("assistant"):
        with st.status("Workflowï¼šRETRIEVE â†’ GRADE â†’ (å¿…è¦æ™‚) KG_EXTRACT â†’ TRANSFORM â†’ GENERATE â†’ CHECK", expanded=True) as status:
            result = run_chat_workflow_with_ui(
                client=client,
                api_key=api_key,
                store=st.session_state.store,
                question=prompt,
                kg_mode=st.session_state.get("kg_mode", "AUTO"),
                max_query_rewrites=2,
                max_generate_retries=2,
                top_k=10,
            )
            status.update(state="complete", expanded=False)

        st.markdown("## æœ€çµ‚å›ç­”")
        if result.get("render_mode") == "bullets":
            render_bullets_with_badges(result["final_answer"], badge_color="blue")
        else:
            render_text_with_badges(result["final_answer"], badge_color="gray")

        with st.expander("æŸ¥çœ‹ debugï¼ˆquery history / logs / contextï¼‰"):
            st.markdown("### Query history")
            st.code("\n".join([f"{i}. {q}" for i, q in enumerate(result.get("query_history", []))]))
            st.markdown("### Logs")
            st.text("\n".join(result.get("logs", [])))
            st.markdown("### Contextï¼ˆç¯€éŒ„ï¼‰")
            st.text((result.get("context", "") or "")[:12000])

    st.session_state.chat_history.append({"role": "assistant", "kind": "text", "content": result["final_answer"]})
