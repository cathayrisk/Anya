# app.py
# -*- coding: utf-8 -*-
from __future__ import annotations

import os
import re
import io
import uuid
import math
import time
import json
import hashlib
import threading
import ast
from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple, List
from concurrent.futures import ThreadPoolExecutor, as_completed

import streamlit as st
import numpy as np
import pandas as pd
import faiss
from pypdf import PdfReader

from openai import OpenAI

try:
    import fitz  # pymupdf
    HAS_PYMUPDF = True
except Exception:
    HAS_PYMUPDF = False

# =========================
# DeepAgents / LangChain importsï¼ˆå¯è¨ºæ–·ç‰ˆï¼‰
# =========================
HAS_DEEPAGENTS = False
DEEPAGENTS_IMPORT_ERRORS: list[str] = []

create_deep_agent = None
init_chat_model = None
ChatOpenAI = None

try:
    from deepagents import create_deep_agent as _create_deep_agent
    create_deep_agent = _create_deep_agent
except Exception as e:
    DEEPAGENTS_IMPORT_ERRORS.append(f"deepagents import failed: {repr(e)}")

try:
    # æ–°ç‰ˆ LangChain å¸¸ç”¨
    from langchain.chat_models import init_chat_model as _init_chat_model
    init_chat_model = _init_chat_model
except Exception as e:
    DEEPAGENTS_IMPORT_ERRORS.append(f"langchain.chat_models.init_chat_model import failed: {repr(e)}")

try:
    # fallbackï¼šç”¨ ChatOpenAIï¼ˆå¾ˆå¤šç’°å¢ƒæ˜¯é€™å€‹æœ€ç©©ï¼‰
    from langchain_openai import ChatOpenAI as _ChatOpenAI
    ChatOpenAI = _ChatOpenAI
except Exception as e:
    DEEPAGENTS_IMPORT_ERRORS.append(f"langchain_openai.ChatOpenAI import failed: {repr(e)}")

HAS_DEEPAGENTS = (create_deep_agent is not None) and ((init_chat_model is not None) or (ChatOpenAI is not None))


def _require_deepagents():
    if HAS_DEEPAGENTS:
        return
    st.error("DeepAgent ä¾è³´è¼‰å…¥å¤±æ•—ï¼ˆä¸ä¸€å®šæ˜¯æ²’å®‰è£ï¼Œå¯èƒ½æ˜¯ç‰ˆæœ¬/ä¾è³´ä¸ç›¸å®¹ï¼‰ã€‚")
    if DEEPAGENTS_IMPORT_ERRORS:
        st.markdown("### ä¾è³´éŒ¯èª¤ç´°ç¯€ï¼ˆè«‹æŠŠé€™æ®µè²¼çµ¦æˆ‘ï¼Œæˆ‘å°±èƒ½ç²¾æº–æŒ‡ä½ è©²è£å“ªå€‹ç‰ˆæœ¬ï¼‰")
        for msg in DEEPAGENTS_IMPORT_ERRORS:
            st.code(msg)
    else:
        st.info("ï¼ˆæ²’æœ‰æ•æ‰åˆ°éŒ¯èª¤ç´°ç¯€ï¼Œè«‹ç¢ºèª app.py æ˜¯å¦å·²æ•´æª”è¦†è“‹ç‚ºæœ€æ–°ç‰ˆï¼‰")
    st.stop()


def _make_langchain_llm(model_name: str, temperature: float = 0.0):
    """
    å›å‚³ LangChain çš„ chat model instanceï¼š
    - å„ªå…ˆ init_chat_model
    - fallback ChatOpenAI
    """
    if init_chat_model is not None:
        # init_chat_model ä¸€èˆ¬æ¥å— "openai:gpt-4o" é€™ç¨®æ ¼å¼
        # ä½ ç”¨ gpt-5.2 â†’ "openai:gpt-5.2"
        if model_name.startswith("openai:"):
            return init_chat_model(model=model_name, temperature=temperature)
        return init_chat_model(model=f"openai:{model_name}", temperature=temperature)

    if ChatOpenAI is not None:
        # ChatOpenAI ç›´æ¥ç”¨ "gpt-5.2"
        if model_name.startswith("openai:"):
            model_name = model_name.split("openai:", 1)[1]
        return ChatOpenAI(model=model_name, temperature=temperature)

    raise RuntimeError("No LangChain LLM factory available.")


# =========================
# Streamlit configï¼ˆåªå‘¼å«ä¸€æ¬¡ï¼‰
# =========================
st.set_page_config(page_title="ç ”ç©¶å ±å‘ŠåŠ©æ‰‹ï¼ˆDeepAgent + Badgesï¼‰", layout="wide")
st.title("ç ”ç©¶å ±å‘ŠåŠ©æ‰‹ï¼ˆDeepAgent + Badgesï¼‰")


# =========================
# å›ºå®šæ¨¡å‹è¨­å®š
# =========================
EMBEDDING_MODEL = "text-embedding-3-small"

MODEL_MAIN = "gpt-5.2"
MODEL_GRADER = "gpt-4.1-mini"
MODEL_WEB = "gpt-5.2"

# =========================
# æ•ˆèƒ½åƒæ•¸
# =========================
EMBED_BATCH_SIZE = 256
OCR_MAX_WORKERS = 2

CORPUS_DEFAULT_MAX_CHUNKS = 24
CORPUS_PER_REPORT_QUOTA = 6

# DeepAgent budgetsï¼ˆå¯é æ¸¬æˆæœ¬ï¼‰
DA_MAX_DOC_SEARCH_CALLS = 14
DA_MAX_WEB_SEARCH_CALLS = 4
DA_MAX_REWRITE_ROUNDS = 2
DA_MAX_CLAIMS = 10

# chunk_id leak guardï¼ˆåªæ“‹ chunk_id / _p.._c.. é€™é¡æ˜ç¢ºæ¨£å¼ï¼‰
CHUNK_ID_LEAK_PAT = re.compile(r"(chunk_id\s*=\s*|_p(?:na|\d+)_c\d+)", re.IGNORECASE)


# =========================
# å°å·¥å…·
# =========================
def norm_space(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())


def estimate_tokens_from_chars(n_chars: int) -> int:
    if n_chars <= 0:
        return 0
    return max(1, int(math.ceil(n_chars / 3.6)))


def chunk_text(text: str, chunk_size: int = 900, overlap: int = 150) -> list[str]:
    text = norm_space(text)
    if not text:
        return []
    out = []
    i = 0
    while i < len(text):
        j = min(len(text), i + chunk_size)
        out.append(text[i:j])
        if j == len(text):
            break
        i = max(0, j - overlap)
    return out


def sha1_bytes(data: bytes) -> str:
    return hashlib.sha1(data).hexdigest()


def sha1_text(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()[:10]


def truncate_filename(name: str, max_len: int = 44) -> str:
    if len(name) <= max_len:
        return name
    base, ext = os.path.splitext(name)
    keep = max(10, max_len - len(ext) - 1)
    return f"{base[:keep]}â€¦{ext}"


# =========================
# OpenAI client + wrappers
# =========================
def get_openai_api_key() -> str:
    if "OPENAI_KEY" in st.secrets and st.secrets["OPENAI_KEY"]:
        return st.secrets["OPENAI_KEY"]
    if os.environ.get("OPENAI_API_KEY"):
        return os.environ["OPENAI_API_KEY"]
    if os.environ.get("OPENAI_KEY"):
        return os.environ["OPENAI_KEY"]
    raise RuntimeError("Missing OpenAI API key. Set st.secrets['OPENAI_KEY'] or env OPENAI_API_KEY.")


def get_client(api_key: str) -> OpenAI:
    return OpenAI(api_key=api_key)


def embed_texts(client: OpenAI, texts: list[str]) -> np.ndarray:
    resp = client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=texts,
        encoding_format="float",
    )
    vecs = np.array([d.embedding for d in resp.data], dtype=np.float32)
    norms = np.linalg.norm(vecs, axis=1, keepdims=True)
    norms = np.where(norms == 0, 1.0, norms)
    return vecs / norms


def _to_messages(system: str, user: Any) -> list[Dict[str, Any]]:
    return [{"role": "system", "content": system}, {"role": "user", "content": user}]


def call_gpt(
    client: OpenAI,
    *,
    model: str,
    system: str,
    user: Any,
    effort: str = "medium",
    tools: Optional[list] = None,
    include_sources: bool = False
) -> Tuple[str, Optional[list[Dict[str, Any]]]]:
    messages = _to_messages(system, user)
    resp = client.responses.create(
        model=model,
        input=messages,
        tools=tools,
        tool_choice="auto" if tools else "none",
        parallel_tool_calls=True if tools else None,
        reasoning={"effort": effort} if model.startswith("gpt-") else None,
        include=["web_search_call.action.sources"] if (tools and include_sources) else None,
        truncation="auto",
    )
    out_text = resp.output_text
    sources = None
    if tools and include_sources:
        try:
            sources_list = []
            if hasattr(resp, "output") and resp.output:
                for item in resp.output:
                    d = item if isinstance(item, dict) else getattr(item, "__dict__", {})
                    if isinstance(d, dict) and d.get("type") == "web_search_call":
                        action = d.get("action", {}) or {}
                        if isinstance(action, dict) and action.get("sources"):
                            sources_list.extend(action["sources"])
            sources = sources_list if sources_list else None
        except Exception:
            sources = None
    return out_text, sources


# =========================
# æª”æ¡ˆ / OCR
# =========================
@dataclass
class FileRow:
    file_id: str
    file_sig: str
    name: str
    ext: str
    bytes_len: int
    pages: Optional[int]
    extracted_chars: int
    token_est: int

    text_pages: Optional[int]
    text_pages_ratio: Optional[float]

    blank_pages: Optional[int]
    blank_ratio: Optional[float]
    likely_scanned: bool
    use_ocr: bool


def extract_pdf_text_pages_pypdf(pdf_bytes: bytes) -> list[Tuple[int, str]]:
    reader = PdfReader(io.BytesIO(pdf_bytes))
    out = []
    for i, p in enumerate(reader.pages):
        try:
            t = p.extract_text() or ""
        except Exception:
            t = ""
        out.append((i + 1, norm_space(t)))
    return out


def extract_pdf_text_pages_pymupdf(pdf_bytes: bytes) -> list[Tuple[int, str]]:
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    out = []
    for i in range(doc.page_count):
        page = doc.load_page(i)
        t = page.get_text("text") or ""
        out.append((i + 1, norm_space(t)))
    return out


def extract_pdf_text_pages(pdf_bytes: bytes) -> list[Tuple[int, str]]:
    if HAS_PYMUPDF:
        try:
            return extract_pdf_text_pages_pymupdf(pdf_bytes)
        except Exception:
            return extract_pdf_text_pages_pypdf(pdf_bytes)
    return extract_pdf_text_pages_pypdf(pdf_bytes)


def analyze_pdf_text_quality(
    pdf_pages: list[Tuple[int, str]],
    min_chars_per_page: int = 40,
) -> Tuple[int, int, float, int, float]:
    if not pdf_pages:
        return 0, 0, 1.0, 0, 0.0
    lens = [len(t) for _, t in pdf_pages]
    blank = sum(1 for L in lens if L <= min_chars_per_page)
    total_pages = max(1, len(lens))
    blank_ratio = blank / total_pages
    text_pages = total_pages - blank
    text_pages_ratio = text_pages / total_pages
    return sum(lens), blank, blank_ratio, text_pages, text_pages_ratio


def should_suggest_ocr(ext: str, pages: Optional[int], extracted_chars: int, blank_ratio: Optional[float]) -> bool:
    if ext != ".pdf":
        return False
    if pages is None or pages <= 0:
        return True
    if blank_ratio is not None and blank_ratio >= 0.6:
        return True
    avg = extracted_chars / max(1, pages)
    return avg < 120


def ocr_image_bytes(client: OpenAI, image_bytes: bytes) -> str:
    system = "ä½ æ˜¯ä¸€å€‹OCRå·¥å…·ã€‚åªè¼¸å‡ºå¯è¦‹æ–‡å­—èˆ‡è¡¨æ ¼å…§å®¹ï¼ˆè‹¥æœ‰è¡¨æ ¼ç”¨ Markdown è¡¨æ ¼ï¼‰ã€‚ä¸­æ–‡è«‹ç”¨ç¹é«”ä¸­æ–‡ã€‚ä¸è¦åŠ è©•è«–ã€‚"
    user_content = [
        {"type": "input_text", "text": "è«‹æ“·å–åœ–ç‰‡ä¸­æ‰€æœ‰å¯è¦‹æ–‡å­—ï¼ˆåŒ…å«å°å­—/è¨»è…³ï¼‰ã€‚è‹¥ç„¡æ³•è¾¨è­˜è«‹æ¨™è¨˜[ç„¡æ³•è¾¨è­˜]ã€‚"},
        {"type": "input_image", "image_bytes": image_bytes},
    ]
    text, _ = call_gpt(client, model=MODEL_GRADER, system=system, user=user_content, effort="none")
    return text


def ocr_pdf_pages_parallel(client: OpenAI, pdf_bytes: bytes, dpi: int = 180) -> list[Tuple[int, str]]:
    if not HAS_PYMUPDF:
        raise RuntimeError("æœªå®‰è£ pymupdfï¼ˆfitzï¼‰ï¼Œç„¡æ³•åš PDF OCRã€‚è«‹ pip install pymupdf")

    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    zoom = dpi / 72.0
    mat = fitz.Matrix(zoom, zoom)

    def render_page(i: int) -> Tuple[int, bytes]:
        page = doc.load_page(i)
        pix = page.get_pixmap(matrix=mat, alpha=False)
        return i + 1, pix.tobytes("png")

    page_imgs = [render_page(i) for i in range(doc.page_count)]
    results: Dict[int, str] = {}

    with ThreadPoolExecutor(max_workers=OCR_MAX_WORKERS) as ex:
        futs = {ex.submit(ocr_image_bytes, client, img_bytes): page_no for page_no, img_bytes in page_imgs}
        for fut in as_completed(futs):
            page_no = futs[fut]
            try:
                results[page_no] = norm_space(fut.result())
            except Exception:
                results[page_no] = ""

    return [(p, results.get(p, "")) for p, _ in page_imgs]


# =========================
# FAISS store
# =========================
@dataclass
class Chunk:
    chunk_id: str
    report_id: str
    title: str
    page: Optional[int]
    text: str


class FaissStore:
    def __init__(self, dim: int):
        self.index = faiss.IndexFlatIP(dim)
        self.chunks: list[Chunk] = []

    def add(self, vecs: np.ndarray, chunks: list[Chunk]) -> None:
        self.index.add(vecs)
        self.chunks.extend(chunks)

    def search(self, qvec: np.ndarray, k: int = 8) -> list[Tuple[float, Chunk]]:
        if self.index.ntotal == 0:
            return []
        scores, idx = self.index.search(qvec.astype(np.float32), k)
        out = []
        for s, i in zip(scores[0], idx[0]):
            if i < 0 or i >= len(self.chunks):
                continue
            out.append((float(s), self.chunks[i]))
        return out


# =========================
# å¼•ç”¨ badgeï¼ˆåªé¡¯ç¤º [title pX]ï¼‰
# =========================
CIT_RE = re.compile(r"\[[^\]]+?\s+p(\d+|-)\s*\]")
BULLET_RE = re.compile(r"^\s*(?:[-â€¢*]|\d+\.)\s+")
CIT_PARSE_RE = re.compile(r"\[([^\]]+?)\s+p(\d+|-)\s*\]")


def _parse_citations(cits: list[str]) -> list[Dict[str, str]]:
    parsed = []
    for c in cits:
        m = CIT_PARSE_RE.search(c)
        if m:
            parsed.append({"title": m.group(1).strip(), "page": m.group(2).strip()})
    return parsed


def _badge_directive(label: str, color: str) -> str:
    safe = label.replace("[", "(").replace("]", ")")
    return f":{color}-badge[{safe}]"


def render_bullets_inline_badges(md_bullets: str, badge_color: str = "green"):
    lines = [l.rstrip() for l in (md_bullets or "").splitlines() if l.strip()]
    for line in lines:
        if not BULLET_RE.match(line):
            continue
        full_cits = [m.group(0) for m in re.finditer(r"\[[^\]]+?\s+p(\d+|-)\s*\]", line)]
        clean = re.sub(r"\[[^\]]+?\s+p(\d+|-)\s*\]", "", line).strip()
        parsed = _parse_citations(full_cits)
        badges = [_badge_directive(f"{it['title']} p{it['page']}", badge_color) for it in parsed]
        st.markdown(clean + (" " + " ".join(badges) if badges else ""))


def render_text_with_badges(md_text: str, badge_color: str = "gray"):
    cits = [m.group(0) for m in re.finditer(r"\[[^\]]+?\s+p(\d+|-)\s*\]", md_text or "")]
    clean = re.sub(r"\[[^\]]+?\s+p(\d+|-)\s*\]", "", md_text or "").strip()
    st.markdown(clean if clean else "ï¼ˆç„¡å…§å®¹ï¼‰")
    parsed = _parse_citations(sorted(set(cits)))
    if parsed:
        badges = [_badge_directive(f"{it['title']} p{it['page']}", badge_color) for it in parsed]
        st.markdown("ä¾†æºï¼š" + " ".join(badges))


def bullets_all_have_citations(md: str) -> bool:
    lines = (md or "").splitlines()
    if not any(BULLET_RE.match(l) for l in lines):
        return False
    for line in lines:
        if BULLET_RE.match(line) and not CIT_RE.search(line):
            return False
    return True


def any_bullets(md: str) -> bool:
    return any(BULLET_RE.match(l) for l in (md or "").splitlines())


# =========================
# QA é¡¯ç¤ºï¼šä¸€èˆ¬ Markdown + ä¾†æºè† å›Šï¼ˆä¸é¡¯ç¤º JSONï¼‰
# =========================
def _try_parse_json_or_py_literal(text: str) -> Optional[Any]:
    t = (text or "").strip()
    if not t:
        return None

    # JSON
    if t.startswith("{") or t.startswith("["):
        try:
            return json.loads(t)
        except Exception:
            pass

    # Python dict literalï¼ˆdeepagents æœ‰æ™‚æœƒå‡ºç¾ {'content': '...'}ï¼‰
    if t.startswith("{") and t.endswith("}"):
        try:
            return ast.literal_eval(t)
        except Exception:
            return None

    return None


def _extract_main_text_from_payload(payload: Any) -> Optional[str]:
    if isinstance(payload, dict):
        for k in ("content", "answer", "final", "output", "text", "message"):
            v = payload.get(k)
            if isinstance(v, str) and v.strip():
                return v
        # æœ‰äº›æœƒæŠŠ messages æ”¾åœ¨ list
        msgs = payload.get("messages")
        if isinstance(msgs, list) and msgs:
            last = msgs[-1]
            if isinstance(last, dict):
                c = last.get("content")
                if isinstance(c, str) and c.strip():
                    return c
            return str(last)
        return None

    if isinstance(payload, list):
        # å¦‚æœæ˜¯ list[str] å°±ä¸²èµ·ä¾†
        if all(isinstance(x, str) for x in payload):
            return "\n".join([x for x in payload if x.strip()])
        return str(payload)

    return None


def _dedup_keep_order(items: list[str]) -> list[str]:
    seen = set()
    out = []
    for x in items:
        if x in seen:
            continue
        seen.add(x)
        out.append(x)
    return out


def _extract_citation_strings(text: str) -> list[str]:
    if not text:
        return []
    return [m.group(0) for m in re.finditer(r"\[[^\]]+?\s+p(\d+|-)\s*\]", text)]


def _strip_citations_from_text(text: str) -> str:
    if not text:
        return ""
    # ç§»é™¤å¼•ç”¨ï¼Œä½†ä¿ç•™ Markdown çµæ§‹
    return re.sub(r"\s*\[[^\]]+?\s+p(\d+|-)\s*\]\s*", " ", text).strip()


def _group_citations_for_badges(cits: list[str]) -> dict[str, list[str]]:
    grouped: dict[str, list[str]] = {}
    for c in cits:
        m = CIT_PARSE_RE.search(c)
        if not m:
            continue
        title = m.group(1).strip()
        page = m.group(2).strip()
        grouped.setdefault(title, []).append(page)

    # å»é‡ã€æ’åºï¼ˆæ•¸å­—é ç¢¼å…ˆï¼Œ'-' å¾Œï¼‰
    for title, pages in grouped.items():
        pages = _dedup_keep_order(pages)

        def _key(p: str):
            if p.isdigit():
                return (0, int(p))
            if p == "-":
                return (1, 10**9)
            return (2, p)

        grouped[title] = sorted(pages, key=_key)

    return grouped


def _format_pages_compact(pages: list[str], max_keep: int = 4) -> str:
    """
    æŠŠ pages è®ŠæˆçŸ­å­—ä¸²ï¼Œä¾‹å¦‚ï¼š
      ['1','2','3','6'] -> 'p1,2,3,6'
      ['1','2','3','4','5','6'] -> 'p1,2,3,+3'
    """
    if not pages:
        return "p-"
    if len(pages) <= max_keep:
        return "p" + ",".join(pages)
    keep_n = max(1, max_keep - 1)  # ä¿ç•™å‰ N-1 å€‹ï¼Œæœ€å¾Œç”¨ +n
    kept = pages[:keep_n]
    more = len(pages) - keep_n
    return "p" + ",".join(kept) + f",+{more}"


def render_markdown_answer_with_source_badges(answer_text: str, badge_color: str = "green"):
    """
    QA å°ˆç”¨ï¼š
    - æŠŠ JSON / Python dict å­—ä¸²è½‰æˆä¸€èˆ¬ Markdown é¡¯ç¤º
    - ä¾†æºç”¨è† å›Š badgeï¼ˆé›†ä¸­åœ¨ä¸‹æ–¹ï¼‰
    """
    raw = (answer_text or "").strip()

    # é˜² chunk_id å¤–æ´©
    if raw and CHUNK_ID_LEAK_PAT.search(raw):
        raw = CHUNK_ID_LEAK_PAT.sub("", raw)

    payload = _try_parse_json_or_py_literal(raw)
    if payload is not None:
        extracted = _extract_main_text_from_payload(payload)
        if extracted is not None:
            raw = extracted.strip()

    cits = _dedup_keep_order(_extract_citation_strings(raw))
    clean = _strip_citations_from_text(raw)

    # é¡¯ç¤ºæ­£æ–‡ï¼ˆä¸€èˆ¬ Markdownï¼‰
    st.markdown(clean if clean else "ï¼ˆç„¡å…§å®¹ï¼‰")

    # é¡¯ç¤ºä¾†æº badgeï¼ˆåˆä½µé ç¢¼ï¼‰
    if cits:
        grouped = _group_citations_for_badges(cits)
        badges = []
        for title in sorted(grouped.keys()):
            pages = grouped[title]
            label = f"{title} {_format_pages_compact(pages, max_keep=4)}"
            badges.append(_badge_directive(label, badge_color))
        if badges:
            st.markdown("ä¾†æºï¼š" + " ".join(badges))


def render_debug_panel(files: Optional[dict]):
    """Debug å€å¡Šï¼štabs + å…§å®¹é è¦½ï¼Œé¿å…å™´ä¸€å¤§å¨ keysã€‚"""
    if not files or not isinstance(files, dict):
        st.write("ï¼ˆæ²’æœ‰ filesï¼‰")
        return

    def _file_to_str(file_obj) -> str:
        if isinstance(file_obj, dict) and "data" in file_obj:
            v = file_obj["data"]
            if isinstance(v, (bytes, bytearray)):
                return v.decode("utf-8", errors="ignore")
            return str(v)
        if isinstance(file_obj, (bytes, bytearray)):
            return file_obj.decode("utf-8", errors="ignore")
        return str(file_obj)

    def _sanitize_text(t: str) -> str:
        t = (t or "").strip()
        if not t:
            return ""
        if CHUNK_ID_LEAK_PAT.search(t):
            t = CHUNK_ID_LEAK_PAT.sub("", t)
        # å¦‚æœå…§å®¹æœ¬èº«æ˜¯ dict/JSON å­—ä¸²ï¼Œä¹Ÿå˜—è©¦æŠ½ content
        payload = _try_parse_json_or_py_literal(t)
        if payload is not None:
            extracted = _extract_main_text_from_payload(payload)
            if isinstance(extracted, str) and extracted.strip():
                t = extracted.strip()
                if CHUNK_ID_LEAK_PAT.search(t):
                    t = CHUNK_ID_LEAK_PAT.sub("", t)
        return t

    all_keys = sorted([k for k in files.keys() if isinstance(k, str)])
    evidence_keys = [k for k in all_keys if k.startswith("/evidence/")]

    draft = _sanitize_text(_file_to_str(files.get("/draft.md", ""))) if "/draft.md" in files else ""
    review = _sanitize_text(_file_to_str(files.get("/review.md", ""))) if "/review.md" in files else ""

    tab1, tab2, tab3, tab4 = st.tabs(["ç¸½è¦½", "draft.md", "review.md", "evidence"])

    with tab1:
        st.write(f"files keysï¼š{len(all_keys)}")
        st.write(f"evidenceï¼š{len(evidence_keys)}")
        st.code("\n".join(all_keys[:400]), language="text")

    with tab2:
        if draft:
            st.code(draft[:20000], language="markdown")
        else:
            st.write("ï¼ˆæ²’æœ‰ /draft.mdï¼‰")

    with tab3:
        if review:
            st.code(review[:20000], language="markdown")
        else:
            st.write("ï¼ˆæ²’æœ‰ /review.mdï¼‰")

    with tab4:
        if evidence_keys:
            st.code("\n".join(evidence_keys[:600]), language="text")
        else:
            st.write("ï¼ˆæ²’æœ‰ /evidence/ æª”æ¡ˆï¼‰")


# =========================
# Indexingï¼ˆå¢é‡ï¼šOCR + embeddingsï¼‰
# =========================
def build_indices_incremental_no_kg(
    client: OpenAI,
    file_rows: list[FileRow],
    file_bytes_map: Dict[str, bytes],
    store: Optional[FaissStore],
    processed_keys: set,
    chunk_size: int = 900,
    overlap: int = 150,
) -> Tuple[FaissStore, Dict[str, Any], set]:
    if store is None:
        dim = embed_texts(client, ["dim_probe"]).shape[1]
        store = FaissStore(dim)

    stats = {"new_reports": 0, "new_chunks": 0}
    new_chunks: list[Chunk] = []
    new_texts: list[str] = []

    to_process = []
    for r in file_rows:
        key = (r.file_sig, bool(r.use_ocr))
        if key not in processed_keys:
            to_process.append(r)

    for row in to_process:
        data = file_bytes_map[row.file_id]
        report_id = row.file_id
        title = os.path.splitext(row.name)[0]
        stats["new_reports"] += 1

        if row.ext == ".pdf":
            if row.use_ocr:
                pages = ocr_pdf_pages_parallel(client, data)
            else:
                pages = extract_pdf_text_pages(data)
        elif row.ext == ".txt":
            pages = [(None, norm_space(data.decode("utf-8", errors="ignore")))]
        elif row.ext in (".png", ".jpg", ".jpeg"):
            txt = norm_space(ocr_image_bytes(client, data))
            pages = [(None, txt)]
        else:
            pages = [(None, "")]

        for page_no, page_text in pages:
            if not page_text:
                continue
            for i, ch in enumerate(chunk_text(page_text, chunk_size=chunk_size, overlap=overlap)):
                cid = f"{report_id}_p{page_no if page_no else 'na'}_c{i}"
                new_chunks.append(Chunk(cid, report_id, title, page_no if isinstance(page_no, int) else None, ch))
                new_texts.append(ch)

        processed_keys.add((row.file_sig, bool(row.use_ocr)))

    if new_texts:
        vecs_list = []
        for i in range(0, len(new_texts), EMBED_BATCH_SIZE):
            vecs_list.append(embed_texts(client, new_texts[i:i + EMBED_BATCH_SIZE]))
        vecs = np.vstack(vecs_list)
        store.add(vecs, new_chunks)

    stats["new_chunks"] = len(new_chunks)
    return store, stats, processed_keys


# =========================
# é è¨­è¼¸å‡ºï¼ˆä¸€æ¬¡ä¸‰ä»½ï¼‰
# =========================
def _split_default_bundle(text: str) -> Dict[str, str]:
    t = (text or "").strip()
    pattern = re.compile(
        r"###\s*SUMMARY\s*(.*?)###\s*CLAIMS\s*(.*?)###\s*CHAIN\s*(.*)$",
        re.IGNORECASE | re.DOTALL,
    )
    m = pattern.search(t)
    if not m:
        return {"summary": "", "claims": "", "chain": ""}
    return {"summary": m.group(1).strip(), "claims": m.group(2).strip(), "chain": m.group(3).strip()}


def pick_corpus_chunks_for_default(all_chunks: list[Chunk]) -> list[Chunk]:
    by_title: Dict[str, list[Chunk]] = {}
    for c in all_chunks:
        by_title.setdefault(c.title, []).append(c)

    kw = re.compile(
        r"(outlook|risk|implication|forecast|scenario|inflation|rate|credit|spread|cap rate|vacancy|supply|demand|rental|office|retail|residential|logistics|hotel|reits)",
        re.I,
    )

    def score(c: Chunk) -> float:
        s = 0.0
        if kw.search(c.text or ""):
            s += 6.0
        if c.page is not None:
            s += max(0.0, 2.0 - min(2.0, float(c.page) / 6.0))
        s += min(2.0, len(c.text) / 1400.0)
        return s

    chosen: list[Chunk] = []
    for title, chunks in sorted(by_title.items(), key=lambda x: x[0]):
        by_page: Dict[int, list[Chunk]] = {}
        for c in chunks:
            p = c.page if c.page is not None else 0
            by_page.setdefault(p, []).append(c)

        page_best = []
        for p, cs in by_page.items():
            cs = sorted(cs, key=score, reverse=True)
            page_best.append(cs[0])

        page_best = sorted(page_best, key=score, reverse=True)
        chosen.extend(page_best[:CORPUS_PER_REPORT_QUOTA])

    chosen = sorted(chosen, key=score, reverse=True)[:CORPUS_DEFAULT_MAX_CHUNKS]
    return chosen


def render_chunks_for_model(chunks: list[Chunk], max_chars_each: int = 900) -> str:
    parts = []
    for c in chunks:
        head = f"[{c.title} p{c.page if c.page is not None else '-'}]"
        parts.append(head + "\n" + c.text[:max_chars_each])
    return "\n\n".join(parts)


def generate_default_outputs_bundle(client: OpenAI, title: str, ctx: str, max_retries: int = 2) -> Dict[str, str]:
    system = (
        "ä½ æ˜¯åš´è¬¹çš„ç ”ç©¶åŠ©ç†ï¼Œåªèƒ½æ ¹æ“šæˆ‘æä¾›çš„è³‡æ–™å›ç­”ï¼Œä¸å¯è…¦è£œã€‚\n"
        "ç¡¬æ€§è¦å‰‡ï¼š\n"
        "1) ä½ å¿…é ˆè¼¸å‡ºä¸‰å€‹å€å¡Šï¼Œä¸”é †åº/æ¨™é¡Œå›ºå®šï¼š### SUMMARYã€### CLAIMSã€### CHAINã€‚\n"
        "2) æ¯å€‹å€å¡Šéƒ½å¿…é ˆæ˜¯ç´” bulletï¼ˆæ¯è¡Œä»¥ - é–‹é ­ï¼‰ï¼Œä¸è¦æ®µè½ã€‚\n"
        "3) æ¯å€‹ bullet å¥å°¾å¿…é ˆé™„å¼•ç”¨ï¼Œæ ¼å¼å›ºå®šï¼š[å ±å‘Šåç¨± pé ]\n"
        "4) å¼•ç”¨ä¸­çš„ã€å ±å‘Šåç¨±ã€å¿…é ˆæ˜¯è³‡æ–™ç‰‡æ®µæ–¹æ‹¬è™Ÿå…§çš„é‚£å€‹åç¨±ã€‚\n"
    )
    user = (
        f"è«‹é‡å°ã€Š{title}ã€‹ä¸€æ¬¡è¼¸å‡ºä¸‰ä»½å…§å®¹ï¼ˆèåˆå¤šä»½å ±å‘Šï¼‰ï¼š\n"
        f"- SUMMARYï¼š8~14 bullets\n"
        f"- CLAIMSï¼š8~14 bullets\n"
        f"- CHAINï¼š6~12 bullets\n\n"
        f"è³‡æ–™ï¼š\n{ctx}\n"
    )

    last = ""
    for _ in range(max_retries + 1):
        out, _ = call_gpt(client, model=MODEL_MAIN, system=system, user=user, effort="medium")
        parts = _split_default_bundle(out)
        ok = bullets_all_have_citations(parts["summary"]) and bullets_all_have_citations(parts["claims"]) and bullets_all_have_citations(parts["chain"])
        if ok:
            return parts
        last = out
        user += "\n\nã€å¼·åˆ¶ä¿®æ­£ã€‘æ•´ä»½é‡å¯«ï¼šä¸‰å€å¡Šçš†ç‚ºç´” bulletï¼Œä¸”æ¯å€‹ bullet å¥å°¾éƒ½æœ‰ [å ±å‘Šåç¨± pé ]ã€‚"
    return _split_default_bundle(last)


# =========================
# DeepAgentï¼ˆchunk_id åªåœ¨ tool JSON å…§éƒ¨ä½¿ç”¨ï¼‰
# =========================
def ensure_deep_agent(client: OpenAI, store: FaissStore, enable_web: bool):
    _require_deepagents()

    # é‡è¦ï¼šä¸€å®šè¦ç”¨ langchain_core çš„ BaseTool/StructuredTool
    # ToolNode æœƒç”¨ isinstance(tool, BaseTool) åˆ¤æ–·ï¼›å¦‚æœä½ ç”¨åˆ°èˆŠè·¯å¾‘ç”¢ç”Ÿçš„ toolï¼Œå¯èƒ½æœƒè¢«ç•¶æˆã€Œä¸æ˜¯ BaseToolã€åˆå» convert ä¸€æ¬¡è€Œç‚¸ã€‚
    from langchain_core.tools import BaseTool, StructuredTool

    if "deep_agent" not in st.session_state:
        st.session_state.deep_agent = None
    if "deep_agent_web_flag" not in st.session_state:
        st.session_state.deep_agent_web_flag = None

    if (st.session_state.deep_agent is not None) and (st.session_state.deep_agent_web_flag == bool(enable_web)):
        return st.session_state.deep_agent

    # å¦‚æœéƒ¨ç½²ç”¨äº† -OOï¼Œdocstring æœƒè¢«ç§»é™¤ï¼›æˆ‘å€‘èµ° StructuredTool+description å°±ä¸æ€•
    try:
        import sys
        if getattr(sys, "flags", None) and getattr(sys.flags, "optimize", 0) >= 2:
            st.caption("âš ï¸ åµæ¸¬åˆ° Python optimize(-OO) å¯èƒ½ç§»é™¤ docstringï¼›å·²æ”¹ç”¨ StructuredTool(description) é¿å…æ­¤å•é¡Œã€‚")
    except Exception:
        pass

    lock = threading.Lock()
    usage = {"doc_search_calls": 0, "web_search_calls": 0}

    def _inc(name: str, limit: int) -> bool:
        """
        å›å‚³ True è¡¨ç¤ºé‚„åœ¨ budget å…§ï¼›
        å›å‚³ False è¡¨ç¤ºè¶…å‡º budgetï¼ˆä¸ raiseï¼Œé¿å…æ•´å€‹ agent graph çˆ†æ‰ï¼‰ã€‚
        """
        with lock:
            usage[name] += 1
            return usage[name] <= limit
            
    # -------------------------
    # åŸå§‹å·¥å…·å‡½å¼ï¼ˆç¶­æŒä½ åŸæœ¬é‚è¼¯ï¼‰
    # -------------------------
    def _get_usage_fn() -> str:
        with lock:
            return json.dumps(usage, ensure_ascii=False)

    def _doc_list_fn() -> str:
        by_title: Dict[str, int] = {}
        for c in store.chunks:
            by_title[c.title] = by_title.get(c.title, 0) + 1
        lines = [f"- {t} (chunks={n})" for t, n in sorted(by_title.items(), key=lambda x: x[0])]
        return "\n".join(lines) if lines else "ï¼ˆç›®å‰æ²’æœ‰ä»»ä½•å·²ç´¢å¼•æ–‡ä»¶ï¼‰"

    def _doc_search_fn(query: str, k: int = 8) -> str:
        _inc("doc_search_calls", DA_MAX_DOC_SEARCH_CALLS)

        if not _inc("doc_search_calls", DA_MAX_DOC_SEARCH_CALLS):
            # ä¸ raiseï¼Œå›å‚³å¯è®€çš„ error è®“ agent çŸ¥é“è¦åœ
            return json.dumps(
                {
                    "hits": [],
                    "error": f"Budget exceeded: doc_search_calls > {DA_MAX_DOC_SEARCH_CALLS}",
                },
                ensure_ascii=False,
            )
        
        q = (query or "").strip()
        if not q:
            return json.dumps({"hits": []}, ensure_ascii=False)

        qvec = embed_texts(client, [q])
        hits = store.search(qvec, k=max(1, min(12, int(k))))

        payload = {"hits": []}
        for score, ch in hits:
            payload["hits"].append({
                "title": ch.title,
                "page": str(ch.page) if ch.page is not None else "-",
                "chunk_id": ch.chunk_id,  # internal only
                "text": (ch.text or "")[:1200],
            })
        return json.dumps(payload, ensure_ascii=False)

    def _doc_get_chunk_fn(chunk_id: str, max_chars: int = 2600) -> str:
        cid = (chunk_id or "").strip()
        if not cid:
            return ""
        for c in store.chunks:
            if c.chunk_id == cid:
                return (c.text or "")[:max_chars]
        return ""

    # -------------------------
    # é—œéµä¿®æ­£ï¼šå…¨éƒ¨åŒ…æˆ BaseToolï¼ˆæ˜ç¢º descriptionï¼‰
    # -------------------------
    def _mk_tool(fn, name: str, description: str) -> BaseTool:
        # ä¸€å¾‹é¡¯å¼æä¾› descriptionï¼Œå®Œå…¨ä¸ä¾è³´ docstring
        return StructuredTool.from_function(fn, name=name, description=description)

    tool_get_usage = _mk_tool(
        _get_usage_fn,
        "get_usage",
        "Get current tool usage counters as JSON (budget/debug).",
    )
    tool_doc_list = _mk_tool(
        _doc_list_fn,
        "doc_list",
        "List indexed documents and chunk counts.",
    )
    tool_doc_search = _mk_tool(
        _doc_search_fn,
        "doc_search",
        "Semantic search over indexed chunks. Returns JSON hits with title/page/chunk_id/text (chunk_id is internal).",
    )
    tool_doc_get_chunk = _mk_tool(
        _doc_get_chunk_fn,
        "doc_get_chunk",
        "Fetch full text for a given chunk_id for close reading. Returns text only.",
    )

    tools: list[BaseTool] = [tool_get_usage, tool_doc_list, tool_doc_search, tool_doc_get_chunk]

    tool_web_search_summary: Optional[BaseTool] = None
    if enable_web:
        def _web_search_summary_fn(query: str) -> str:
            _inc("web_search_calls", DA_MAX_WEB_SEARCH_CALLS)
            q = (query or "").strip()
            if not q:
                return "ï¼ˆquery ç‚ºç©ºï¼‰"

            system = (
                "ä½ æ˜¯ç ”ç©¶åŠ©ç†ã€‚ç”¨ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰æ•´ç† web_search çµæœæˆ 2-4 æ®µæ‘˜è¦ï¼Œä¿ç•™æ—¥æœŸ/åè©ã€‚"
                "è‹¥çŸ›ç›¾è¦æŒ‡å‡ºã€‚æœ€å¾Œç”¨ Sources: åˆ—å‡º title + urlã€‚"
            )
            user = f"Search term: {q}"
            text, sources = call_gpt(
                client,
                model=MODEL_WEB,
                system=system,
                user=user,
                effort="medium",
                tools=[{"type": "web_search"}],
                include_sources=True,
            )
            src_lines = []
            for s in (sources or [])[:8]:
                if isinstance(s, dict):
                    t = s.get("title") or s.get("source") or "source"
                    u = s.get("url") or ""
                    if u:
                        src_lines.append(f"- {t} {u}".strip())

            out_text = (text or "").strip()
            if src_lines:
                out_text = (out_text + "\n\nSources:\n" + "\n".join(src_lines)).strip()
            return f"[WebSearch:{q[:30]} p-]\n" + out_text[:2400]

        tool_web_search_summary = _mk_tool(
            _web_search_summary_fn,
            "web_search_summary",
            "Run lightweight web_search and return a short Traditional Chinese summary with sources.",
        )
        tools.append(tool_web_search_summary)

    # é˜²å‘†ï¼šå¦‚æœé€™è£¡é‚„æ··é€²è£¸ functionï¼Œç›´æ¥åœ¨ UI é¡¯ç¤ºï¼Œè€Œä¸æ˜¯è®“å®ƒåœ¨ langchain å…§éƒ¨çˆ†ç‚¸é‚„è¢« redacted
    bad = [t for t in tools if not isinstance(t, BaseTool)]
    if bad:
        st.error("Internal error: tools list contains non-BaseTool items.ï¼ˆé¿å…è¢« Streamlit redactedï¼Œé€™è£¡å…ˆæ””æˆªï¼‰")
        st.code("\n".join([f"- {repr(x)} type={type(x)}" for x in bad])[:8000])
        st.stop()

    # -------------------------
    # promptsï¼ˆæ²¿ç”¨ä½ åŸæœ¬å…§å®¹ï¼‰
    # -------------------------
    retriever_prompt = f"""
ä½ æ˜¯æ–‡ä»¶æª¢ç´¢å°ˆå®¶ï¼ˆåªå…è¨±ä½¿ç”¨ doc_list/doc_search/doc_get_chunk/get_usageï¼‰ã€‚

facet å­ä»»å‹™æ ¼å¼ï¼š
facet_slug: <è‹±æ–‡å°å¯«_åº•ç·š>
facet_goal: <é€™å€‹é¢å‘è¦å›ç­”ä»€éº¼>
hints: <å¯èƒ½çš„é—œéµå­—/æŒ‡æ¨™/åè©ï¼ˆå¯ç©ºï¼‰>

ç¡¬è¦å‰‡ï¼š
- ä½ è¦å¯«å…¥ /evidence/doc_<facet_slug>.md
- evidence å…§å®¹åªèƒ½åŒ…å«ï¼š
  1) å¼•ç”¨æ¨™é ­ï¼š[å ±å‘Šåç¨± pé ]ï¼ˆçµ•å°ä¸èƒ½å‡ºç¾ chunk_idï¼‰
  2) åŸæ–‡ç‰‡æ®µï¼ˆå¯æˆªæ–·ï¼‰
  3) ä¸€è¡Œèªªæ˜ã€Œé€™æ®µæ”¯æŒä»€éº¼ã€
- ä½ å¯ä»¥ç”¨ doc_search æ‹¿åˆ° chunk_idï¼Œç„¶å¾Œç”¨ doc_get_chunk(chunk_id=...) ç²¾è®€ï¼Œ
  ä½† chunk_id çµ•å°ä¸èƒ½å¯«é€² evidenceã€‚

è‹¥é‡åˆ° Budget exceededï¼šç«‹åˆ»åœæ­¢ä¸¦åœ¨ evidence æ˜è¬›ã€Œè­‰æ“šä¸è¶³ã€ã€‚
æœ€å¾Œå›è¦† orchestratorï¼šâ‰¤150 å­—æ‘˜è¦ï¼ˆæ‰¾åˆ°ä»€éº¼ + æœ€å¤§ç¼ºå£ï¼‰
"""

    writer_prompt = f"""
ä½ æ˜¯å¯«ä½œ/æ•´ç†å°ˆå®¶ï¼ˆç”¨ read_file/glob/grep/write_file/edit_file/lsï¼‰ã€‚
ä½ å¿…é ˆæ•´åˆ /evidence/ åº•ä¸‹æ‰€æœ‰æª”æ¡ˆï¼ˆdoc_*.md èˆ‡å¯é¸ web_*.mdï¼‰ã€‚

ä»»å‹™é¡å‹åˆ¤æ–·ï¼š
- å®Œæ•´å ±å‘Š/ç« ç¯€ â†’ REPORT
- å–®é¡Œå›ç­” â†’ QA
- æ•´ç†çŸ¥è­˜è„ˆçµ¡ â†’ KNOWLEDGE
- ä½¿ç”¨è€…è²¼è‰ç¨¿è¦æŸ¥æ ¸/é™¤éŒ¯ â†’ VERIFY_DRAFTï¼ˆæœ€å¤š {DA_MAX_CLAIMS} æ¢ä¸»å¼µï¼‰

å¼•ç”¨è¦å‰‡ï¼ˆåš´æ ¼ï¼‰ï¼š
- QAï¼šç´” bulletï¼ˆæ¯è¡Œ -ï¼‰ï¼Œæ¯å€‹ bullet å¥å°¾å¿…æœ‰å¼•ç”¨ [å ±å‘Šåç¨± pé ] æˆ– [WebSearch:* p-]
- REPORT/KNOWLEDGE/VERIFYï¼šMarkdownï¼›æ¯å€‹éæ¨™é¡Œæ®µè½è‡³å°‘ 1 å€‹å¼•ç”¨
- enable_web=falseï¼šä¸å¾—å‡ºç¾ WebSearch
- draft çµ•å°ä¸èƒ½å‡ºç¾ chunk_id

æŠŠçµæœå¯«åˆ° /draft.md
"""

    verifier_prompt = f"""
ä½ æ˜¯å¯©ç¨¿æŸ¥æ ¸å°ˆå®¶ï¼ˆç”¨ read_file/edit_file/grepï¼‰ã€‚
ä»»å‹™ï¼šæª¢æŸ¥ /draft.md æ˜¯å¦ç¬¦åˆå¼•ç”¨è¦†è“‹ï¼Œä¸¦åšã€æœ€å°‘æ”¹å‹•ã€ä¿®æ­£ã€‚

è¦å‰‡ï¼š
- QAï¼šæ¯å€‹ bullet å¥å°¾å¿…æœ‰ [.. p..]
- å…¶ä»–ï¼šæ¯å€‹éæ¨™é¡Œæ®µè½è‡³å°‘ 1 å€‹å¼•ç”¨ [.. p..]
- enable_web=falseï¼šä¸å¾—å‡ºç¾ WebSearch
- è‹¥ /draft.md å‡ºç¾ chunk_id ç—•è·¡ï¼ˆchunk_id= æˆ– _p*_c*ï¼‰ï¼Œå¿…é ˆç§»é™¤ã€‚

æœ€å¤šä¿®æ­£ {DA_MAX_REWRITE_ROUNDS} è¼ªï¼š
- æ¯è¼ªï¼šread /draft.md â†’ edit_file ä¿®æ­£ â†’ write /review.md è¨˜éŒ„
"""

    subagents = [
        {
            "name": "retriever",
            "description": "å¾ä¸Šå‚³æ–‡ä»¶å‘é‡åº«æ‰¾è­‰æ“šï¼Œå¯« /evidence/doc_*.mdï¼ˆä¸å« chunk_idï¼‰",
            "system_prompt": retriever_prompt,
            "tools": [tool_get_usage, tool_doc_list, tool_doc_search, tool_doc_get_chunk],  # BaseTool only
            "model": f"openai:{MODEL_MAIN}",
        },
        {
            "name": "writer",
            "description": "æ•´åˆ /evidence/ â†’ ç”¢ç”Ÿ /draft.md",
            "system_prompt": writer_prompt,
            "tools": [],
            "model": f"openai:{MODEL_MAIN}",
        },
        {
            "name": "verifier",
            "description": "æª¢æŸ¥å¼•ç”¨è¦†è“‹ä¸¦ä¿®ç¨¿ /draft.mdï¼Œå¯« /review.md",
            "system_prompt": verifier_prompt,
            "tools": [],
            "model": f"openai:{MODEL_MAIN}",
        },
    ]

    if enable_web:
        web_prompt = """
ä½ æ˜¯ç¶²è·¯æœå°‹å°ˆå®¶ï¼ˆåªå…è¨± web_search_summary/get_usageï¼›ä¸å…è¨± doc_*ï¼‰ã€‚
facet å­ä»»å‹™æ ¼å¼åŒ retrieverã€‚

ç¡¬è¦å‰‡ï¼š
- å¯«å…¥ /evidence/web_<facet_slug>.md
- æ¯æ®µè¦ä¿ç•™å¼•ç”¨æ¨™é ­ [WebSearch:... p-]
- ç¦æ­¢æé€ ä¾†æºï¼›è‹¥ Budget exceeded å°±åœæ­¢ä¸¦å¯«æ˜ç¼ºå£
"""
        subagents.insert(
            1,
            {
                "name": "web-researcher",
                "description": "ç”¨ OpenAI å…§å»º web_search åšå°‘é‡é«˜å“è³ªæœå°‹ï¼Œå¯« /evidence/web_*.md",
                "system_prompt": web_prompt,
                "tools": [tool_web_search_summary, tool_get_usage] if tool_web_search_summary else [tool_get_usage],
                "model": f"openai:{MODEL_MAIN}",
            },
        )

    orchestrator_prompt = f"""
ä½ æ˜¯ Deep Doc Orchestratorï¼ˆæ–‡ä»¶å„ªå…ˆï¼›enable_web={str(enable_web).lower()}ï¼‰ã€‚

å›ºå®šæµç¨‹ï¼ˆå¿…åšï¼‰ï¼š
1) write_todosï¼šåˆ— 5~9 æ­¥ï¼ˆå«ï¼šæ‹† facetsã€å¹³è¡Œè’è­‰ã€å¯«ä½œã€å¯©ç¨¿ï¼‰
2) write_file /evidence/README.md è¨˜éŒ„æœ¬æ¬¡éœ€æ±‚èˆ‡ enable_web
3) æ‹† 2â€“4 å€‹ facetsï¼ˆé¢å‘ï¼Œä¸æ˜¯ç« ç¯€ï¼‰ï¼š
   - QA/KNOWLEDGEï¼šdefinitions, metrics, implications, limitations
   - REPORTï¼šscope, key_findings, risks, recommendations
   - VERIFYï¼šclaims_support, contradictions, missing_evidence, rewrite_suggestions
4) åŒä¸€è¼ªç”¨å¤šå€‹ task() å¹³è¡Œæ´¾å·¥ï¼š
   - æ¯å€‹ facet è‡³å°‘æ´¾ 1 å€‹ retriever
   - enable_web=true ä¸”éœ€è¦å¤–éƒ¨èƒŒæ™¯æ™‚ï¼Œå°åŒ facet å†æ´¾ 1 å€‹ web-researcher
5) å« writer ç”¢ç”Ÿ /draft.md
6) å« verifier ä¿®ç¨¿ï¼ˆæœ€å¤š {DA_MAX_REWRITE_ROUNDS} è¼ªï¼‰
7) read_file /draft.md ä½œç‚ºæœ€çµ‚å›ç­”

å¼•ç”¨èˆ‡éš±ç§è¦å‰‡ï¼š
- /evidence èˆ‡ /draft çµ•å°ä¸èƒ½å‡ºç¾ chunk_id
- å¼•ç”¨åªèƒ½ç”¨ [å ±å‘Šåç¨± pé ] æˆ– [WebSearch:* p-]
- chunk_id åªå…è¨±å­˜åœ¨æ–¼ doc_search çš„ JSON è£¡ï¼Œç”¨ä¾† doc_get_chunk ç²¾è®€
"""

    llm = _make_langchain_llm(model_name=f"openai:{MODEL_MAIN}", temperature=0.0)

    agent = create_deep_agent(
        model=llm,
        tools=tools,  # âœ… BaseTool instances only
        system_prompt=orchestrator_prompt,
        subagents=subagents,
        debug=False,
        name="deep-doc-agent",
    ).with_config({"recursion_limit": 90})

    st.session_state.deep_agent = agent
    st.session_state.deep_agent_web_flag = bool(enable_web)
    return agent


def deep_agent_run_with_live_status(agent, user_text: str) -> Tuple[str, Optional[dict]]:
    status_lines_added = set()
    last_files = set()
    final_state = None

    def emit(status_obj, key: str, line: str):
        if key in status_lines_added:
            return
        status_lines_added.add(key)
        status_obj.write(line)

    with st.status("DeepAgent åŸ·è¡Œä¸­â€¦ï¼ˆå¯å±•é–‹æŸ¥çœ‹é€²åº¦ï¼‰", expanded=True) as s:
        emit(s, "start", "ğŸš€ å•Ÿå‹• DeepAgentâ€¦")
        emit(s, "plan_hint", "ğŸ§­ è¦åŠƒä¸­ï¼ˆwrite_todosï¼‰â€¦")

        try:
            for state in agent.stream(
                {"messages": [{"role": "user", "content": user_text}]},
                stream_mode="values",
            ):
                final_state = state
                files = state.get("files") or {}
                file_keys = set(files.keys()) if isinstance(files, dict) else set()

                if any(k.startswith("/evidence/") for k in file_keys):
                    emit(s, "evidence", "ğŸ“š è’è­‰ä¸­ï¼ˆ/evidence/ ç”¢ç”Ÿä¸­ï¼›retriever/web-researcher å¯èƒ½åœ¨å¹³è¡Œè·‘ï¼‰â€¦")

                if "/draft.md" in file_keys:
                    emit(s, "draft", "âœï¸ å¯«ä½œå®Œæˆï¼ˆ/draft.md å·²ç”Ÿæˆï¼‰")

                if "/review.md" in file_keys:
                    emit(s, "review", "ğŸ§ª å¯©ç¨¿/è£œå¼•ç”¨ä¸­ï¼ˆ/review.md æ›´æ–°ä¸­ï¼‰")

                new_files = file_keys - last_files
                if new_files:
                    emit(
                        s,
                        f"new_{len(status_lines_added)}",
                        f"ğŸ—‚ï¸ æ–°å¢æª”æ¡ˆï¼š{', '.join(sorted(list(new_files))[:6])}" + ("â€¦" if len(new_files) > 6 else ""),
                    )
                    last_files = file_keys

        except Exception as e:
            # âœ… Budget exceededï¼šä¸è¦å† invoke() é‡è·‘ï¼ˆæœƒå†çˆ†ä¸€æ¬¡ï¼‰ï¼Œæ”¹æˆä¿ç•™ç›®å‰ final_state ä¸¦å›å‚³æç¤º
            msg = str(e)
            if "Budget exceeded" in msg:
                emit(s, "budget", f"âš ï¸ å·²é”å·¥å…·é ç®—ä¸Šé™ï¼š{msg}ï¼ˆåœæ­¢åŠ æœè­‰ï¼Œæ”¹ç”¨ç›®å‰å·²ç”¢å‡ºçš„å…§å®¹ï¼‰")
                # ä¸åš invoke()ï¼Œä¿ç•™ç›®å‰ final_stateï¼ˆå¯èƒ½å·²ç”¢ç”Ÿéƒ¨åˆ† /evidence/ï¼‰
            else:
                emit(s, "fallback", f"âš ï¸ ä¸²æµä¸å¯ç”¨ï¼Œæ”¹ç”¨ invoke()ï¼ˆ{e}ï¼‰")
                final_state = agent.invoke({"messages": [{"role": "user", "content": user_text}]})

        
        files = (final_state or {}).get("files") or {}

        def _file_to_str(file_obj):
            if isinstance(file_obj, dict) and "data" in file_obj:
                v = file_obj["data"]
                if isinstance(v, (bytes, bytearray)):
                    return v.decode("utf-8", errors="ignore")
                return str(v)
            if isinstance(file_obj, (bytes, bytearray)):
                return file_obj.decode("utf-8", errors="ignore")
            return str(file_obj)

        final_text = ""
        if isinstance(files, dict) and "/draft.md" in files:
            final_text = (_file_to_str(files["/draft.md"]) or "").strip()

        if not final_text:
            msgs = (final_state or {}).get("messages") or []
            if msgs:
                last = msgs[-1]
                final_text = getattr(last, "content", None) or str(last)

        if final_text and CHUNK_ID_LEAK_PAT.search(final_text):
            final_text = CHUNK_ID_LEAK_PAT.sub("", final_text)

        emit(s, "done", "âœ… DeepAgent å®Œæˆ")
        s.update(state="complete", expanded=False)

    return final_text or "ï¼ˆDeepAgent æ²’æœ‰ç”¢å‡ºå…§å®¹ï¼‰", files if isinstance(files, dict) and files else None


# =========================
# Session init
# =========================
OPENAI_API_KEY = get_openai_api_key()

# âœ… é—œéµï¼šlangchain_openai / deepagents æœƒä¾è³´é€™å€‹ç’°å¢ƒè®Šæ•¸
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

# ï¼ˆå¯é¸ï¼‰ä½ åŸæœ¬ä¹Ÿç”¨ OPENAI_KEYï¼Œé †æ‰‹ä¹Ÿè£œï¼Œé¿å…å…¶ä»–åœ°æ–¹æŠ“ä¸åˆ°
os.environ.setdefault("OPENAI_KEY", OPENAI_API_KEY)

client = get_client(OPENAI_API_KEY)

if "file_rows" not in st.session_state:
    st.session_state.file_rows = []
if "file_bytes" not in st.session_state:
    st.session_state.file_bytes = {}
if "store" not in st.session_state:
    st.session_state.store = None
if "processed_keys" not in st.session_state:
    st.session_state.processed_keys = set()
if "default_outputs" not in st.session_state:
    st.session_state.default_outputs = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "enable_web_search_agent" not in st.session_state:
    st.session_state.enable_web_search_agent = False


# =========================
# File table helpersï¼ˆpandas / data_editorï¼‰
# =========================
def file_rows_to_df(rows: list[FileRow]) -> pd.DataFrame:
    recs = []
    for r in rows:
        if r.ext == ".pdf":
            pages_str = "-" if r.pages is None else str(r.pages)
            text_pages_str = "-" if r.text_pages is None else str(r.text_pages)
            text_ratio_str = "-" if r.text_pages_ratio is None else f"{r.text_pages_ratio:.0%}"
        else:
            pages_str = "-" if r.pages is None else str(r.pages)
            text_pages_str = "-"
            text_ratio_str = "-"

        if r.ext == ".pdf" and r.likely_scanned:
            suggest = "å»ºè­° OCR"
        elif r.ext in (".png", ".jpg", ".jpeg"):
            suggest = "å¿… OCR"
        else:
            suggest = ""

        recs.append({
            "_file_id": r.file_id,
            "ä½¿ç”¨OCR": bool(r.use_ocr),
            "æª”å": truncate_filename(r.name, 52),
            "æ ¼å¼": r.ext.replace(".", ""),
            "é æ•¸": pages_str,
            "æ–‡å­—é ": text_pages_str,
            "æ–‡å­—%": text_ratio_str,
            "tokenä¼°ç®—": int(r.token_est),
            "å»ºè­°": suggest,
        })
    return pd.DataFrame(recs)


def sync_df_to_file_rows(df: pd.DataFrame, rows: list[FileRow]) -> None:
    id_to_row_idx = {r.file_id: i for i, r in enumerate(rows)}
    for _, rec in df.iterrows():
        fid = rec.get("_file_id")
        if fid not in id_to_row_idx:
            continue
        i = id_to_row_idx[fid]

        ext = rows[i].ext
        if ext in (".png", ".jpg", ".jpeg"):
            rows[i].use_ocr = True
        elif ext == ".txt":
            rows[i].use_ocr = False
        else:
            rows[i].use_ocr = bool(rec.get("ä½¿ç”¨OCR", rows[i].use_ocr))


# =========================
# Popoverï¼šæ–‡ä»¶ç®¡ç† + DeepAgent è¨­å®š
# =========================
with st.popover("ğŸ“¦ æ–‡ä»¶ç®¡ç†ï¼ˆä¸Šå‚³ / OCR / å»ºç´¢å¼• / DeepAgentè¨­å®šï¼‰", width="content"):
    st.caption("æ”¯æ´ PDF/TXT/PNG/JPGã€‚PDF è‹¥æ–‡å­—æŠ½å–åå°‘æœƒå»ºè­° OCRï¼ˆé€æª”å¯å‹¾é¸ï¼‰ã€‚")

    st.session_state.enable_web_search_agent = st.checkbox(
        "å•Ÿç”¨ç¶²è·¯æœå°‹ Agentï¼ˆå¯èˆ‡æ–‡ä»¶æª¢ç´¢å¹³è¡Œï¼›æœƒå¢åŠ æˆæœ¬ï¼‰",
        value=bool(st.session_state.enable_web_search_agent),
        help="é è¨­é—œï¼šå°ˆæ³¨åªç”¨ä¸Šå‚³æ–‡ä»¶ã€‚é–‹å•Ÿå¾Œï¼ŒDeepAgent æœƒå¤šä¸€å€‹ web-researcher å­ä»£ç†ï¼Œå¿…è¦æ™‚åŒæ™‚æŸ¥å¤–éƒ¨èƒŒæ™¯ã€‚",
    )

    uploaded = st.file_uploader(
        "ä¸Šå‚³æ–‡ä»¶",
        type=["pdf", "txt", "png", "jpg", "jpeg"],
        accept_multiple_files=True,
    )

    if uploaded:
        existing = {(r.name, r.bytes_len) for r in st.session_state.file_rows}
        for f in uploaded:
            data = f.read()
            if (f.name, len(data)) in existing:
                continue

            ext = os.path.splitext(f.name)[1].lower()
            fid = str(uuid.uuid4())[:10]
            sig = sha1_bytes(data)
            st.session_state.file_bytes[fid] = data

            pages = None
            extracted_chars = 0
            blank_pages = None
            blank_ratio = None
            text_pages = None
            text_pages_ratio = None

            if ext == ".pdf":
                pdf_pages = extract_pdf_text_pages(data)
                pages = len(pdf_pages)
                extracted_chars, blank_pages, blank_ratio, text_pages, text_pages_ratio = analyze_pdf_text_quality(pdf_pages)
            elif ext == ".txt":
                extracted_chars = len(norm_space(data.decode("utf-8", errors="ignore")))

            token_est = estimate_tokens_from_chars(extracted_chars)
            likely_scanned = should_suggest_ocr(ext, pages, extracted_chars, blank_ratio)

            if ext in (".png", ".jpg", ".jpeg"):
                use_ocr = True
            elif ext == ".txt":
                use_ocr = False
            else:
                use_ocr = bool(likely_scanned)

            st.session_state.file_rows.append(
                FileRow(
                    file_id=fid,
                    file_sig=sig,
                    name=f.name,
                    ext=ext,
                    bytes_len=len(data),
                    pages=pages,
                    extracted_chars=extracted_chars,
                    token_est=token_est,
                    text_pages=text_pages,
                    text_pages_ratio=text_pages_ratio,
                    blank_pages=blank_pages,
                    blank_ratio=blank_ratio,
                    likely_scanned=likely_scanned,
                    use_ocr=use_ocr,
                )
            )

    st.markdown("### æ–‡ä»¶æ¸…å–®ï¼ˆå¯é€æª”å‹¾é¸ OCRï¼‰")

    if not st.session_state.file_rows:
        st.info("å°šæœªä¸Šå‚³æ–‡ä»¶ã€‚")
    else:
        df = file_rows_to_df(st.session_state.file_rows)

        edited = st.data_editor(
            df.drop(columns=["_file_id"]),
            key="file_table_editor",
            width="stretch",
            hide_index=True,
            disabled=["æª”å", "æ ¼å¼", "é æ•¸", "æ–‡å­—é ", "æ–‡å­—%", "tokenä¼°ç®—", "å»ºè­°"],
            column_config={
                "ä½¿ç”¨OCR": st.column_config.CheckboxColumn("ä½¿ç”¨OCR", help="é€æª”é¸æ“‡æ˜¯å¦å•Ÿç”¨ OCRï¼ˆPDF å¯é¸ï¼›åœ–æª”å›ºå®šOCRï¼›TXTå›ºå®šä¸OCRï¼‰"),
            },
        )

        df_for_sync = df.copy()
        df_for_sync["ä½¿ç”¨OCR"] = edited["ä½¿ç”¨OCR"].values
        sync_df_to_file_rows(df_for_sync, st.session_state.file_rows)

        st.divider()
        col1, col2 = st.columns([1, 1])
        build_btn = col1.button("ğŸš€ å»ºç«‹ç´¢å¼• + é è¨­è¼¸å‡º", type="primary", width="stretch")
        clear_btn = col2.button("ğŸ§¹ æ¸…ç©ºå…¨éƒ¨", width="stretch")

        if clear_btn:
            st.session_state.file_rows = []
            st.session_state.file_bytes = {}
            st.session_state.store = None
            st.session_state.processed_keys = set()
            st.session_state.default_outputs = None
            st.session_state.chat_history = []
            st.session_state.deep_agent = None
            st.session_state.deep_agent_web_flag = None
            st.rerun()

        if build_btn:
            need_ocr = any(r.ext == ".pdf" and r.use_ocr for r in st.session_state.file_rows)
            if need_ocr and not HAS_PYMUPDF:
                st.error("ä½ æœ‰å‹¾é¸ PDF OCRï¼Œä½†ç’°å¢ƒæœªå®‰è£ pymupdfã€‚è«‹å…ˆ pip install pymupdfã€‚")
                st.stop()

            with st.status("å»ºç´¢å¼•ä¸­ï¼ˆOCR + embeddingsï¼‰...", expanded=True) as s:
                t0 = time.perf_counter()
                store, stats, processed_keys = build_indices_incremental_no_kg(
                    client,
                    st.session_state.file_rows,
                    st.session_state.file_bytes,
                    st.session_state.store,
                    st.session_state.processed_keys,
                )
                st.session_state.store = store
                st.session_state.processed_keys = processed_keys
                s.write(f"æ–°å¢å ±å‘Šæ•¸ï¼š{stats['new_reports']}")
                s.write(f"æ–°å¢ chunksï¼š{stats['new_chunks']}")
                s.write(f"è€—æ™‚ï¼š{time.perf_counter() - t0:.2f}s")
                s.update(state="complete")

            with st.status("ç”¢ç”Ÿé è¨­è¼¸å‡ºï¼ˆæ‘˜è¦/ä¸»å¼µ/æ¨è«–éˆï¼‰...", expanded=True) as s2:
                chosen = pick_corpus_chunks_for_default(st.session_state.store.chunks)
                ctx = render_chunks_for_model(chosen)
                bundle = generate_default_outputs_bundle(client, "æ•´é«”èåˆï¼ˆå…¨éƒ¨ä¸Šå‚³å ±å‘Šï¼‰", ctx, max_retries=2)
                st.session_state.default_outputs = bundle
                s2.update(state="complete")

            st.session_state.chat_history = []
            st.session_state.chat_history.append({
                "role": "assistant",
                "kind": "default",
                "title": "æ•´é«”èåˆï¼ˆå…¨éƒ¨ä¸Šå‚³å ±å‘Šï¼‰",
                **st.session_state.default_outputs,
            })
            st.session_state.deep_agent = None
            st.session_state.deep_agent_web_flag = None
            st.rerun()


# =========================
# ä¸»ç•«é¢ï¼šç‹€æ…‹ + Chat
# =========================
if st.session_state.store is None:
    st.info("å°šæœªå»ºç«‹ç´¢å¼•ã€‚è«‹å…ˆåœ¨ popover ä¸Šå‚³ä¸¦å»ºç«‹ç´¢å¼•ã€‚")
    st.stop()

st.success(f"å·²å»ºç«‹ç´¢å¼•ï¼šæª”æ¡ˆæ•¸={len(st.session_state.file_rows)} / chunks={len(st.session_state.store.chunks)}")
st.caption("å¼•ç”¨ badge åªé¡¯ç¤ºã€å ±å‘Šåç¨± + é ç¢¼ã€ï¼›chunk_id åªåœ¨ç³»çµ±å…§éƒ¨ç”¨ä¾†ç²¾è®€èˆ‡æ ¡å°ã€‚")

st.divider()
st.subheader("Chatï¼ˆDeepAgent + Badgesï¼‰")

# é¡¯ç¤º chat history
for msg in st.session_state.chat_history:
    with st.chat_message(msg.get("role", "assistant")):
        if msg.get("kind") == "default":
            st.markdown(f"## é è¨­è¼¸å‡ºï¼š{msg.get('title','')}")
            st.markdown("### 1) å ±å‘Šæ‘˜è¦")
            render_bullets_inline_badges(msg.get("summary", ""), badge_color="green")
            st.markdown("### 2) æ ¸å¿ƒä¸»å¼µ")
            render_bullets_inline_badges(msg.get("claims", ""), badge_color="violet")
            st.markdown("### 3) æ¨è«–éˆ")
            render_bullets_inline_badges(msg.get("chain", ""), badge_color="orange")
        else:
            # âœ… QAï¼šç”¨ä¸€èˆ¬ Markdownï¼ˆä¸é¡¯ç¤º JSONï¼‰ï¼Œä¾†æºç”¨è† å›Š badge
            content = msg.get("content", "")
            render_markdown_answer_with_source_badges(content, badge_color="green")

prompt = st.chat_input("è«‹è¼¸å…¥å•é¡Œï¼ˆä¹Ÿå¯è²¼è‰ç¨¿è¦æˆ‘æŸ¥æ ¸/é™¤éŒ¯ï¼‰ã€‚")
if prompt:
    st.session_state.chat_history.append({"role": "user", "kind": "text", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        agent = ensure_deep_agent(
            client=client,
            store=st.session_state.store,
            enable_web=bool(st.session_state.enable_web_search_agent),
        )
        answer_text, files = deep_agent_run_with_live_status(agent, prompt)

        # âœ… ä¸è¦é¡¯ç¤ºã€Œæœ€çµ‚å›ç­”ã€ï¼šç›´æ¥ä¸€èˆ¬ QA èŠå¤©è¼¸å‡º
        render_markdown_answer_with_source_badges(answer_text, badge_color="green")

        with st.expander("Debug", expanded=False):
            render_debug_panel(files)

    st.session_state.chat_history.append({"role": "assistant", "kind": "text", "content": answer_text})
