# app.py
# -*- coding: utf-8 -*-
"""
ç ”ç©¶å ±å‘ŠåŠ©æ‰‹ï¼ˆFAISS + OpenAI embeddings + LangExtract KG + Chat + Workflow UIï¼‰
é ˜åŸŸï¼šç¸½ç¶“ / é‡‘è / è²¡å‹™ / æ°£å€™é¢¨éšª / æ°¸çºŒé‡‘è

ä½ è¦æ±‚çš„é‡é»ï¼š
- UI ä¸ç”¨ tabs
- st.popover ä¸Šå‚³ + è¡¨æ ¼é¡¯ç¤ºï¼ˆé æ•¸/å­—æ•¸/token/ç©ºç™½é æ¯”ä¾‹/å»ºè­°OCR/ä½¿ç”¨OCRï¼‰
- OCRï¼šé€æª”å‹¾é¸ï¼ˆæƒæ PDF æœƒè‡ªå‹•å»ºè­°ï¼‰
- å‘é‡ï¼šFAISS + text-embedding-3-smallï¼ˆå›ºå®šï¼‰
- LLMï¼šgpt-5.2ï¼ˆå›ºå®šï¼‰
- LangExtractï¼šgpt-5.2ï¼ˆå›ºå®šï¼‰
- é è¨­è¼¸å‡ºï¼šæ‘˜è¦/æ ¸å¿ƒä¸»å¼µ/æ¨è«–éˆï¼ˆæ¯å€‹ bullet å¿…é ˆå¼•ç”¨ [å ±å‘Š pé  | chunk_id]ï¼‰
- Chatï¼šgradingï¼ˆyes/noï¼‰+ è‡ªå‹•é‡è©¦ï¼›UI é¡¯ç¤º RETRIEVE / GRADE / TRANSFORM / GENERATE + ä¸­é–“ç”¢ç‰©æ¼‚äº®å‘ˆç¾
- UI å¼·åŒ–ï¼šæ¯ä¸€æ­¥é¡¯ç¤º âœ…/âŒ + è€—æ™‚ï¼ˆç§’ï¼‰ï¼›relevant chunks æœ‰ expander çœ‹å…¨æ–‡

ç’°å¢ƒè®Šæ•¸ï¼š
- OPENAI_API_KEY å¿…å¡«

ä¾è³´ï¼š
streamlit, openai, langextract[openai], pypdf, numpy, faiss-cpu, networkx
OCR é¡å¤–ï¼špymupdf
"""

from __future__ import annotations

import os
import re
import io
import uuid
import math
import time
from dataclasses import dataclass, asdict
from typing import Any, Dict, List, Optional, Tuple

import streamlit as st
import numpy as np
import faiss
import networkx as nx
from pypdf import PdfReader

from openai import OpenAI
import langextract as lx

try:
    import fitz  # pymupdf
    HAS_PYMUPDF = True
except Exception:
    HAS_PYMUPDF = False


# =========================
# å›ºå®šæ¨¡å‹è¨­å®šï¼ˆä¾ä½ éœ€æ±‚ï¼šä¸è®“ä½¿ç”¨è€…è¼¸å…¥ï¼‰
# =========================
EMBEDDING_MODEL = "text-embedding-3-small"
LLM_MODEL = "gpt-5.2"
LX_MODEL = "gpt-5.2"
OCR_MODEL = "gpt-4.1-mini"  # è‹¥ä½ ç¢ºèª gpt-5.2 æ”¯æ´ visionï¼Œå¯æ”¹æˆ gpt-5.2


# =========================
# å°å·¥å…·ï¼šå­—æ•¸/token ä¼°ç®—ã€æ–‡å­—æ¸…ç†
# =========================
def norm_space(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def estimate_tokens_from_chars(n_chars: int) -> int:
    # ç²—ä¼°ï¼šæ¯ token ç´„ 3.6 charsï¼ˆä¸­è‹±æ··åˆæŠ˜è¡·ï¼‰
    if n_chars <= 0:
        return 0
    return max(1, int(math.ceil(n_chars / 3.6)))

def chunk_text(text: str, chunk_size: int = 900, overlap: int = 150) -> List[str]:
    text = norm_space(text)
    if not text:
        return []
    out = []
    i = 0
    while i < len(text):
        j = min(len(text), i + chunk_size)
        out.append(text[i:j])
        if j == len(text):
            break
        i = max(0, j - overlap)
    return out


# =========================
# OpenAI helpers
# =========================
def get_client() -> OpenAI:
    return OpenAI()

def embed_texts(client: OpenAI, texts: List[str]) -> np.ndarray:
    resp = client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=texts,
        encoding_format="float",
    )
    vecs = np.array([d.embedding for d in resp.data], dtype=np.float32)
    norms = np.linalg.norm(vecs, axis=1, keepdims=True)
    norms = np.where(norms == 0, 1.0, norms)
    return vecs / norms

def gen_text(client: OpenAI, system: str, user: str, model: str = LLM_MODEL) -> str:
    resp = client.responses.create(
        model=model,
        input=[
            {"role": "system", "content": system},
            {"role": "user", "content": user},
        ],
    )
    if getattr(resp, "output_text", None):
        return resp.output_text
    return str(resp)

def gen_yesno(client: OpenAI, system: str, user: str) -> str:
    out = gen_text(
        client,
        system=system + "\n\nåªå›è¦† 'yes' æˆ– 'no'ï¼ˆå°å¯«ï¼‰ï¼Œä¸è¦åŠ ä»»ä½•å…¶ä»–æ–‡å­—ã€‚",
        user=user,
        model=LLM_MODEL,
    ).strip().lower()
    if "yes" in out and "no" in out:
        return "yes" if out.find("yes") < out.find("no") else "no"
    if "yes" in out:
        return "yes"
    if "no" in out:
        return "no"
    return "no"


# =========================
# æª”æ¡ˆè®€å– / æƒæåµæ¸¬ / OCR
# =========================
@dataclass
class FileRow:
    file_id: str
    name: str
    ext: str
    bytes_len: int

    pages: Optional[int]
    extracted_chars: int
    token_est: int

    blank_pages: Optional[int]
    blank_ratio: Optional[float]

    likely_scanned: bool
    use_ocr: bool


def extract_pdf_text_pages(pdf_bytes: bytes) -> List[Tuple[int, str]]:
    reader = PdfReader(io.BytesIO(pdf_bytes))
    pages = []
    for i, p in enumerate(reader.pages):
        try:
            t = p.extract_text() or ""
        except Exception:
            t = ""
        pages.append((i + 1, norm_space(t)))
    return pages

def analyze_pdf_text_quality(pdf_pages: List[Tuple[int, str]], min_chars_per_page: int = 40) -> Tuple[int, int, float]:
    if not pdf_pages:
        return 0, 0, 1.0
    lens = [len(t) for _, t in pdf_pages]
    blank = sum(1 for L in lens if L <= min_chars_per_page)
    ratio = blank / max(1, len(lens))
    return sum(lens), blank, ratio

def should_suggest_ocr(ext: str, pages: Optional[int], extracted_chars: int, blank_ratio: Optional[float]) -> bool:
    if ext != ".pdf":
        return False
    if pages is None or pages <= 0:
        return True
    if blank_ratio is not None and blank_ratio >= 0.6:
        return True
    avg = extracted_chars / max(1, pages)
    if avg < 120:
        return True
    return False

def ocr_image_bytes_with_openai(client: OpenAI, image_bytes: bytes, model: str = OCR_MODEL) -> str:
    system = "ä½ æ˜¯ä¸€å€‹OCRå·¥å…·ã€‚åªè¼¸å‡ºå¯è¦‹æ–‡å­—èˆ‡è¡¨æ ¼å…§å®¹ï¼ˆè‹¥æœ‰è¡¨æ ¼ç”¨ Markdown è¡¨æ ¼ï¼‰ã€‚ä¸­æ–‡è«‹ç”¨ç¹é«”ä¸­æ–‡ã€‚ä¸è¦åŠ è©•è«–ã€‚"
    user_content = [
        {"type": "input_text", "text": "è«‹æ“·å–åœ–ç‰‡ä¸­æ‰€æœ‰å¯è¦‹æ–‡å­—ï¼ˆåŒ…å«å°å­—/è¨»è…³ï¼‰ã€‚è‹¥ç„¡æ³•è¾¨è­˜è«‹æ¨™è¨˜[ç„¡æ³•è¾¨è­˜]ã€‚"},
        {"type": "input_image", "image_bytes": image_bytes},
    ]
    resp = client.responses.create(
        model=model,
        input=[
            {"role": "system", "content": system},
            {"role": "user", "content": user_content},
        ],
    )
    return resp.output_text if getattr(resp, "output_text", None) else str(resp)

def ocr_pdf_pages_with_openai(client: OpenAI, pdf_bytes: bytes, dpi: int = 180) -> List[Tuple[int, str]]:
    if not HAS_PYMUPDF:
        raise RuntimeError("æœªå®‰è£ pymupdfï¼ˆfitzï¼‰ï¼Œç„¡æ³•åš PDF OCRã€‚è«‹ pip install pymupdf")

    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    out: List[Tuple[int, str]] = []
    zoom = dpi / 72.0
    mat = fitz.Matrix(zoom, zoom)

    for i in range(doc.page_count):
        page = doc.load_page(i)
        pix = page.get_pixmap(matrix=mat, alpha=False)
        img_bytes = pix.tobytes("png")
        text = norm_space(ocr_image_bytes_with_openai(client, img_bytes))
        out.append((i + 1, text))
    return out


# =========================
# FAISS store
# =========================
@dataclass
class Chunk:
    chunk_id: str
    report_id: str
    title: str
    page: Optional[int]
    text: str

class FaissStore:
    def __init__(self, dim: int):
        self.index = faiss.IndexFlatIP(dim)
        self.chunks: List[Chunk] = []

    def add(self, vecs: np.ndarray, chunks: List[Chunk]) -> None:
        self.index.add(vecs)
        self.chunks.extend(chunks)

    def search(self, qvec: np.ndarray, k: int = 6) -> List[Tuple[float, Chunk]]:
        if self.index.ntotal == 0:
            return []
        scores, idx = self.index.search(qvec.astype(np.float32), k)
        out = []
        for s, i in zip(scores[0], idx[0]):
            if i < 0 or i >= len(self.chunks):
                continue
            out.append((float(s), self.chunks[i]))
        return out


# =========================
# KGï¼ˆNetworkX MultiDiGraphï¼‰ï¼šä¿ç•™ provenance
# =========================
@dataclass
class Prov:
    report_id: str
    title: str
    page: Optional[int]
    char_start: Optional[int]
    char_end: Optional[int]
    snippet: str

ALLOWED_RELATIONS = {
    "CAUSES", "DRIVES", "AFFECTS", "INCREASES", "DECREASES", "CORRELATES_WITH",
    "ANNOUNCES", "TIGHTENS", "EASES",
    "ASSUMES_SCENARIO", "HAS_RISK", "HAS_METRIC", "TARGETS",
    "IS_A", "LOCATED_IN", "HAS_TIME", "HAS_SOURCE",
    "MENTIONS",
}

def norm_rel(r: str) -> str:
    r = norm_space(r).upper().replace(" ", "_")
    r = re.sub(r"[^A-Z0-9_]+", "", r)
    mapping = {
        "IMPACTS": "AFFECTS",
        "IMPACT": "AFFECTS",
        "INCREASE": "INCREASES",
        "DECREASE": "DECREASES",
        "CORRELATES": "CORRELATES_WITH",
        "CORRELATION": "CORRELATES_WITH",
        "SCENARIO": "ASSUMES_SCENARIO",
    }
    return mapping.get(r, r)

class KnowledgeGraph:
    def __init__(self):
        self.g = nx.MultiDiGraph()

    def add_edge(self, s: str, r: str, o: str, prov: Prov, attrs: Optional[Dict[str, Any]] = None):
        s = norm_space(s)
        o = norm_space(o)
        r = norm_rel(r)
        if not s or not o or not r:
            return
        if r not in ALLOWED_RELATIONS:
            return

        if s not in self.g:
            self.g.add_node(s, label=s)
        if o not in self.g:
            self.g.add_node(o, label=o)

        self.g.add_edge(
            s, o, key=str(uuid.uuid4()),
            relation=r,
            prov=asdict(prov),
            attrs=attrs or {},
        )

    def find_nodes_in_query(self, query: str, max_n: int = 2) -> List[str]:
        q = norm_space(query)
        hits = []
        for n in self.g.nodes():
            if len(n) >= 4 and n in q:
                hits.append(n)
        return hits[:max_n]

    def bfs_context(self, start: str, max_edges: int = 18) -> List[Dict[str, Any]]:
        if start not in self.g:
            return []
        out = []
        for u, v, k, data in nx.edge_bfs(self.g, start):
            out.append({"u": u, "v": v, "rel": data.get("relation"), "prov": data.get("prov")})
            if len(out) >= max_edges:
                break
        return out


# =========================
# LangExtractï¼šåªæŠ½ claim / relation
# =========================
def lx_prompt() -> str:
    return (
        "Extract structured information from macro/finance/climate-risk/sustainable-finance reports.\n"
        "Rules:\n"
        "1) Use exact text spans for extraction_text. Do NOT paraphrase.\n"
        "2) Extract only two classes: claim, relation.\n"
        "3) claim.attributes may include: theme, stance, confidence, time, implication.\n"
        "4) relation.attributes must include: {subject, relation, object}. Optional: {time, polarity, qualifier}.\n"
        "5) Only extract relations explicitly supported by text; if unsure, skip.\n"
    )

def lx_examples() -> List[lx.data.ExampleData]:
    t1 = (
        "We expect US CPI inflation to decelerate in 2025Q2 as energy prices fall. "
        "The Fed is likely to keep policy restrictive through mid-2025."
    )
    ex1 = lx.data.ExampleData(
        text=t1,
        extractions=[
            lx.data.Extraction(
                extraction_class="claim",
                extraction_text="US CPI inflation to decelerate in 2025Q2",
                attributes={"theme": "inflation_outlook", "confidence": "medium"},
            ),
            lx.data.Extraction(
                extraction_class="relation",
                extraction_text="as energy prices fall",
                attributes={"subject": "energy prices", "relation": "DECREASES", "object": "US CPI inflation", "time": "2025Q2"},
            ),
            lx.data.Extraction(
                extraction_class="relation",
                extraction_text="keep policy restrictive through mid-2025",
                attributes={"subject": "The Fed", "relation": "TIGHTENS", "object": "policy stance", "time": "mid-2025"},
            ),
        ],
    )
    t2 = (
        "Under the NGFS Net Zero 2050 scenario, transition risk increases for the energy sector, "
        "while physical risk remains elevated in coastal real estate."
    )
    ex2 = lx.data.ExampleData(
        text=t2,
        extractions=[
            lx.data.Extraction(
                extraction_class="relation",
                extraction_text="Under the NGFS Net Zero 2050 scenario",
                attributes={"subject": "Report", "relation": "ASSUMES_SCENARIO", "object": "NGFS Net Zero 2050"},
            ),
            lx.data.Extraction(
                extraction_class="relation",
                extraction_text="transition risk increases for the energy sector",
                attributes={"subject": "transition risk", "relation": "AFFECTS", "object": "energy sector", "polarity": "increase"},
            ),
            lx.data.Extraction(
                extraction_class="relation",
                extraction_text="physical risk remains elevated in coastal real estate",
                attributes={"subject": "physical risk", "relation": "AFFECTS", "object": "coastal real estate", "polarity": "high"},
            ),
        ],
    )
    return [ex1, ex2]

def run_langextract(text: str, api_key: str) -> lx.data.AnnotatedDocument:
    return lx.extract(
        text_or_documents=text,
        prompt_description=lx_prompt(),
        examples=lx_examples(),
        model_id=LX_MODEL,
        api_key=api_key,
        extraction_passes=2,
        max_char_buffer=1200,
        max_workers=8,
        fence_output=True,
        use_schema_constraints=False,
    )


# =========================
# å¼•ç”¨æª¢æŸ¥
# =========================
CIT_RE = re.compile(r"\[[^\]]+\|\s*[^\]]+\]")  # [å ±å‘Š pé  | chunk_id]
BULLET_RE = re.compile(r"^\s*(?:[-â€¢*]|\d+\.)\s+")

def bullets_all_have_citations(md: str) -> Tuple[bool, List[str]]:
    bad_lines = []
    lines = (md or "").splitlines()
    has_bullet = any(BULLET_RE.match(l) for l in lines)
    for line in lines:
        if BULLET_RE.match(line):
            if not CIT_RE.search(line):
                bad_lines.append(line)
    if not has_bullet:
        return False, ["ï¼ˆæ²’æœ‰ç”¢å‡ºä»»ä½• bullet æ¢åˆ—ï¼‰"]
    return (len(bad_lines) == 0), bad_lines

def paragraphs_all_have_citations(md: str) -> Tuple[bool, List[str]]:
    paras = [p.strip() for p in re.split(r"\n\s*\n", md or "") if p.strip()]
    bad = []
    if not paras:
        return False, ["ï¼ˆæ²’æœ‰è¼¸å‡ºä»»ä½•æ®µè½ï¼‰"]
    for p in paras:
        if not CIT_RE.search(p):
            bad.append(p[:120])
    return (len(bad) == 0), bad

def generate_with_bullet_citation_guard(client: OpenAI, user: str, max_retries: int = 2) -> str:
    system = (
        "ä½ æ˜¯åš´è¬¹çš„ç ”ç©¶åŠ©ç†ã€‚\n"
        "ç¡¬æ€§è¦å‰‡ï¼š\n"
        "1) åªèƒ½æ ¹æ“šæˆ‘æä¾›çš„è³‡æ–™å›ç­”ï¼Œä¸å¯è…¦è£œã€‚\n"
        "2) ä½ å¿…é ˆè¼¸å‡ºã€Œç´” bullet æ¢åˆ—ã€ï¼ˆæ¯è¡Œä»¥ - é–‹é ­ï¼‰ã€‚ä¸è¦è¼¸å‡ºæ®µè½ã€‚\n"
        "3) æ¯ä¸€å€‹ bullet çš„å¥å°¾éƒ½å¿…é ˆé™„å¼•ç”¨ï¼Œæ ¼å¼å›ºå®šï¼š[å ±å‘Š pé  | chunk_id]\n"
        "4) è‹¥ä½ ç„¡æ³•æ›¿æŸ bullet æ‰¾åˆ°å¼•ç”¨ï¼Œè«‹ä¸è¦å¯«é‚£å€‹ bulletï¼Œæ”¹å¯«æˆ–åˆªæ‰ã€‚\n"
    )

    last = ""
    for _ in range(max_retries + 1):
        out = gen_text(client, system, user, model=LLM_MODEL)
        ok, _bad = bullets_all_have_citations(out)
        if ok:
            return out
        last = out
        user = user + "\n\nã€å¼·åˆ¶ä¿®æ­£ã€‘è«‹é‡æ–°è¼¸å‡ºç´” bulletï¼Œä¸¦ä¿è­‰æ¯å€‹ bullet å¥å°¾éƒ½æœ‰ [å ±å‘Š pé  | chunk_id]ã€‚"
    return last

def generate_with_paragraph_citation_guard(client: OpenAI, user: str, max_retries: int = 2) -> str:
    system = (
        "ä½ æ˜¯åš´è¬¹çš„ç ”ç©¶åŠ©ç†ã€‚\n"
        "ç¡¬æ€§è¦å‰‡ï¼š\n"
        "1) åªèƒ½æ ¹æ“šæˆ‘æä¾›çš„ Context å›ç­”ï¼Œä¸å¯è…¦è£œã€‚\n"
        "2) è«‹ç”¨ 2~4 æ®µå›ç­”ã€‚\n"
        "3) æ¯ä¸€æ®µè‡³å°‘è¦æœ‰ 1 å€‹å¼•ç”¨ï¼Œæ ¼å¼å›ºå®šï¼š[å ±å‘Š pé  | chunk_id]\n"
        "4) è‹¥åšä¸åˆ°å¼•ç”¨ï¼Œè«‹åˆªæ‰é‚£æ®µä¸¦æ”¹å¯«ã€‚\n"
    )
    last = ""
    for _ in range(max_retries + 1):
        out = gen_text(client, system, user, model=LLM_MODEL)
        ok, _bad = paragraphs_all_have_citations(out)
        if ok:
            return out
        last = out
        user = user + "\n\nã€å¼·åˆ¶ä¿®æ­£ã€‘ä¸Šä¸€ç‰ˆæœ‰æ®µè½ç¼ºå¼•ç”¨ã€‚è«‹ç¢ºä¿æ¯æ®µè‡³å°‘ä¸€å€‹ [å ±å‘Š pé  | chunk_id]ã€‚"
    return last


# =========================
# å»ºç´¢å¼•ï¼šFAISS + KG
# =========================
def build_indices(
    client: OpenAI,
    api_key: str,
    file_rows: List[FileRow],
    file_bytes_map: Dict[str, bytes],
    chunk_size: int = 900,
    overlap: int = 150,
) -> Tuple[FaissStore, KnowledgeGraph, Dict[str, Any]]:
    dim = embed_texts(client, ["dim_probe"]).shape[1]
    store = FaissStore(dim)
    kg = KnowledgeGraph()

    stats = {"reports": 0, "chunks": 0, "kg_nodes": 0, "kg_edges": 0}

    all_chunks: List[Chunk] = []
    all_texts: List[str] = []

    for row in file_rows:
        data = file_bytes_map[row.file_id]
        report_id = row.file_id
        title = os.path.splitext(row.name)[0]
        stats["reports"] += 1

        pages: List[Tuple[Optional[int], str]] = []
        if row.ext == ".pdf":
            if row.use_ocr:
                pages = [(p, t) for p, t in ocr_pdf_pages_with_openai(client, data)]
            else:
                pages = [(p, t) for p, t in extract_pdf_text_pages(data)]
        elif row.ext == ".txt":
            pages = [(None, norm_space(data.decode("utf-8", errors="ignore")))]
        elif row.ext in (".png", ".jpg", ".jpeg"):
            text = norm_space(ocr_image_bytes_with_openai(client, data))
            pages = [(None, text)]
        else:
            pages = [(None, "")]

        # chunks
        for page_no, page_text in pages:
            if not page_text:
                continue
            for i, ch in enumerate(chunk_text(page_text, chunk_size=chunk_size, overlap=overlap)):
                cid = f"{report_id}_p{page_no if page_no else 'na'}_c{i}"
                all_chunks.append(
                    Chunk(
                        chunk_id=cid,
                        report_id=report_id,
                        title=title,
                        page=page_no if isinstance(page_no, int) else None,
                        text=ch,
                    )
                )
                all_texts.append(ch)

        # LangExtract -> KGï¼ˆpage ç´šï¼‰
        for page_no, page_text in pages:
            if not page_text:
                continue
            ann = run_langextract(page_text, api_key=api_key)

            for e in ann.extractions:
                cls = getattr(e, "extraction_class", "")
                etext = getattr(e, "extraction_text", "")
                attrs = getattr(e, "attributes", {}) or {}
                cstart = getattr(e, "char_start", None)
                cend = getattr(e, "char_end", None)

                snippet = page_text[:220]
                if cstart is not None and cend is not None and 0 <= cstart < len(page_text):
                    snippet = page_text[max(0, cstart - 80): min(len(page_text), cend + 80)]

                prov = Prov(
                    report_id=report_id,
                    title=title,
                    page=page_no if isinstance(page_no, int) else None,
                    char_start=cstart,
                    char_end=cend,
                    snippet=snippet,
                )

                if cls == "relation":
                    s = attrs.get("subject", "")
                    r = attrs.get("relation", "")
                    o = attrs.get("object", "")
                    kg.add_edge(s=s, r=r, o=o, prov=prov, attrs=attrs)
                elif cls == "claim":
                    claim_node = f"CLAIM: {norm_space(etext)}"
                    kg.add_edge(s=title, r="MENTIONS", o=claim_node, prov=prov, attrs=attrs)

    # embed chunks
    if all_texts:
        vecs_list = []
        bs = 64
        for i in range(0, len(all_texts), bs):
            vecs_list.append(embed_texts(client, all_texts[i:i+bs]))
        vecs = np.vstack(vecs_list)
        store.add(vecs, all_chunks)

    stats["chunks"] = len(store.chunks)
    stats["kg_nodes"] = kg.g.number_of_nodes()
    stats["kg_edges"] = kg.g.number_of_edges()
    return store, kg, stats


# =========================
# é è¨­è¼¸å‡ºï¼ˆæ‘˜è¦/æ ¸å¿ƒä¸»å¼µ/æ¨è«–éˆï¼‰â†’ æ¨é€åˆ° Chat
# =========================
def pick_chunks_for_report(all_chunks: List[Chunk], title: str, max_n: int = 12) -> List[Chunk]:
    kw = re.compile(r"(conclusion|outlook|risk|implication|forecast|scenario|inflation|rate|credit|spread|emission|transition|physical)", re.I)

    def score(c: Chunk) -> float:
        s = 0.0
        if c.page is not None:
            s += max(0.0, 8.0 - min(8.0, float(c.page)))
        if kw.search(c.text or ""):
            s += 6.0
        s += min(2.0, len(c.text) / 1200.0)
        return s

    cands = [c for c in all_chunks if c.title == title]
    cands = sorted(cands, key=score, reverse=True)
    return cands[:max_n]

def render_chunks_with_ids(chunks: List[Chunk], max_chars_each: int = 900) -> str:
    parts = []
    for c in chunks:
        head = f"[{c.title} p{c.page if c.page else '-'} | {c.chunk_id}]"
        parts.append(head + "\n" + c.text[:max_chars_each])
    return "\n\n".join(parts)

def make_default_outputs_for_report(client: OpenAI, all_chunks: List[Chunk], title: str) -> Dict[str, str]:
    reps = pick_chunks_for_report(all_chunks, title, max_n=12)
    ctx = render_chunks_with_ids(reps)

    summary_user = (
        f"è«‹é‡å°å ±å‘Šã€Š{title}ã€‹è¼¸å‡ºã€Œæ‘˜è¦ã€ï¼š\n"
        f"- è«‹è¼¸å‡º 8~14 å€‹ bullet\n"
        f"- æ¯å€‹ bullet éƒ½è¦åŒ…å«ä¸€å€‹å…·é«”è³‡è¨Šé»ï¼ˆçµè«–/é æ¸¬/å‡è¨­/é¢¨éšª/æƒ…å¢ƒ/é™åˆ¶/å¸‚å ´å«æ„ï¼‰\n"
        f"- æ¯å€‹ bullet å¥å°¾å¿…é ˆæœ‰å¼•ç”¨ [å ±å‘Š pé  | chunk_id]\n\n"
        f"è³‡æ–™ï¼š\n{ctx}"
    )
    claims_user = (
        f"è«‹é‡å°å ±å‘Šã€Š{title}ã€‹è¼¸å‡ºã€Œæ ¸å¿ƒä¸»å¼µã€ï¼š\n"
        f"- è«‹è¼¸å‡º 8~14 å€‹ bullet\n"
        f"- æ¯å€‹ bullet æ˜¯ä¸€æ¢å¯é©—è­‰ä¸»å¼µï¼ˆå«æ¢ä»¶/æƒ…å¢ƒ/æœŸé–“æ›´å¥½ï¼‰\n"
        f"- æ¯å€‹ bullet å¥å°¾å¿…é ˆæœ‰å¼•ç”¨ [å ±å‘Š pé  | chunk_id]\n\n"
        f"è³‡æ–™ï¼š\n{ctx}"
    )
    chain_user = (
        f"è«‹é‡å°å ±å‘Šã€Š{title}ã€‹è¼¸å‡ºã€Œæ¨è«–éˆ/å‚³å°æ©Ÿåˆ¶ã€ï¼š\n"
        f"- è«‹è¼¸å‡º 6~12 å€‹ bullet\n"
        f"- æ ¼å¼ç¤ºä¾‹ï¼šé©…å‹•å› å­ â†’ ä¸­ä»‹è®Šæ•¸ â†’ çµè«–/å¸‚å ´å«æ„ â†’ é¢¨éšª/ä¸ç¢ºå®šæ€§\n"
        f"- æ¯å€‹ bullet å¥å°¾å¿…é ˆæœ‰å¼•ç”¨ [å ±å‘Š pé  | chunk_id]\n\n"
        f"è³‡æ–™ï¼š\n{ctx}"
    )

    summary = generate_with_bullet_citation_guard(client, summary_user, max_retries=2)
    claims = generate_with_bullet_citation_guard(client, claims_user, max_retries=2)
    chain = generate_with_bullet_citation_guard(client, chain_user, max_retries=2)
    return {"summary": summary, "claims": claims, "chain": chain}

def push_default_outputs_to_chat(default_outputs: Dict[str, Dict[str, str]]):
    st.session_state.chat_history.append({
        "role": "assistant",
        "content": "æˆ‘å…ˆæŠŠä¸Šå‚³å ±å‘Šçš„ã€Œé è¨­è¼¸å‡ºã€æ•´ç†å¥½å›‰ï¼ˆæ‘˜è¦/æ ¸å¿ƒä¸»å¼µ/æ¨è«–éˆï¼›æ¯å€‹ bullet éƒ½æœ‰å¼•ç”¨ï¼‰ã€‚ä½ æ¥ä¸‹ä¾†å¯ä»¥ç›´æ¥åœ¨ä¸‹æ–¹å•å•é¡Œã€‚",
    })
    for title, out in default_outputs.items():
        md = (
            f"## é è¨­è¼¸å‡ºï¼š{title}\n\n"
            f"### 1) å ±å‘Šæ‘˜è¦\n{out['summary']}\n\n"
            f"### 2) æ ¸å¿ƒä¸»å¼µ\n{out['claims']}\n\n"
            f"### 3) æ¨è«–éˆ / å‚³å°æ©Ÿåˆ¶\n{out['chain']}\n"
        )
        st.session_state.chat_history.append({"role": "assistant", "content": md})


# =========================
# Chat Workflowï¼ˆUIï¼šâœ…/âŒ + è€—æ™‚ + ç”¢ç‰©æ¼‚äº®å‘ˆç¾ï¼‰
# =========================
def want_bullets(question: str) -> bool:
    return bool(re.search(r"(åˆ—å‡º|æœ‰å“ªäº›|æ‰€æœ‰|æ¸…å–®|å½™ç¸½|æ‘˜è¦|ç¸½çµ)", question))

def grade_doc_relevance(client: OpenAI, question: str, doc_text: str) -> str:
    system = (
        "ä½ æ˜¯è² è²¬è©•ä¼°æ–‡ä»¶ç‰‡æ®µèˆ‡ä½¿ç”¨è€…å•é¡Œæ˜¯å¦ç›¸é—œçš„è©•åˆ†è€…ã€‚"
        "è‹¥ç‰‡æ®µå«æœ‰å¯ç”¨ä¾†å›ç­”å•é¡Œçš„é—œéµäº‹å¯¦æˆ–æ¨è«–ç·šç´¢ï¼Œå› yesï¼›å¦å‰‡å› noã€‚"
        "ä¸éœ€è¦åš´æ ¼ï¼Œåªè¦æ’é™¤æ˜é¡¯ä¸ç›¸é—œã€‚"
    )
    user = f"Question:\n{question}\n\nDocument:\n{doc_text[:2200]}"
    return gen_yesno(client, system, user)

def rewrite_question(client: OpenAI, question: str) -> str:
    system = (
        "ä½ æ˜¯å°‡ä½¿ç”¨è€…å•é¡Œæ”¹å¯«æˆæ›´é©åˆå‘é‡æª¢ç´¢çš„æŸ¥è©¢èªå¥çš„å°ˆå®¶ã€‚"
        "è«‹ä¿ç•™åŸæ„ï¼Œè£œä¸Šå¯æª¢ç´¢çš„é—œéµå­—ï¼ˆä¾‹å¦‚ï¼šé€šè†¨ã€åˆ©ç‡ã€æ®–åˆ©ç‡æ›²ç·šã€ä¿¡ç”¨åˆ©å·®ã€NGFSã€transition riskã€physical riskã€WACIâ€¦ï¼‰ã€‚"
        "è¼¸å‡ºä¸€è¡Œæ”¹å¯«å¾Œçš„å•é¡Œå³å¯ã€‚"
    )
    return gen_text(client, system, question, model=LLM_MODEL).strip()

def grade_hallucination(client: OpenAI, context: str, answer: str) -> str:
    system = (
        "ä½ æ˜¯è² è²¬åˆ¤æ–·å›ç­”æ˜¯å¦æœ‰è¢« Context æ”¯æŒçš„è©•åˆ†è€…ã€‚"
        "è‹¥å›ç­”çš„é—œéµä¸»å¼µéƒ½èƒ½åœ¨ Context æ‰¾åˆ°æ”¯æŒï¼ˆåŒ…å«å¼•ç”¨ç‰‡æ®µï¼‰ï¼Œå› yesï¼›è‹¥æœ‰ç·¨é€ æˆ–è¶…å‡º Contextï¼Œå› noã€‚"
    )
    user = f"Context:\n{context[:9000]}\n\nAnswer:\n{answer[:4500]}"
    return gen_yesno(client, system, user)

def grade_answer(client: OpenAI, question: str, answer: str) -> str:
    system = (
        "ä½ æ˜¯è² è²¬åˆ¤æ–·å›ç­”æ˜¯å¦çœŸæ­£å›æ‡‰ä½¿ç”¨è€…å•é¡Œçš„è©•åˆ†è€…ã€‚"
        "è‹¥å›ç­”æœ‰ç›´æ¥å›è¦†å•é¡Œã€ä¸”çµæ§‹æ¸…æ¥šï¼Œå› yesï¼›å¦å‰‡å› noã€‚"
    )
    user = f"Question:\n{question}\n\nAnswer:\n{answer[:4500]}"
    return gen_yesno(client, system, user)

def build_retrieval_packages(
    client: OpenAI,
    store: FaissStore,
    kg: KnowledgeGraph,
    question: str,
    top_k: int = 10,
) -> Tuple[List[Dict[str, Any]], str]:
    qvec = embed_texts(client, [question])
    hits = store.search(qvec, k=top_k)
    retrieved = [{"chunk": ch, "score": score} for score, ch in hits]

    vec_parts = []
    for score, ch in hits:
        vec_parts.append(f"[{ch.title} p{ch.page if ch.page else '-'} | {ch.chunk_id} | score={score:.3f}]\n{ch.text}")

    kg_parts = []
    starts = kg.find_nodes_in_query(question, max_n=2)
    for s in starts:
        edges = kg.bfs_context(s, max_edges=18)
        for e in edges:
            prov = e.get("prov") or {}
            src = f"{prov.get('title','')} p{prov.get('page') if prov.get('page') else '-'}"
            kg_parts.append(
                f"- {e['u']} --[{e['rel']}]--> {e['v']} ã€”ä¾†æºï¼š{src}ã€•\n"
                f"  snippet: {str(prov.get('snippet',''))[:180]}"
            )

    parts = []
    if kg_parts:
        parts.append("ã€KG ç·šç´¢ã€‘\n" + "\n".join(kg_parts[:24]))
    if vec_parts:
        parts.append("ã€æª¢ç´¢ç‰‡æ®µã€‘\n" + "\n\n".join(vec_parts))

    context = "\n\n".join(parts) if parts else "ï¼ˆæ‰¾ä¸åˆ°ä»»ä½•ç›¸é—œå…§å®¹ï¼‰"
    return retrieved, context

def generate_answer_from_context(client: OpenAI, question: str, context: str) -> str:
    if want_bullets(question):
        user = f"Context:\n{context}\n\nQuestion:\n{question}"
        return generate_with_bullet_citation_guard(client, user, max_retries=1)
    user = (
        "è«‹å›ç­”å•é¡Œï¼Œä¸¦å‹™å¿…ä¾ç…§ï¼šçµè«– â†’ ä¾æ“šï¼ˆå¼•ç”¨ï¼‰â†’ æ¨è«–/è§£é‡‹ï¼ˆè‹¥ç‚ºæ„ç¾©/ç‚ºä½•/æ©Ÿåˆ¶ï¼‰ã€‚\n\n"
        f"Context:\n{context}\n\nQuestion:\n{question}"
    )
    return generate_with_paragraph_citation_guard(client, user, max_retries=1)

def _step_table(step_state: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:
    rows = []
    for name, info in step_state.items():
        rows.append({
            "Step": name,
            "Status": info.get("status", "PENDING"),
            "Seconds": info.get("seconds", None),
            "Note": info.get("note", ""),
        })
    return rows

def run_chat_workflow_with_ui(
    client: OpenAI,
    store: FaissStore,
    kg: KnowledgeGraph,
    question: str,
    *,
    max_query_rewrites: int = 2,
    max_generate_retries: int = 2,
    top_k: int = 10,
) -> Dict[str, Any]:
    """
    UI å…§é¡¯ç¤ºï¼š
    - Step Summaryï¼ˆâœ…/âŒ + ç§’ï¼‰
    - Query history
    - Retrieved / Graded table
    - Relevant chunks expanderï¼ˆå…¨æ–‡ï¼‰
    - Draft answer
    - Hallucination/Answer grade
    """
    query_history: List[str] = [question]
    logs: List[str] = []
    final_context = ""
    final_answer = ""

    # å³æ™‚ step summary
    step_state = {
        "RETRIEVE": {"status": "PENDING", "seconds": None, "note": ""},
        "GRADE": {"status": "PENDING", "seconds": None, "note": ""},
        "TRANSFORM": {"status": "PENDING", "seconds": None, "note": ""},
        "GENERATE": {"status": "PENDING", "seconds": None, "note": ""},
        "CHECK": {"status": "PENDING", "seconds": None, "note": ""},
    }

    step_summary_ph = st.empty()
    query_hist_ph = st.empty()

    def update_step_summary():
        step_summary_ph.markdown("#### Step Summaryï¼ˆâœ…/âŒ + è€—æ™‚ï¼‰")
        step_summary_ph.dataframe(_step_table(step_state), use_container_width=True, hide_index=True)

    def set_step(name: str, status: str, seconds: Optional[float] = None, note: str = ""):
        step_state[name]["status"] = status
        step_state[name]["seconds"] = (round(seconds, 3) if seconds is not None else None)
        step_state[name]["note"] = note
        update_step_summary()

    update_step_summary()
    query_hist_ph.markdown("#### Query history")
    query_hist_ph.code("\n".join([f"{i}. {q}" for i, q in enumerate(query_history)]))

    q = question

    for rewrite_round in range(max_query_rewrites + 1):
        # ---------- RETRIEVE ----------
        t0 = time.perf_counter()
        try:
            retrieved, raw_context = build_retrieval_packages(client, store, kg, q, top_k=top_k)
            set_step("RETRIEVE", "âœ… OK", time.perf_counter() - t0, note=f"top_k={top_k}, got={len(retrieved)}")
        except Exception as e:
            set_step("RETRIEVE", "âŒ FAIL", time.perf_counter() - t0, note=str(e))
            return {
                "final_answer": "æª¢ç´¢éšæ®µå¤±æ•—ï¼Œè«‹æŸ¥çœ‹ debugã€‚",
                "query_history": query_history,
                "context": "",
                "logs": logs + [f"[ERROR] RETRIEVE: {e}"],
            }

        # Pretty retrieved table
        st.markdown(f"### RETRIEVEï¼ˆround {rewrite_round}ï¼‰")
        retrieved_rows = []
        for it in retrieved:
            ch: Chunk = it["chunk"]
            retrieved_rows.append({
                "score": round(float(it["score"]), 4),
                "å ±å‘Š": ch.title,
                "é ": ch.page if ch.page is not None else "-",
                "chunk_id": ch.chunk_id,
                "preview": (ch.text[:140] + "â€¦") if len(ch.text) > 140 else ch.text,
            })
        st.dataframe(retrieved_rows, use_container_width=True, hide_index=True)

        # ---------- GRADE ----------
        t1 = time.perf_counter()
        relevant: List[Dict[str, Any]] = []
        graded_rows = []

        st.markdown("### GRADEï¼ˆdoc relevance yes/noï¼‰")
        prog = st.progress(0, text="gradingâ€¦")

        try:
            for i, it in enumerate(retrieved):
                ch: Chunk = it["chunk"]
                verdict = grade_doc_relevance(client, q, ch.text)

                graded_rows.append({
                    "grade": verdict,
                    "score": round(float(it["score"]), 4),
                    "å ±å‘Š": ch.title,
                    "é ": ch.page if ch.page is not None else "-",
                    "chunk_id": ch.chunk_id,
                    "preview": (ch.text[:140] + "â€¦") if len(ch.text) > 140 else ch.text,
                })
                if verdict == "yes":
                    relevant.append(it)

                prog.progress((i + 1) / max(1, len(retrieved)), text=f"gradingâ€¦ {i+1}/{len(retrieved)}")

            set_step("GRADE", "âœ… OK", time.perf_counter() - t1, note=f"relevant={len(relevant)}/{len(retrieved)}")
        except Exception as e:
            set_step("GRADE", "âŒ FAIL", time.perf_counter() - t1, note=str(e))
            return {
                "final_answer": "æ–‡ä»¶è©•åˆ†éšæ®µå¤±æ•—ï¼Œè«‹æŸ¥çœ‹ debugã€‚",
                "query_history": query_history,
                "context": raw_context,
                "logs": logs + [f"[ERROR] GRADE: {e}"],
            }

        st.dataframe(graded_rows, use_container_width=True, hide_index=True)

        # Relevant chunks expanderï¼ˆå…¨æ–‡ï¼‰
        st.markdown("### Relevant Chunksï¼ˆYESï¼‰")
        if not relevant:
            st.info("é€™ä¸€è¼ªæ²’æœ‰æ‰¾åˆ°ç›¸é—œ chunksï¼ˆå…¨éƒ¨è¢«åˆ¤å®š noï¼‰ã€‚")
        else:
            rel_sorted = sorted(relevant, key=lambda x: x["score"], reverse=True)[:top_k]
            rel_rows = []
            for it in rel_sorted:
                ch = it["chunk"]
                rel_rows.append({
                    "score": round(float(it["score"]), 4),
                    "å ±å‘Š": ch.title,
                    "é ": ch.page if ch.page is not None else "-",
                    "chunk_id": ch.chunk_id,
                    "preview": (ch.text[:180] + "â€¦") if len(ch.text) > 180 else ch.text,
                })
            st.dataframe(rel_rows, use_container_width=True, hide_index=True)

            st.markdown("#### å±•é–‹çœ‹å…¨æ–‡")
            for it in rel_sorted:
                ch = it["chunk"]
                with st.expander(f"{ch.title} p{ch.page if ch.page else '-'} | {ch.chunk_id} | score={it['score']:.3f}"):
                    st.text(ch.text)

        # è‹¥æ²’æœ‰ relevantï¼šTRANSFORM
        if not relevant:
            if rewrite_round < max_query_rewrites:
                t2 = time.perf_counter()
                st.markdown("### TRANSFORMï¼ˆrewrite queryï¼‰")
                new_q = rewrite_question(client, q)
                query_history.append(new_q)
                query_hist_ph.code("\n".join([f"{i}. {qq}" for i, qq in enumerate(query_history)]))
                set_step("TRANSFORM", "âœ… OK", time.perf_counter() - t2, note="rewrite applied")
                q = new_q
                continue
            else:
                set_step("TRANSFORM", "âŒ SKIP", None, note="rewrite limit reached")
                final_answer = "è³‡æ–™ä¸è¶³ï¼šæª¢ç´¢ä¸åˆ°è¶³å¤ ç›¸é—œå…§å®¹ã€‚ä½ å¯ä»¥æ›å€‹å•æ³•æˆ–ä¸Šå‚³æ›´å¤šå ±å‘Šã€‚"
                final_context = raw_context
                return {"final_answer": final_answer, "query_history": query_history, "context": final_context, "logs": logs}

        # build context from relevant
        rel_sorted = sorted(relevant, key=lambda x: x["score"], reverse=True)[:min(top_k, len(relevant))]
        vec_parts = []
        for it in rel_sorted:
            ch = it["chunk"]
            vec_parts.append(f"[{ch.title} p{ch.page if ch.page else '-'} | {ch.chunk_id} | score={it['score']:.3f}]\n{ch.text}")

        kg_part = ""
        if "ã€KG ç·šç´¢ã€‘" in raw_context:
            kg_part = raw_context.split("ã€æª¢ç´¢ç‰‡æ®µã€‘")[0].strip()

        context = "\n\n".join([p for p in [kg_part, "ã€æª¢ç´¢ç‰‡æ®µã€‘\n" + "\n\n".join(vec_parts)] if p.strip()])
        final_context = context

        # ---------- GENERATE + CHECK loop ----------
        for gen_round in range(max_generate_retries + 1):
            t3 = time.perf_counter()
            st.markdown(f"### GENERATEï¼ˆround {gen_round}ï¼‰")
            ans = generate_answer_from_context(client, q, context)
            set_step("GENERATE", "âœ… OK", time.perf_counter() - t3, note=f"gen_round={gen_round}")

            st.markdown("#### Draft answer")
            st.markdown(ans)

            t4 = time.perf_counter()
            st.markdown("### CHECKï¼ˆhallucination / answerï¼‰")
            hall = grade_hallucination(client, context, ans)
            good = grade_answer(client, q, ans)
            set_step("CHECK", "âœ… OK", time.perf_counter() - t4, note=f"hall={hall}, answer_ok={good}")
            logs.append(f"[CHECK] gen_round={gen_round} hall={hall} answer_ok={good}")

            st.write({"hallucination": hall, "answer_ok": good})

            if hall == "yes" and good == "yes":
                final_answer = ans
                return {"final_answer": final_answer, "query_history": query_history, "context": final_context, "logs": logs}

            # hallucination fail -> regenerate (same query)
            if hall == "no":
                continue

            # answer fail -> break to transform
            if good == "no":
                break

        # ç”Ÿæˆä¸ç†æƒ³ â†’ TRANSFORM
        if rewrite_round < max_query_rewrites:
            t2 = time.perf_counter()
            st.markdown("### TRANSFORMï¼ˆrewrite queryï¼‰")
            new_q = rewrite_question(client, q)
            query_history.append(new_q)
            query_hist_ph.code("\n".join([f"{i}. {qq}" for i, qq in enumerate(query_history)]))
            set_step("TRANSFORM", "âœ… OK", time.perf_counter() - t2, note="rewrite applied")
            q = new_q
            continue

        set_step("TRANSFORM", "âŒ SKIP", None, note="rewrite limit reached")
        final_answer = "è³‡æ–™ä¸è¶³ï¼šå·²å¤šæ¬¡å˜—è©¦ä»ç„¡æ³•ç”¢ç”Ÿå¯è¢«è­‰æ“šæ”¯æŒä¸”å›æ‡‰å•é¡Œçš„ç­”æ¡ˆã€‚å»ºè­°æ›å•æ³•æˆ–å¢åŠ è³‡æ–™ã€‚"
        return {"final_answer": final_answer, "query_history": query_history, "context": final_context, "logs": logs}

    final_answer = "è³‡æ–™ä¸è¶³ï¼šå·¥ä½œæµæœªèƒ½å®Œæˆã€‚"
    return {"final_answer": final_answer, "query_history": query_history, "context": final_context, "logs": logs}


# =========================
# Streamlit UIï¼ˆä¸ä½¿ç”¨ tabsï¼‰
# =========================
st.set_page_config(page_title="ç ”ç©¶å ±å‘ŠåŠ©æ‰‹ï¼ˆWorkflow UIï¼‰", layout="wide")
st.title("ç ”ç©¶å ±å‘ŠåŠ©æ‰‹ï¼ˆFAISS + LangExtract + Chat + Workflow UIï¼‰")

api_key = os.environ.get("OPENAI_API_KEY", "").strip()
if not api_key:
    st.error("è«‹å…ˆè¨­å®šç’°å¢ƒè®Šæ•¸ OPENAI_API_KEYã€‚")
    st.stop()

client = get_client()

# Session State
if "file_rows" not in st.session_state:
    st.session_state.file_rows: List[FileRow] = []
if "file_bytes" not in st.session_state:
    st.session_state.file_bytes: Dict[str, bytes] = {}

if "store" not in st.session_state:
    st.session_state.store: Optional[FaissStore] = None
if "kg" not in st.session_state:
    st.session_state.kg = KnowledgeGraph()
if "default_outputs" not in st.session_state:
    st.session_state.default_outputs: Dict[str, Dict[str, str]] = {}
if "chat_history" not in st.session_state:
    st.session_state.chat_history: List[Dict[str, str]] = []


# ===== ä¸Šå‚³ popover =====
with st.popover("ğŸ“¤ ä¸Šå‚³æ–‡ä»¶", use_container_width=True):
    st.caption("æ”¯æ´ PDF/TXT/PNG/JPGã€‚PDF è‹¥æŠ½åˆ°æ–‡å­—åå°‘æœƒè‡ªå‹•å»ºè­° OCRï¼ˆé€æª”å¯å‹¾é¸ï¼‰ã€‚")
    up = st.file_uploader(
        "é¸æ“‡æª”æ¡ˆ",
        type=["pdf", "txt", "png", "jpg", "jpeg"],
        accept_multiple_files=True,
    )
    if up:
        existing_keys = {(r.name, r.bytes_len) for r in st.session_state.file_rows}
        for f in up:
            data = f.read()
            key = (f.name, len(data))
            if key in existing_keys:
                continue

            ext = os.path.splitext(f.name)[1].lower()
            fid = str(uuid.uuid4())[:10]
            st.session_state.file_bytes[fid] = data

            pages = None
            extracted_chars = 0
            blank_pages = None
            blank_ratio = None

            if ext == ".pdf":
                pdf_pages = extract_pdf_text_pages(data)
                pages = len(pdf_pages)
                extracted_chars, blank_pages, blank_ratio = analyze_pdf_text_quality(pdf_pages)
            elif ext == ".txt":
                text = norm_space(data.decode("utf-8", errors="ignore"))
                extracted_chars = len(text)
            elif ext in (".png", ".jpg", ".jpeg"):
                extracted_chars = 0

            token_est = estimate_tokens_from_chars(extracted_chars)
            likely_scanned = should_suggest_ocr(ext, pages, extracted_chars, blank_ratio)
            use_ocr = True if ext in (".png", ".jpg", ".jpeg") else bool(likely_scanned)

            st.session_state.file_rows.append(
                FileRow(
                    file_id=fid,
                    name=f.name,
                    ext=ext,
                    bytes_len=len(data),
                    pages=pages,
                    extracted_chars=extracted_chars,
                    token_est=token_est,
                    blank_pages=blank_pages,
                    blank_ratio=blank_ratio,
                    likely_scanned=likely_scanned,
                    use_ocr=use_ocr,
                )
            )

# ===== æª”æ¡ˆè¡¨æ ¼ + OCR å‹¾é¸ =====
st.subheader("å·²ä¸Šå‚³æ–‡ä»¶")
if not st.session_state.file_rows:
    st.info("é‚„æ²’æœ‰ä¸Šå‚³æ–‡ä»¶ã€‚é»ã€ŒğŸ“¤ ä¸Šå‚³æ–‡ä»¶ã€é–‹å§‹ã€‚")
else:
    table_data = []
    for r in st.session_state.file_rows:
        note = ""
        if r.ext == ".pdf" and r.likely_scanned:
            note = "æ–‡å­—æŠ½å–åå°‘ï¼Œå¯èƒ½æ˜¯æƒæPDFï¼Œå»ºè­° OCR"
        elif r.ext in (".png", ".jpg", ".jpeg"):
            note = "åœ–ç‰‡æª”ï¼šä¸€å®šæœƒ OCR"
        elif r.ext == ".txt":
            note = "æ–‡å­—æª”ï¼šä¸éœ€è¦ OCR"

        table_data.append({
            "file_id": r.file_id,
            "æª”å": r.name,
            "æ ¼å¼": r.ext,
            "é æ•¸": r.pages if r.pages is not None else "-",
            "æŠ½åˆ°å­—æ•¸(å…¨æ–‡)": r.extracted_chars,
            "tokenä¼°ç®—(ç²—ä¼°)": r.token_est,
            "ç©ºç™½é /é æ•¸": f"{r.blank_pages}/{r.pages}" if r.blank_pages is not None and r.pages else "-",
            "ç©ºç™½é æ¯”ä¾‹": f"{r.blank_ratio:.2f}" if r.blank_ratio is not None else "-",
            "å»ºè­°OCR": r.likely_scanned,
            "ä½¿ç”¨OCR": r.use_ocr,
            "å‚™è¨»": note,
        })

    disabled_cols = ["file_id", "æª”å", "æ ¼å¼", "é æ•¸", "æŠ½åˆ°å­—æ•¸(å…¨æ–‡)", "tokenä¼°ç®—(ç²—ä¼°)", "ç©ºç™½é /é æ•¸", "ç©ºç™½é æ¯”ä¾‹", "å»ºè­°OCR", "å‚™è¨»"]
    edited = st.data_editor(
        table_data,
        use_container_width=True,
        hide_index=True,
        disabled=disabled_cols,
        column_config={
            "ä½¿ç”¨OCR": st.column_config.CheckboxColumn("ä½¿ç”¨OCR", help="PDF å­—æ•¸å¤ªå°‘æ™‚å»ºè­°å‹¾é¸ OCRï¼ˆæœƒæ›´æ…¢ä¸”èŠ±è²»è¼ƒé«˜ï¼‰"),
        },
    )

    use_ocr_map = {row["file_id"]: bool(row["ä½¿ç”¨OCR"]) for row in edited}
    for i, r in enumerate(st.session_state.file_rows):
        if r.ext in (".png", ".jpg", ".jpeg"):
            st.session_state.file_rows[i].use_ocr = True
        elif r.ext == ".txt":
            st.session_state.file_rows[i].use_ocr = False
        else:
            st.session_state.file_rows[i].use_ocr = use_ocr_map.get(r.file_id, r.use_ocr)

    c1, c2, c3 = st.columns([1, 1, 2])
    with c1:
        build_btn = st.button("ğŸš€ å»ºç«‹ç´¢å¼• + é è¨­è¼¸å‡º", type="primary", use_container_width=True)
    with c2:
        clear_btn = st.button("ğŸ§¹ æ¸…ç©º", use_container_width=True)
    with c3:
        st.caption("æœƒå»ºç«‹ï¼šFAISS + LangExtract KG + é è¨­è¼¸å‡ºï¼ˆæ‘˜è¦/æ ¸å¿ƒä¸»å¼µ/æ¨è«–éˆï¼‰â†’ æ¨é€åˆ° Chatã€‚")

    if clear_btn:
        st.session_state.file_rows = []
        st.session_state.file_bytes = {}
        st.session_state.store = None
        st.session_state.kg = KnowledgeGraph()
        st.session_state.default_outputs = {}
        st.session_state.chat_history = []
        st.rerun()

    if build_btn:
        need_ocr = any(r.ext == ".pdf" and r.use_ocr for r in st.session_state.file_rows)
        if need_ocr and not HAS_PYMUPDF:
            st.error("ä½ æœ‰å‹¾é¸ PDF OCRï¼Œä½†ç’°å¢ƒæœªå®‰è£ pymupdfã€‚è«‹å…ˆ pip install pymupdfï¼Œå†é‡è©¦ã€‚")
            st.stop()

        with st.status("å»ºç´¢å¼•ä¸­ï¼ˆå‘é‡ + KGï¼‰...", expanded=True) as s1:
            store, kg, stats = build_indices(
                client=client,
                api_key=api_key,
                file_rows=st.session_state.file_rows,
                file_bytes_map=st.session_state.file_bytes,
            )
            st.session_state.store = store
            st.session_state.kg = kg
            s1.update(label=f"å®Œæˆç´¢å¼•ï¼šchunks={stats['chunks']} / KG nodes={stats['kg_nodes']} edges={stats['kg_edges']}", state="complete")

        titles = sorted({c.title for c in st.session_state.store.chunks})
        with st.status("ç”¢ç”Ÿé è¨­è¼¸å‡ºï¼ˆæ‘˜è¦/æ ¸å¿ƒä¸»å¼µ/æ¨è«–éˆï¼›æ¯å€‹ bullet å¿…é ˆå¼•ç”¨ï¼‰...", expanded=True) as s2:
            default_outputs = {}
            for title in titles:
                default_outputs[title] = make_default_outputs_for_report(client, st.session_state.store.chunks, title)
            st.session_state.default_outputs = default_outputs
            s2.update(label="é è¨­è¼¸å‡ºå®Œæˆ", state="complete")

        st.session_state.chat_history = []
        push_default_outputs_to_chat(st.session_state.default_outputs)

st.divider()

# ===== Chat ä¸»ç•«é¢ï¼ˆå”¯ä¸€ï¼‰=====
st.subheader("Chatï¼ˆWorkflowï¼šRETRIEVE / GRADE / TRANSFORM / GENERATEï¼›å« âœ…/âŒ + è€—æ™‚ + å±•é–‹å…¨æ–‡ï¼‰")

for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if st.session_state.store is None:
    st.info("è«‹å…ˆä¸Šå‚³æ–‡ä»¶ä¸¦å»ºç«‹ç´¢å¼•ã€‚")
else:
    prompt = st.chat_input("è¼¸å…¥å•é¡Œï¼šç†è§£å«æ„/ç‚ºä½•é€™æ¨£é™³è¿°/å‚³å°æ©Ÿåˆ¶/é‡çµ„æ–°å ±å‘Š/åˆ—å‡ºæ‰€æœ‰â€¦")
    if prompt:
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.status("Workflow runningâ€¦", expanded=True) as status:
                result = run_chat_workflow_with_ui(
                    client=client,
                    store=st.session_state.store,
                    kg=st.session_state.kg,
                    question=prompt,
                    max_query_rewrites=2,
                    max_generate_retries=2,
                    top_k=10,
                )
                status.update(label="Workflow done", state="complete", expanded=False)

            st.markdown("## æœ€çµ‚å›ç­”")
            st.markdown(result["final_answer"])

            with st.expander("æŸ¥çœ‹ debugï¼ˆquery history / logs / contextï¼‰"):
                st.markdown("### Query history")
                st.code("\n".join([f"{i}. {q}" for i, q in enumerate(result.get("query_history", []))]))
                st.markdown("### Logs")
                st.text("\n".join(result.get("logs", [])))
                st.markdown("### Contextï¼ˆç¯€éŒ„ï¼‰")
                st.text((result.get("context", "") or "")[:12000])

        st.session_state.chat_history.append({"role": "assistant", "content": result["final_answer"]})
