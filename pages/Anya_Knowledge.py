# pages/78_Anya_Knowledge.py
# -*- coding: utf-8 -*-
"""
å®‰å¦®äºçŸ¥è­˜åº« â€” æ–‡ä»¶ä¸Šå‚³èˆ‡å‘é‡åŒ–å„²å­˜

åŠŸèƒ½ï¼š
  1. ä¸Šå‚³å¤šç¨®æ ¼å¼æ–‡ä»¶ï¼ˆPDFã€DOCXã€PPTXã€XLSXã€TXTã€PNGã€JPG ç­‰ï¼‰
  2. è‡ªå‹•èƒå–æ–‡å­—ï¼ˆPDF/åœ–ç‰‡å« OCRï¼‰ä¸¦åˆ‡å‰² chunk
  3. æ‰¹æ¬¡å‘é‡åŒ–å¾ŒæŒä¹…å„²å­˜è‡³ Supabase knowledge_chunks è¡¨
  4. æ”¯æ´å‘½åçŸ¥è­˜ç©ºé–“ï¼Œä¾›å…¶ä»–é é¢ï¼ˆå¦‚ 77_Anya_Test.pyï¼‰å–ç”¨

Supabase éœ€åœ¨ SQL Editor åŸ·è¡Œï¼š
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  create extension if not exists vector;

  create table if not exists knowledge_chunks (
    id          bigserial primary key,
    namespace   text not null,
    filename    text,
    chunk_index int,
    content     text not null,
    embedding   vector(1536),
    created_at  timestamptz default now()
  );

  create index if not exists knowledge_chunks_ns_idx
    on knowledge_chunks (namespace);

  create index if not exists knowledge_chunks_emb_idx
    on knowledge_chunks using ivfflat (embedding vector_cosine_ops)
    with (lists = 100);

  create or replace function match_knowledge_chunks(
    query_embedding  vector(1536),
    match_threshold  float,
    match_count      int,
    namespace_filter text
  )
  returns table (
    id bigint, namespace text, filename text,
    chunk_index int, content text, similarity float
  )
  language sql stable as $$
    select id, namespace, filename, chunk_index, content,
           1 - (embedding <=> query_embedding) as similarity
    from knowledge_chunks
    where namespace = namespace_filter
      and 1 - (embedding <=> query_embedding) > match_threshold
    order by embedding <=> query_embedding
    limit match_count;
  $$;
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""

# â”€â”€ æ¨™æº–å‡½å¼åº«ï¼ˆä¸æœƒå¤±æ•—ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import base64
import io
import os
import re
import tempfile
from datetime import datetime
from typing import Dict, List, Optional, Tuple

# â”€â”€ Streamlit ä¸€å®šè¦æ˜¯ç¬¬ä¸€å€‹è¢« import çš„å¤–éƒ¨å¥—ä»¶ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import streamlit as st

# â”€â”€â”€ é é¢è¨­å®šï¼ˆå¿…é ˆæ˜¯ç¬¬ä¸€å€‹ Streamlit å‘¼å«ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="å®‰å¦®äºçŸ¥è­˜åº«",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="collapsed",
)

# â”€â”€â”€ æ¨™é¡Œï¼ˆç«‹å³æ¸²æŸ“ï¼Œç¢ºä¿é é¢ä¸ç©ºç™½ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ“š å®‰å¦®äºçŸ¥è­˜åº«")
st.caption(
    "æ”¯æ´ PDF Â· DOCX Â· PPTX Â· XLSX Â· TXT Â· PNG Â· JPG â€” "
    "PDF / åœ–ç‰‡å« OCR Â· å‘é‡åŒ–å¾ŒæŒä¹…å„²å­˜ Â· ä¾› Anya å•ç­”å–ç”¨"
)

# â”€â”€â”€ ç¬¬ä¸‰æ–¹å¥—ä»¶ï¼ˆä»»ä½•ä¸€å€‹å¤±æ•—éƒ½é¡¯ç¤ºéŒ¯èª¤ï¼Œè€Œéç©ºç™½é ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    import pandas as pd
    from openai import OpenAI
    from pypdf import PdfReader
    from supabase import create_client, Client
    from langchain_openai import OpenAIEmbeddings
except ImportError as _import_err:
    st.error(
        f"**ç¼ºå°‘å¿…è¦å¥—ä»¶ï¼Œé é¢ç„¡æ³•è¼‰å…¥ã€‚**\n\n"
        f"éŒ¯èª¤ï¼š`{_import_err}`\n\n"
        "è«‹ç¢ºèª `requirements.txt` å·²å®‰è£ä¸¦é‡å•Ÿ Streamlitã€‚"
    )
    st.stop()

# â”€â”€ Optional deps â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HAS_PYMUPDF = False
_fitz = None
try:
    import fitz as _fitz_mod  # type: ignore
    _fitz = _fitz_mod
    HAS_PYMUPDF = True
except Exception:
    pass

HAS_SPLITTER = False
_RecursiveTextSplitter = None
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter as _R
    _RecursiveTextSplitter = _R
    HAS_SPLITTER = True
except Exception:
    pass

HAS_UNSTRUCTURED_LOADERS = False
_UWordLoader = _UPPTLoader = _UExcelLoader = None
try:
    from langchain_community.document_loaders.word_document import UnstructuredWordDocumentLoader as _UW
    from langchain_community.document_loaders.powerpoint import UnstructuredPowerPointLoader as _UP
    from langchain_community.document_loaders.excel import UnstructuredExcelLoader as _UE
    _UWordLoader, _UPPTLoader, _UExcelLoader = _UW, _UP, _UE
    HAS_UNSTRUCTURED_LOADERS = True
except Exception:
    pass

EMBED_BATCH_SIZE = 256
_CHUNK_SIZE = 900
_CHUNK_OVERLAP = 150

# â”€â”€â”€ API Keys & Clients â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY") or st.secrets.get("OPENAI_KEY")
    if not OPENAI_API_KEY:
        st.error("æ‰¾ä¸åˆ° OpenAI API Keyï¼Œè«‹åœ¨ .streamlit/secrets.toml è¨­å®š OPENAI_API_KEY æˆ– OPENAI_KEYã€‚")
        st.stop()
    os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]

    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
    embeddings_model = OpenAIEmbeddings(
        openai_api_key=OPENAI_API_KEY,
        model="text-embedding-3-small",
    )
except Exception as _init_err:
    st.error(f"åˆå§‹åŒ–å¤±æ•—ï¼š{_init_err}")
    st.stop()

SUPPORTED_TYPES = ["pdf", "docx", "doc", "pptx", "xlsx", "xls", "txt", "png", "jpg", "jpeg"]

# â”€â”€â”€ Session State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_defaults: dict = {
    "kg_namespace": "",
    "kg_processed_files": set(),      # file_key = "filename::namespace"
    "kg_file_namespaces": {},         # {filename: namespace}
    "kg_file_ocr": {},                # {filename: bool} PDF OCR åå¥½
    "kg_file_tags": {},               # {filename: tag} åˆ†é¡æ¨™ç±¤
}
for _k, _v in _defaults.items():
    if _k not in st.session_state:
        st.session_state[_k] = _v


def get_namespace() -> str:
    return st.session_state.kg_namespace.strip()


# â”€â”€ æ–‡å­—å·¥å…· â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def norm_space(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())


def detect_image_mime_by_ext(ext: str) -> str:
    ext = (ext or "").lower()
    if ext in (".jpg", ".jpeg"):
        return "image/jpeg"
    if ext == ".png":
        return "image/png"
    return "application/octet-stream"


def chunk_text(text: str) -> List[str]:
    text = norm_space(text)
    if not text:
        return []
    if HAS_SPLITTER and _RecursiveTextSplitter is not None:
        splitter = _RecursiveTextSplitter(
            chunk_size=_CHUNK_SIZE,
            chunk_overlap=_CHUNK_OVERLAP,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", "ï¼›", ";", "ï¼Œ", ",", " ", ""],
        )
        docs = splitter.create_documents([text])
        return [norm_space(d.page_content) for d in docs if norm_space(d.page_content)]
    out, i = [], 0
    while i < len(text):
        j = min(len(text), i + _CHUNK_SIZE)
        out.append(text[i:j])
        if j == len(text):
            break
        i = max(0, j - _CHUNK_OVERLAP)
    return out


def extract_pdf_text_pages(pdf_bytes: bytes) -> List[Tuple[int, str]]:
    reader = PdfReader(io.BytesIO(pdf_bytes))
    out: List[Tuple[int, str]] = []
    for i, page in enumerate(reader.pages):
        try:
            t = page.extract_text() or ""
        except Exception:
            t = ""
        out.append((i + 1, norm_space(t)))
    return out


def analyze_pdf_text_quality(pdf_pages: List[Tuple[int, str]]) -> Tuple[int, int, float, int, float]:
    if not pdf_pages:
        return 0, 0, 1.0, 0, 0.0
    lens = [len(t) for _, t in pdf_pages]
    blank = sum(1 for L in lens if L <= 40)
    total = max(1, len(lens))
    blank_ratio = blank / total
    text_pages = total - blank
    return sum(lens), blank, blank_ratio, text_pages, text_pages / total


def should_suggest_ocr_pdf(pages: int, extracted_chars: int, blank_ratio: float) -> bool:
    if pages <= 0:
        return True
    if blank_ratio >= 0.6:
        return True
    return (extracted_chars / max(1, pages)) < 120


def extract_office_text_blocks(filename: str, ext: str, data: bytes) -> List[Tuple[Optional[int], str]]:
    if not HAS_UNSTRUCTURED_LOADERS:
        return []
    with tempfile.NamedTemporaryFile(delete=False, suffix=ext) as tmp:
        tmp.write(data)
        tmp_path = tmp.name
    try:
        if ext in (".doc", ".docx"):
            loader = _UWordLoader(tmp_path, mode="single")
        elif ext == ".pptx":
            loader = _UPPTLoader(tmp_path, mode="single")
        elif ext in (".xls", ".xlsx"):
            loader = _UExcelLoader(tmp_path, mode="single")
        else:
            return []
        docs = loader.load()
        full = norm_space("\n\n".join(
            (d.page_content or "").strip() for d in (docs or []) if (d.page_content or "").strip()
        ))
        return [(1, full)] if full else []
    except Exception:
        return []
    finally:
        try:
            os.unlink(tmp_path)
        except Exception:
            pass


# â”€â”€â”€ OCR å‡½æ•¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

OCR_MODEL = "gpt-4o-mini"


def ocr_image_gpt4o(image_bytes: bytes, mime: str) -> str:
    b64 = base64.b64encode(image_bytes).decode()
    try:
        resp = openai_client.chat.completions.create(
            model=OCR_MODEL,
            messages=[{
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": (
                            "è«‹æ“·å–åœ–ç‰‡ä¸­æ‰€æœ‰å¯è¦‹æ–‡å­—ï¼ˆå«å°å­—ã€è¡¨æ ¼ã€è¨»è…³ï¼‰ã€‚"
                            "è¡¨æ ¼ç”¨ Markdown æ ¼å¼è¼¸å‡ºã€‚åªè¼¸å‡ºæ–‡å­—ï¼Œä¸è¦è©•è«–æˆ–è§£é‡‹ã€‚"
                        ),
                    },
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:{mime};base64,{b64}"},
                    },
                ],
            }],
            max_tokens=4096,
            temperature=0,
        )
        return norm_space(resp.choices[0].message.content or "")
    except Exception as e:
        st.toast(f"OCR å¤±æ•—ï¼š{e}", icon="âš ï¸")
        return ""


def ocr_pdf_gpt4o(pdf_bytes: bytes) -> List[Tuple[int, str]]:
    if not HAS_PYMUPDF or _fitz is None:
        st.warning("âš ï¸ æœªå®‰è£ pymupdfï¼Œç„¡æ³•å° PDF åš OCRã€‚è«‹ `pip install pymupdf`")
        return []
    doc = _fitz.open(stream=pdf_bytes, filetype="pdf")
    mat = _fitz.Matrix(180 / 72, 180 / 72)
    results: List[Tuple[int, str]] = []
    for i in range(doc.page_count):
        page = doc.load_page(i)
        pix = page.get_pixmap(matrix=mat, alpha=False)
        img_bytes = pix.tobytes("png")
        text = ocr_image_gpt4o(img_bytes, "image/png")
        results.append((i + 1, text))
    return results


# â”€â”€â”€ æ–‡å­—èƒå–åˆ†æ´¾ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_text_from_file(
    file_bytes: bytes,
    filename: str,
    use_ocr: bool = False,
) -> Tuple[List[Tuple[Optional[int], str]], bool]:
    ext = os.path.splitext(filename)[1].lower()
    ocr_used = False

    if ext == ".pdf":
        if use_ocr:
            pages = ocr_pdf_gpt4o(file_bytes)
            ocr_used = True
        else:
            raw = extract_pdf_text_pages(file_bytes)
            pages = [(pno, txt) for pno, txt in raw]
            total_chars, _, blank_ratio, _, _ = analyze_pdf_text_quality(raw)
            if should_suggest_ocr_pdf(len(raw), total_chars, blank_ratio):
                st.info(
                    f"ğŸ’¡ **{filename}** åµæ¸¬ç‚ºæƒæç‰ˆ PDFï¼ˆæ“·å–å­—å…ƒå°‘ï¼‰ï¼Œ"
                    "å»ºè­°å‹¾é¸å³ä¸Šè§’ã€Œå¼·åˆ¶ PDF OCRã€å¾Œé‡æ–°è™•ç†ã€‚"
                )

    elif ext in (".png", ".jpg", ".jpeg"):
        mime = detect_image_mime_by_ext(ext)
        text = ocr_image_gpt4o(file_bytes, mime)
        pages = [(None, text)]
        ocr_used = True

    elif ext in (".doc", ".docx", ".pptx", ".xls", ".xlsx"):
        if HAS_UNSTRUCTURED_LOADERS:
            blocks = extract_office_text_blocks(filename, ext, file_bytes)
            pages = [(blk[0], blk[1]) for blk in blocks]
            total_chars = sum(len(t) for _, t in pages)
            if total_chars < 100:
                st.warning(
                    f"âš ï¸ **{filename}** èƒå–æ–‡å­—éå°‘ï¼ˆ{total_chars} å­—å…ƒï¼‰ã€‚"
                    "è‹¥æ–‡ä»¶ä»¥åœ–ç‰‡ç‚ºä¸»ï¼Œå»ºè­°å°‡åœ–ç‰‡å¦å­˜ç‚º PNG/JPG å†ä¸Šå‚³é€é OCR æ“·å–ã€‚"
                )
        else:
            st.warning(
                f"âš ï¸ æœªå®‰è£ unstructuredï¼Œç„¡æ³•è™•ç† **{filename}**ã€‚"
                "è«‹ `pip install 'unstructured[all-docs]'`"
            )
            pages = []

    elif ext == ".txt":
        text = norm_space(file_bytes.decode("utf-8", errors="replace"))
        pages = [(None, text)]

    else:
        st.warning(f"âš ï¸ ä¸æ”¯æ´çš„æ ¼å¼ï¼š{ext}")
        pages = []

    return pages, ocr_used


# â”€â”€â”€ Supabase æ“ä½œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def batch_embed_and_save(
    chunks: List[str],
    filename: str,
    namespace: str,
    chunk_offset: int = 0,
    tag: str = "æœªåˆ†é¡",
) -> int:
    if not chunks:
        return 0
    saved = 0
    for i in range(0, len(chunks), EMBED_BATCH_SIZE):
        batch = chunks[i : i + EMBED_BATCH_SIZE]
        try:
            embs = embeddings_model.embed_documents(batch)
        except Exception as e:
            st.toast(f"Embedding å¤±æ•—ï¼ˆæ‰¹æ¬¡ {i}ï¼‰ï¼š{e}", icon="âš ï¸")
            continue
        rows = [
            {
                "namespace": namespace,
                "filename": filename,
                "chunk_index": chunk_offset + i + j,
                "content": text,
                "embedding": emb,
                "tag": tag,
                "created_at": datetime.now().isoformat(),
            }
            for j, (text, emb) in enumerate(zip(batch, embs))
        ]
        try:
            supabase.table("knowledge_chunks").insert(rows).execute()
            saved += len(rows)
        except Exception as e:
            st.toast(f"Supabase å„²å­˜å¤±æ•—ï¼š{e}", icon="âš ï¸")
    return saved


def load_namespace_summary(namespace: str) -> List[dict]:
    try:
        data = (
            supabase.table("knowledge_chunks")
            .select("filename")
            .eq("namespace", namespace)
            .execute()
            .data
        )
        counts: dict[str, int] = {}
        for row in data or []:
            fn = row.get("filename") or "unknown"
            counts[fn] = counts.get(fn, 0) + 1
        return [{"filename": fn, "chunks": cnt} for fn, cnt in sorted(counts.items())]
    except Exception:
        return []


def load_all_namespace_summary() -> "pd.DataFrame":
    """æ’ˆæ‰€æœ‰ namespace çš„å½™ç¸½ï¼ˆnamespaceã€tagã€æª”æ¡ˆæ•¸ã€chunk æ•¸ã€ä¸Šå‚³æ™‚é–“ï¼‰ã€‚
    åª select namespace + filename + tag + created_atï¼Œé¿å…å‚³è¼¸ embedding å¤§æ¬„ä½ã€‚
    åŠ  limit=10000 é¿å… Supabase å…è²»ç‰ˆå–®æ¬¡å›å‚³åˆ—æ•¸é™åˆ¶é€ æˆèª¤å·®ã€‚
    """
    empty = pd.DataFrame(columns=["namespace", "tag", "æª”æ¡ˆæ•¸", "chunk æ•¸", "ä¸Šå‚³æ™‚é–“"])
    try:
        data = (
            supabase.table("knowledge_chunks")
            .select("namespace, filename, tag, created_at")
            .limit(10000)
            .execute()
            .data
        )
    except Exception:
        return empty
    if not data:
        return empty
    df = pd.DataFrame(data)
    df["filename"] = df["filename"].fillna("unknown")
    df["tag"] = df["tag"].fillna("æœªåˆ†é¡")
    df["created_at"] = pd.to_datetime(df.get("created_at"), errors="coerce", utc=True)
    tag_per_ns = df.groupby("namespace")["tag"].first().reset_index()
    summary = (
        df.groupby("namespace", as_index=False)
        .agg(
            **{
                "æª”æ¡ˆæ•¸": ("filename", "nunique"),
                "chunk æ•¸": ("filename", "size"),
                "ä¸Šå‚³æ™‚é–“": ("created_at", "max"),
            }
        )
        .sort_values("namespace")
        .reset_index(drop=True)
    )
    summary = summary.merge(tag_per_ns, on="namespace", how="left")
    return summary[["namespace", "tag", "æª”æ¡ˆæ•¸", "chunk æ•¸", "ä¸Šå‚³æ™‚é–“"]]


def load_all_files_map() -> "Dict[str, List[dict]]":
    """ä¸€æ¬¡æ’ˆå‡ºæ‰€æœ‰ namespace çš„æª”æ¡ˆåˆ—è¡¨ï¼ˆé¿å… N+1 æŸ¥è©¢ï¼‰ã€‚
    å›å‚³ï¼š{namespace: [{filename, chunks}]}
    """
    try:
        data = (
            supabase.table("knowledge_chunks")
            .select("namespace, filename")
            .limit(10000)
            .execute()
            .data
        )
    except Exception:
        return {}
    if not data:
        return {}
    df = pd.DataFrame(data)
    df["filename"] = df["filename"].fillna("unknown")
    result: Dict[str, List[dict]] = {}
    for ns, grp in df.groupby("namespace"):
        counts = grp["filename"].value_counts()
        result[str(ns)] = [
            {"filename": str(fn), "chunks": int(cnt)}
            for fn, cnt in counts.items()
        ]
    return result


def delete_file_chunks(filename: str, namespace: str) -> None:
    try:
        supabase.table("knowledge_chunks").delete() \
            .eq("namespace", namespace) \
            .eq("filename", filename) \
            .execute()
    except Exception as e:
        st.toast(f"åˆªé™¤å¤±æ•—ï¼š{e}", icon="âš ï¸")


def delete_namespace_chunks(namespace: str) -> None:
    """åˆªé™¤æ•´å€‹çŸ¥è­˜ç©ºé–“çš„æ‰€æœ‰è³‡æ–™ã€‚"""
    try:
        supabase.table("knowledge_chunks").delete().eq("namespace", namespace).execute()
    except Exception as e:
        st.toast(f"åˆªé™¤å¤±æ•—ï¼š{e}", icon="âš ï¸")


def update_namespace_tag(namespace: str, tag: str) -> None:
    """æ›´æ–°æŸå€‹çŸ¥è­˜ç©ºé–“æ‰€æœ‰ chunk çš„ tagã€‚"""
    try:
        supabase.table("knowledge_chunks").update({"tag": tag}).eq("namespace", namespace).execute()
    except Exception as e:
        st.toast(f"æ›´æ–°æ¨™ç±¤å¤±æ•—ï¼š{e}", icon="âš ï¸")


# â”€â”€â”€ ä¸»è¦ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# å…ˆæ’ˆå½™ç¸½è³‡æ–™ï¼ˆä¸‰å€‹ tab éƒ½æœƒç”¨åˆ°ï¼‰
summary_df = load_all_namespace_summary()
available_ns = (
    list(summary_df["namespace"].unique())
    if not summary_df.empty and "namespace" in summary_df.columns
    else []
)

tab_upload, tab_search, tab_manage = st.tabs(["ğŸ“¤ ä¸Šå‚³", "ğŸ” æœå°‹æ¸¬è©¦", "ğŸ“š ç®¡ç†"])


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Tab 1ï¼šä¸Šå‚³
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with tab_upload:
    st.caption(":small[:gray[æ‹–æ›³æˆ–é»é¸ï¼Œå¯å¤šé¸ã€‚OCR å‹¾é¸åªå° PDF æœ‰æ•ˆï¼›Namespace å¯é€æª”ä¿®æ”¹ã€‚]]")
    uploaded = st.file_uploader(
        "ä¸Šå‚³æ–‡ä»¶",
        type=SUPPORTED_TYPES,
        accept_multiple_files=True,
        label_visibility="collapsed",
    )

    if uploaded:
        file_namespace_map: Dict[str, str] = st.session_state.kg_file_namespaces
        file_ocr_map: Dict[str, bool] = st.session_state.kg_file_ocr
        file_tag_map: Dict[str, str] = st.session_state.kg_file_tags

        # æ–°æª”æ¡ˆè¨­é è¨­å€¼
        for f in uploaded:
            stem = os.path.splitext(f.name)[0]
            ext = os.path.splitext(f.name)[1].lower()
            if f.name not in file_namespace_map:
                file_namespace_map[f.name] = stem
            if f.name not in file_ocr_map:
                file_ocr_map[f.name] = ext in (".png", ".jpg", ".jpeg")
            if f.name not in file_tag_map:
                file_tag_map[f.name] = "æœªåˆ†é¡"

        # å»º DataFrame
        rows_data = []
        for f in uploaded:
            ext = os.path.splitext(f.name)[1].lower()
            ns = file_namespace_map.get(f.name, os.path.splitext(f.name)[0])
            file_key = f"{f.name}::{ns}"
            rows_data.append({
                "OCR": file_ocr_map.get(f.name, False),
                "æª”å": f.name,
                "é¡å‹": ext.lstrip(".").upper(),
                "å¤§å°(KB)": round(f.size / 1024, 1),
                "Namespace": ns,
                "æ¨™ç±¤": file_tag_map.get(f.name, "æœªåˆ†é¡"),
                "ç‹€æ…‹": "âœ… å·²å­˜å…¥" if file_key in st.session_state.kg_processed_files else "â³ å¾…è™•ç†",
            })

        edited = st.data_editor(
            pd.DataFrame(rows_data),
            hide_index=True,
            use_container_width=True,
            key="kg_file_editor",
            column_config={
                "OCR": st.column_config.CheckboxColumn(
                    "OCR",
                    help="PDFï¼šå‹¾é¸å¾Œç”¨ GPT-4o Vision OCRï¼ˆé©åˆæƒæç‰ˆï¼Œè²»ç”¨è¼ƒé«˜ï¼‰\nåœ–ç‰‡ï¼šæ°¸é  OCR\nå…¶ä»–æ ¼å¼ï¼šä¸é©ç”¨",
                    width="small",
                ),
                "æª”å": st.column_config.TextColumn("æª”å", disabled=True, width="large"),
                "é¡å‹": st.column_config.TextColumn("é¡å‹", disabled=True, width="small"),
                "å¤§å°(KB)": st.column_config.NumberColumn("å¤§å°(KB)", disabled=True, format="%.1f", width="small"),
                "Namespace": st.column_config.TextColumn("Namespaceï¼ˆå¯ç·¨è¼¯ï¼‰", width="medium"),
                "æ¨™ç±¤": st.column_config.TextColumn("åˆ†é¡æ¨™ç±¤ï¼ˆå¯ç·¨è¼¯ï¼‰", width="medium",
                                                    help="ç”¨ä¾†åœ¨ç®¡ç†é å°‡çŸ¥è­˜åº«åˆ†çµ„ï¼Œä¾‹å¦‚ï¼šæˆ¿åœ°ç”¢ã€è²¡å ±ã€ç ”ç©¶å ±å‘Š"),
                "ç‹€æ…‹": st.column_config.TextColumn("ç‹€æ…‹", disabled=True, width="small"),
            },
            disabled=["æª”å", "é¡å‹", "å¤§å°(KB)", "ç‹€æ…‹"],
        )

        # å›å¯« session state
        for _, row in edited.iterrows():
            fname = row["æª”å"]
            ext = os.path.splitext(fname)[1].lower()
            new_ns = (str(row["Namespace"]) or "").strip() or os.path.splitext(fname)[0]
            file_namespace_map[fname] = new_ns
            file_tag_map[fname] = (str(row.get("æ¨™ç±¤", "")) or "").strip() or "æœªåˆ†é¡"
            if ext in (".png", ".jpg", ".jpeg"):
                file_ocr_map[fname] = True
            elif ext == ".pdf":
                file_ocr_map[fname] = bool(row["OCR"])
            else:
                file_ocr_map[fname] = False

        # æ•´ç†å¾…è™•ç† vs å·²è™•ç†
        file_entries = []
        for _, row in edited.iterrows():
            fname = row["æª”å"]
            ns_row = (str(row["Namespace"]) or "").strip() or os.path.splitext(fname)[0]
            tag_row = (str(row.get("æ¨™ç±¤", "")) or "").strip() or "æœªåˆ†é¡"
            use_ocr = file_ocr_map.get(fname, False)
            file_key = f"{fname}::{ns_row}"
            f_obj = next((f for f in uploaded if f.name == fname), None)
            if f_obj:
                file_entries.append((f_obj, ns_row, use_ocr, tag_row, file_key))

        new_entries = [(f, ns, use_ocr, tag, key) for f, ns, use_ocr, tag, key in file_entries
                       if key not in st.session_state.kg_processed_files]

        if new_entries:
            if st.button(f"ğŸš€ å»ºç«‹çŸ¥è­˜åº«ï¼ˆ{len(new_entries)} å€‹å¾…è™•ç†ï¼‰", type="primary"):
                total_saved = 0
                for f, ns, use_ocr, tag, key in new_entries:
                    file_bytes = f.getvalue()

                    with st.status(f"è™•ç† **{f.name}**ï¼ˆnamespaceï¼š{ns}ï¼Œæ¨™ç±¤ï¼š{tag}ï¼‰...", expanded=True) as status:
                        st.write("ğŸ” èƒå–æ–‡å­—ä¸­...")
                        pages, ocr_used = extract_text_from_file(file_bytes, f.name, use_ocr=use_ocr)
                        total_chars = sum(len(t) for _, t in pages if t)

                        if total_chars == 0:
                            status.update(label=f"âš ï¸ {f.name}ï¼šç„¡æ³•èƒå–æ–‡å­—", state="error")
                            continue

                        ocr_label = "ï¼ˆGPT-4o OCRï¼‰" if ocr_used else ""
                        st.write(f"âœ… {len(pages)} é  Â· {total_chars:,} å­—å…ƒ {ocr_label}")

                        # èƒå–æ–‡å­—é è¦½
                        preview_text = " ".join(t for _, t in pages if t)[:300]
                        with st.expander("ğŸ“„ æ–‡å­—é è¦½ï¼ˆå‰ 300 å­—ï¼‰"):
                            st.text(preview_text + ("â€¦" if len(preview_text) >= 300 else ""))

                        all_chunks: List[str] = []
                        for _, page_text in pages:
                            if page_text:
                                all_chunks.extend(chunk_text(page_text))
                        st.write(f"âœ‚ï¸ åˆ‡å‰²ç‚º {len(all_chunks)} å€‹æ®µè½")

                        st.write(f"ğŸ’¾ å‘é‡åŒ–ä¸¦å­˜å…¥çŸ¥è­˜ç©ºé–“ã€Œ{ns}ã€ï¼ˆæ¨™ç±¤ï¼š{tag}ï¼‰...")
                        saved = batch_embed_and_save(all_chunks, f.name, ns, tag=tag)
                        total_saved += saved
                        st.session_state.kg_processed_files.add(key)

                        status.update(
                            label=f"âœ… {f.name}ï¼š{saved} å€‹æ®µè½å·²å­˜å…¥ [{ns}]",
                            state="complete",
                        )

                st.toast(f"å®Œæˆï¼å…±å­˜å…¥ {total_saved} å€‹æ®µè½", icon="ğŸ‰")
                st.rerun()
        else:
            st.caption("âœ… æ‰€æœ‰å·²ä¸Šå‚³æª”æ¡ˆå‡å·²è™•ç†å®Œæˆã€‚")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Tab 2ï¼šæœå°‹æ¸¬è©¦
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with tab_search:
    st.caption(":small[:gray[è¼¸å…¥å•é¡Œï¼Œé©—è­‰çŸ¥è­˜åº«æœƒå›å‚³å“ªäº›æ®µè½â€”â€”ç¢ºèªçŸ¥è­˜å“è³ªå¾Œå†é€£æ¥å°è©±é é¢ã€‚]]")

    if not available_ns:
        st.info("å°šç„¡çŸ¥è­˜ç©ºé–“è³‡æ–™ï¼Œè«‹å…ˆåœ¨ã€ŒğŸ“¤ ä¸Šå‚³ã€é ä¸Šå‚³æ–‡ä»¶ã€‚", icon="ğŸ’¡")
    else:
        s_col1, s_col2 = st.columns([2, 1])
        with s_col1:
            search_ns = st.selectbox("çŸ¥è­˜ç©ºé–“", options=available_ns, key="search_ns")
        with s_col2:
            search_top_k = st.slider("å›å‚³æ•¸é‡", min_value=3, max_value=10, value=5, key="search_top_k")

        search_query = st.text_input(
            "è¼¸å…¥å•é¡Œæˆ–é—œéµå­—",
            placeholder="ä¾‹ï¼šä»€éº¼æ˜¯é‡åŒ–å¯¬é¬†æ”¿ç­–ï¼Ÿ",
            key="search_query",
        )
        search_threshold = st.slider(
            "ç›¸ä¼¼åº¦é–€æª»",
            min_value=0.30,
            max_value=0.95,
            value=0.50,
            step=0.05,
            help="æ•¸å€¼è¶Šé«˜ä»£è¡¨åªå›å‚³é«˜ç›¸ä¼¼åº¦çš„æ®µè½ï¼›è‹¥æ‰¾ä¸åˆ°çµæœï¼Œå¯å˜—è©¦é™ä½é–€æª»ã€‚",
            key="search_threshold",
        )

        if st.button("ğŸ” æœå°‹", type="primary", key="search_btn") and search_query.strip():
            with st.spinner("å‘é‡åŒ–æŸ¥è©¢ä¸¦æœå°‹..."):
                try:
                    qvec = embeddings_model.embed_query(search_query.strip())
                    result = supabase.rpc(
                        "match_knowledge_chunks",
                        {
                            "query_embedding": qvec,
                            "match_threshold": float(search_threshold),
                            "match_count": int(search_top_k),
                            "namespace_filter": search_ns,
                        },
                    ).execute()
                    hits = result.data or []
                except Exception as search_err:
                    st.error(f"æœå°‹å¤±æ•—ï¼š{search_err}")
                    hits = []

            if hits:
                st.success(f"æ‰¾åˆ° {len(hits)} å€‹ç›¸é—œæ®µè½", icon="âœ…")
                for i, hit in enumerate(hits, 1):
                    sim = hit.get("similarity", 0)
                    fname = hit.get("filename") or "æœªçŸ¥æª”æ¡ˆ"
                    chunk_idx = hit.get("chunk_index", "?")
                    content = hit.get("content") or ""
                    ext = os.path.splitext(fname)[1].lower()
                    icon = (
                        "ğŸ–¼ï¸" if ext in (".png", ".jpg", ".jpeg")
                        else "ğŸ“„" if ext == ".pdf"
                        else "ğŸ“"
                    )
                    label = f"#{i}  {icon} {fname}ï¼ˆæ®µè½ {chunk_idx}ï¼‰â€” ç›¸ä¼¼åº¦ {sim:.3f}"
                    with st.expander(label, expanded=(i == 1)):
                        st.markdown(content)
            else:
                st.warning(
                    f"åœ¨çŸ¥è­˜ç©ºé–“ã€Œ{search_ns}ã€ä¸­æ‰¾ä¸åˆ°ç›¸ä¼¼åº¦ â‰¥ {search_threshold:.2f} çš„æ®µè½ã€‚\n\n"
                    "å»ºè­°ï¼š\n- é™ä½ç›¸ä¼¼åº¦é–€æª»\n- æ›å€‹å•æ³•\n- ç¢ºèªæ–‡ä»¶å·²æ­£ç¢ºå­˜å…¥",
                    icon="ğŸ”",
                )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Tab 3ï¼šç®¡ç†ï¼ˆå¡ç‰‡å¼ + ä¾æ¨™ç±¤åˆ†çµ„ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with tab_manage:
    if summary_df.empty:
        st.info("ç›®å‰æ²’æœ‰ä»»ä½•çŸ¥è­˜ç©ºé–“è³‡æ–™ï¼Œè«‹å…ˆåœ¨ã€ŒğŸ“¤ ä¸Šå‚³ã€é ä¸Šå‚³æ–‡ä»¶ã€‚", icon="ğŸ’¡")
    else:
        # â”€â”€ é ‚éƒ¨ç¸½è¦½ metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        total_ns_count = len(summary_df)
        total_files_count = int(summary_df["æª”æ¡ˆæ•¸"].sum())
        total_chunks_count = int(summary_df["chunk æ•¸"].sum())
        all_tags = sorted(summary_df["tag"].fillna("æœªåˆ†é¡").unique().tolist())

        mc1, mc2, mc3, mc4 = st.columns(4)
        mc1.metric("ğŸ“š çŸ¥è­˜ç©ºé–“", total_ns_count)
        mc2.metric("ğŸ·ï¸ åˆ†é¡æ•¸", len(all_tags))
        mc3.metric("ğŸ“„ æ–‡ä»¶ç¸½æ•¸", total_files_count)
        mc4.metric("ğŸ§© æ®µè½ç¸½æ•¸", f"{total_chunks_count:,}")

        hc1, hc2, hc3 = st.columns([3, 2, 1])
        with hc1:
            # æ¨™ç±¤ç¯©é¸ï¼ˆpills é¢¨æ ¼ç”¨ radio horizontalï¼‰
            filter_opts = ["å…¨éƒ¨"] + all_tags
            selected_filter = st.radio(
                "ç¯©é¸åˆ†é¡",
                filter_opts,
                horizontal=True,
                label_visibility="collapsed",
                key="manage_tag_filter",
            )
        with hc2:
            sort_mode = st.radio(
                "æ’åº",
                ["æœ€æ–°å„ªå…ˆ", "åç¨± A-Z"],
                horizontal=True,
                label_visibility="collapsed",
                key="manage_sort_mode",
            )
        with hc3:
            if st.button("ğŸ”„ é‡æ–°æ•´ç†", use_container_width=True):
                st.rerun()

        st.divider()

        # â”€â”€ ä¸€æ¬¡æ’ˆå‡ºæ‰€æœ‰ namespace çš„æª”æ¡ˆåˆ—è¡¨ï¼ˆé¿å… N+1 æŸ¥è©¢ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        all_files_map = load_all_files_map()

        # â”€â”€ æ±ºå®šè¦é¡¯ç¤ºå“ªäº› tag groups â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if selected_filter == "å…¨éƒ¨":
            tags_to_show = all_tags
            filtered_df = summary_df
        else:
            tags_to_show = [selected_filter]
            filtered_df = summary_df[summary_df["tag"] == selected_filter]

        # â”€â”€ é€ tag åˆ†çµ„é¡¯ç¤ºæ‰‹é¢¨ç´åˆ—è¡¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for tag_group in tags_to_show:
            group_rows = filtered_df[filtered_df["tag"] == tag_group]
            if group_rows.empty:
                continue

            st.markdown(f"**ğŸ·ï¸ {tag_group}**ã€€:small[:gray[ï¼ˆ{len(group_rows)} å€‹çŸ¥è­˜åº«ï¼‰]]")
            st.divider()

            # çµ„å…§æ’åº
            if sort_mode == "æœ€æ–°å„ªå…ˆ":
                group_rows = group_rows.sort_values("ä¸Šå‚³æ™‚é–“", ascending=False)
            else:
                group_rows = group_rows.sort_values("namespace")

            for ns_info in group_rows.to_dict("records"):
                ns_name = ns_info["namespace"]
                ns_tag = str(ns_info.get("tag") or "æœªåˆ†é¡")
                ns_chunks = int(ns_info["chunk æ•¸"])
                ns_file_count = int(ns_info["æª”æ¡ˆæ•¸"])
                ns_file_list = all_files_map.get(ns_name, [])

                ts_raw = ns_info.get("ä¸Šå‚³æ™‚é–“")
                try:
                    ts_str = pd.Timestamp(ts_raw).tz_convert("Asia/Taipei").strftime("%Y-%m-%d %H:%M") if pd.notna(ts_raw) else "â€”"
                except Exception:
                    ts_str = "â€”"

                exp_label = (
                    f"ğŸ“– {ns_name}"
                    f"ã€€:small[:gray[{ns_file_count} å€‹æª”æ¡ˆ Â· {ns_chunks:,} æ®µè½ Â· {ts_str}]]"
                )
                with st.expander(exp_label, expanded=False):
                    # â”€â”€ ç·Šæ¹Šæª”æ¡ˆåˆ—è¡¨ â”€â”€
                    for frow in ns_file_list:
                        fext = os.path.splitext(frow["filename"])[1].lower()
                        ficon = (
                            "ğŸ–¼ï¸" if fext in (".png", ".jpg", ".jpeg")
                            else "ğŸ“„" if fext == ".pdf"
                            else "ğŸ“"
                        )
                        st.caption(f"{ficon} {frow['filename']}ã€€Â·ã€€{frow['chunks']} æ®µè½")

                    st.markdown("")

                    # â”€â”€ æ¨™ç±¤ç·¨è¼¯ + æ“ä½œæŒ‰éˆ• â”€â”€
                    col_inp, col_save, col_srch, col_del = st.columns([4, 2, 1, 1])
                    with col_inp:
                        new_tag_val = st.text_input(
                            "æ¨™ç±¤",
                            value=ns_tag,
                            key=f"tag_inp_{ns_name}",
                            placeholder="è¼¸å…¥åˆ†é¡æ¨™ç±¤...",
                            label_visibility="collapsed",
                        )
                    with col_save:
                        if st.button(
                            "ğŸ’¾ æ›´æ–°æ¨™ç±¤",
                            key=f"tag_save_{ns_name}",
                            use_container_width=True,
                        ):
                            t = new_tag_val.strip() or "æœªåˆ†é¡"
                            update_namespace_tag(ns_name, t)
                            st.toast(f"ã€Œ{ns_name}ã€æ¨™ç±¤å·²æ›´æ–°ç‚ºã€Œ{t}ã€", icon="ğŸ·ï¸")
                            st.rerun()
                    with col_srch:
                        if st.button(
                            "ğŸ”",
                            key=f"srch_ns_{ns_name}",
                            help="åˆ‡æ›åˆ°æœå°‹æ¸¬è©¦ä¸¦é é¸æ­¤çŸ¥è­˜ç©ºé–“",
                            use_container_width=True,
                        ):
                            st.session_state["search_ns"] = ns_name
                            st.toast(f"è«‹åˆ‡æ›åˆ°ã€Œæœå°‹æ¸¬è©¦ã€é ï¼Œå·²é é¸ã€Œ{ns_name}ã€", icon="ğŸ”")
                    with col_del:
                        if st.button(
                            "ğŸ—‘ï¸",
                            key=f"del_ns_{ns_name}",
                            help="åˆªé™¤æ­¤çŸ¥è­˜ç©ºé–“ï¼ˆä¸å¯é‚„åŸï¼‰",
                            use_container_width=True,
                            type="secondary",
                        ):
                            delete_namespace_chunks(ns_name)
                            st.session_state.kg_processed_files = {
                                key for key in st.session_state.kg_processed_files
                                if not key.endswith(f"::{ns_name}")
                            }
                            st.toast(f"å·²åˆªé™¤çŸ¥è­˜ç©ºé–“ã€Œ{ns_name}ã€", icon="ğŸ—‘ï¸")
                            st.rerun()

            st.markdown("")  # æ¯å€‹ tag group ä¹‹é–“ç•™ç©ºè¡Œ
