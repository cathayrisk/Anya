import streamlit as st
from PIL import Image
import base64
import io
from datetime import datetime
from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
import inspect
from typing import Callable, TypeVar
import time

# ==== Streamlit åŸºæœ¬è¨­å®šã€state ====
st.set_page_config(page_title="Anya", layout="wide", page_icon="ğŸ¥œ", initial_sidebar_state="collapsed")

if "messages" not in st.session_state:
    st.session_state.messages = [AIMessage(content="å—¨å—¨ï½å®‰å¦®äºä¾†äº†ï¼ğŸ‘‹ æœ‰ä»€éº¼æƒ³å•å®‰å¦®äºçš„å—ï¼Ÿ")]
if "selected_model" not in st.session_state:
    st.session_state.selected_model = "gpt-4.1"
if "current_model" not in st.session_state:
    st.session_state.current_model = None
if "llm" not in st.session_state:
    st.session_state.llm = None

# ==== OpenAI ç‰©ä»¶ ====
client = OpenAI(api_key=st.secrets["OPENAI_KEY"])

# ==== å‰è™•ç†å·¥å…·ï¼šçµ±ä¸€åœ–ç‰‡æ ¼å¼ & base64 ====
def process_upload_file(file):
    file.seek(0)
    img_bytes = file.read()
    if not img_bytes or len(img_bytes) < 32:
        return None
    try:
        img = Image.open(io.BytesIO(img_bytes))
        fmt = img.format.lower()
        mime = f"image/{fmt}"
        if fmt not in ["png","jpeg","jpg","webp","gif"]:
            return None
        b64 = base64.b64encode(img_bytes).decode()
        return {"bytes": img_bytes, "file_name": file.name, "fmt": fmt, "mime": mime, "b64": b64}
    except Exception:
        return None

# ==== OCRå·¥å…·ç¯„ä¾‹ï¼Œå¯è¤‡è£½ä¸€ä»½å†å¯«å…¶ä»–å¤šåœ–tool ====
@tool
def image_ocr_tool(image_bytes: bytes, file_name: str = "uploaded_file.png") -> str:
    """AI OCRåœ–ç‰‡è­˜åˆ¥å·¥å…·ï¼Œè¼¸å…¥åœ–ç‰‡bytesèˆ‡æª”åï¼Œå›å‚³åœ–ä¸­æ–‡å­—çµæœã€‚"""
    try:
        img = Image.open(io.BytesIO(image_bytes))
        fmt = img.format.lower()
        mime = f"image/{fmt}"
    except Exception as e:
        return f"è§£æåœ–ç‰‡å¤±æ•—ï¼š{e}"

    img_url = f"data:{mime};base64,{base64.b64encode(image_bytes).decode()}"
    try:
        response = client.responses.create(
            model="gpt-4.1-mini",
            input=[
                {"role": "system", "content": "You are an OCR-like data extraction tool that extracts text from images."},
                {"role": "user", "content": [
                    {"type": "input_text", "text":
                        "Please extract all visible text from the image, including any small print or footnotes. "
                        "Ensure no text is omitted, and provide a verbatim transcription of the document. "
                        "Format your answer in Markdown (no code block or triple backticks). "
                        "Do not add any explanations or commentary."
                    },
                    {"type": "input_image", "image_url": img_url, "detail": "high"}
                ]}
            ],
            timeout=45
        )
        return f"---\nfile_name: {file_name}\n---\n{response.output_text.strip()}\n"
    except Exception as e:
        return f"OCRå¤±æ•—ï¼Œè«‹æª¢æŸ¥API/åœ–ç‰‡æ ¼å¼ï¼š{e}"

# ==== ä½ å…¶ä»–çš„å·¥å…·ï¼Œä¾‹å¦‚ddgs_searchã€wiki_tooléƒ½å¯ä»¥é€™æ¨£è¨»å†Š ====
@tool
def echo_tool(text:str) -> str:
    return f"ä½ è¼¸å…¥çš„å…§å®¹æ˜¯ï¼š{text}"

tools = [image_ocr_tool, echo_tool] # <-- ä½ éœ€è¦çš„æ‰€æœ‰@tooléƒ½å¯ä»¥åŠ é€²ä¾†

# ==== LangGraph Agent è¨­å®š ====
ANYA_SYSTEM_PROMPT = """ä½ æ˜¯å®‰å¦®äºï¼Œè«‹æ ¹æ“šç”¨æˆ¶çµ¦çš„ç´”æ–‡å­—å’Œ/æˆ–åœ–ç‰‡æå‡ºé©ç•¶çš„å›ç­”ï¼Œå¯ä»¥è‡ªå‹•åˆ¤æ–·éœ€å‘¼å«OCRæˆ–é€²è¡Œå…¶ä»–å›ç­”ã€‚"""

llm = st.session_state.llm or ChatOpenAI(
    model=st.session_state.selected_model,
    openai_api_key=st.secrets["OPENAI_KEY"],
    temperature=0.0,
    streaming=True,
)
llm_with_tools = llm.bind_tools(tools)

def call_model(state: MessagesState):
    messages = state["messages"]
    sys_msg = SystemMessage(content=ANYA_SYSTEM_PROMPT)
    response = llm_with_tools.invoke([sys_msg] + messages)
    return {"messages": messages + [response]}

tool_node = ToolNode(tools)
def call_tools(state: MessagesState):
    messages = state["messages"]
    last_message = messages[-1]
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    return END

workflow = StateGraph(MessagesState)
workflow.add_node("LLM", call_model)
workflow.add_edge(START, "LLM")
workflow.add_node("tools", tool_node)
workflow.add_conditional_edges("LLM", call_tools)
workflow.add_edge("tools", "LLM")
agent = workflow.compile()

# ==== ç¾ç¾åœ°é¡¯ç¤ºæ­·å² ====
for msg in st.session_state.messages:
    if isinstance(msg, AIMessage):
        st.chat_message("assistant").write(msg.content)
    elif isinstance(msg, HumanMessage):
        # è™•ç†contentå‹æ…‹ï¼Œæœ‰å¤šåœ–çš„è©±ä¹Ÿä¸€æ¨£é †
        if isinstance(msg.content, str):
            st.chat_message("user").write(msg.content)
        elif isinstance(msg.content, list):
            with st.chat_message("user"):
                for block in msg.content:
                    if block.get("type") == "text":
                        st.write(block["text"])
                    elif block.get("type") == "image_url":
                        info = block["image_url"]
                        st.image(info["url"], caption=info.get("file_name", ""), width=220)

# ==== è¼¸å…¥å€ï¼šæ–‡å­—è¼¸å…¥ + æ”¯æ´å¤šåœ–è¼¸å…¥ ====
user_prompt = st.chat_input(
    "wakuwakuï¼å®‰å¦®äºå¯ä»¥å¹«ä½ çœ‹åœ–èªªæ•…äº‹åš•ï¼",
    accept_file="multiple",
    file_type=["jpg", "jpeg", "png"]
)

if user_prompt:
    # 1. çµ„ content_blocks
    content_blocks = []
    user_text = user_prompt.text.strip() if user_prompt.text else ""
    if user_text:
        content_blocks.append({"type": "text", "text": user_text})

    images_for_history = []
    if hasattr(user_prompt, "files"):
        for f in user_prompt.files:
            asset = process_upload_file(f)
            if asset:
                dataurl = f"data:{asset['mime']};base64,{asset['b64']}"
                content_blocks.append({"type": "image_url", "image_url": {
                    "url": dataurl, "file_name": asset["file_name"]
                }})
                images_for_history.append((asset["file_name"], asset["bytes"])) # æ–¹ä¾¿é¡¯ç¤ºç¸®åœ–
            else:
                st.warning(f"{getattr(f,'name','æª”æ¡ˆ')} æ ¼å¼ä¸æ”¯æ´æˆ–å…§å®¹ç•°å¸¸ï½")

    # 2. appendåˆ°messages
    if content_blocks:
        st.session_state.messages.append(HumanMessage(content=content_blocks))
        # UIé¡¯ç¤º
        with st.chat_message("user"):
            for block in content_blocks:
                if block.get("type") == "text":
                    st.write(block["text"])
                elif block.get("type") == "image_url":
                    info = block["image_url"]
                    st.image(info["url"], caption=info.get("file_name", ""), width=220)

    # 3. murmur & agenté‹ä½œ
    all_text = []
    for msg in st.session_state.messages:
        if hasattr(msg, "content"):
            if isinstance(msg.content, str):
                all_text.append(msg.content)
            elif isinstance(msg.content, list):
                for part in msg.content:
                    if part.get("type") == "text":
                        all_text.append(part["text"])
    all_text = "\n".join(all_text)

    status_prompt = f"""ä½ æ˜¯å®‰å¦®äºï¼Œè«‹æ ¹æ“šèŠå¤©ç´€éŒ„è‡ªè¨€è‡ªèªä¸€å¥å¯æ„› murmurï¼ˆ15å­—å…§ï¼‰ã€‚{all_text}"""
    status_response = client.chat.completions.create(
        model="gpt-4.1-nano",
        messages=[{"role": "user", "content": status_prompt}]
    )
    status_label = status_response.choices[0].message.content.strip()

    with st.chat_message("assistant"):
        status = st.status(status_label)
        # å¦‚æœä½ æœ‰ get_streamlit_cb å¯ä»¥åŠ é€²agentå›å‘¼ï¼ˆé€™è£¡å¯ç•¥éï¼‰
        response = agent.invoke({"messages": st.session_state.messages})
        ai_msg = response["messages"][-1]
        st.session_state.messages.append(ai_msg)
        status.update(label="å®‰å¦®äºå›ç­”å®Œç•¢ï¼ğŸ‰", state="complete")
