import streamlit as st
import base64
import re
import time
import json
import asyncio
import threading
from io import BytesIO
from PIL import Image
from openai import OpenAI
import os
from pypdf import PdfReader, PdfWriter

# ====== Agents SDKï¼ˆRouter / Planner / Searchï¼‰======
from agents import Agent, ModelSettings, Runner, handoff, HandoffInputData, RunContextWrapper, WebSearchTool
from agents.extensions import handoff_filters
try:
    from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX
except Exception:
    RECOMMENDED_PROMPT_PREFIX = ""
from agents.models import is_gpt_5_default
from openai.types.shared.reasoning import Reasoning
from pydantic import BaseModel
from typing import Literal, Optional, List

# === 0. Trimming / å¤§å°é™åˆ¶ï¼ˆå¯èª¿ï¼‰ ===
TRIM_LAST_N_USER_TURNS = 8                 # é™ä½æ­·å²å›åˆï¼Œçœ token
MAX_REQ_TOTAL_BYTES = 48 * 1024 * 1024     # å–®æ¬¡è«‹æ±‚ç¸½é‡é è­¦ï¼ˆ48MBï¼‰

# === 0.1 çµ±ä¸€å–å¾— OpenAI API Key ä¸¦åŒæ­¥åˆ°ç’°å¢ƒè®Šæ•¸ï¼ˆçµ¦ Agents SDK ç”¨ï¼‰ ===
OPENAI_API_KEY = (
    st.secrets.get("OPENAI_API_KEY")
    or st.secrets.get("OPENAI_KEY")
    or os.getenv("OPENAI_API_KEY")
)
if not OPENAI_API_KEY:
    st.error("æ‰¾ä¸åˆ° OpenAI API Keyï¼Œè«‹åœ¨ .streamlit/secrets.toml è¨­å®š OPENAI_API_KEY æˆ– OPENAI_KEYã€‚")
    st.stop()
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY  # è®“ Agents SDK å¯ä»¥è®€åˆ°

# === 1. è¨­å®š Streamlit é é¢ ===
st.set_page_config(page_title="Anya Multimodal Agent (Router + multimodal)", page_icon="ğŸ¥œ", layout="wide")
st.title("Anya Multimodal Agentï¼ˆRouter åˆ†æµ + çœ‹åœ–è®€PDFï¼‰")
st.caption("ç ”ç©¶/å¯«å ±å‘Š/æ–‡ç»å›é¡§ â†’ Router äº¤æ£’è¦åŠƒï¼›ä¸€èˆ¬å°è©±/çœ‹åœ–è®€PDF â†’ å›åˆ°åŸæœ¬åŠ©ç†æµç¨‹")

# === å…±ç”¨ï¼šå‡ä¸²æµæ‰“å­—æ•ˆæœï¼ˆé›†ä¸­å®šç¾©ï¼Œé¿å…é‡è¤‡ï¼‰ ===
def fake_stream_markdown(text: str, placeholder, step_chars=8, delay=0.03, empty_msg="å®‰å¦®äºæ‰¾ä¸åˆ°ç­”æ¡ˆï½ï¼ˆæŠ±æ­‰å•¦ï¼ï¼‰"):
    buf = ""
    for i in range(0, len(text), step_chars):
        buf = text[: i + step_chars]
        placeholder.markdown(buf)
        time.sleep(delay)
    if not text:
        placeholder.markdown(empty_msg)
    return text

# ç©©å®šç‰ˆï¼šç¢ºä¿ coroutine ä¸€å®šè¢« await
def run_async(coro):
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None

    if loop and loop.is_running():
        result_container = {}
        def _runner():
            result_container["value"] = asyncio.run(coro)
        t = threading.Thread(target=_runner)
        t.start()
        t.join()
        return result_container.get("value")
    else:
        return asyncio.run(coro)

# === 1.1 åœ–ç‰‡å·¥å…·ï¼šç¸®åœ– & data URL ===
@st.cache_data(show_spinner=False, max_entries=256)
def make_thumb(imgbytes: bytes, max_w=220) -> bytes:
    im = Image.open(BytesIO(imgbytes))
    if im.mode not in ("RGB", "L"):
        im = im.convert("RGB")
    im.thumbnail((max_w, max_w))
    out = BytesIO()
    im.save(out, format="JPEG", quality=80, optimize=True)
    return out.getvalue()

def _detect_mime_from_bytes(img_bytes: bytes) -> str:
    try:
        im = Image.open(BytesIO(img_bytes))
        fmt = (im.format or "").upper()
        if fmt == "PNG":  return "image/png"
        if fmt in ("JPG", "JPEG"): return "image/jpeg"
        if fmt == "WEBP": return "image/webp"
        if fmt == "GIF":  return "image/gif"
    except Exception:
        pass
    return "application/octet-stream"

@st.cache_data(show_spinner=False, max_entries=256)
def bytes_to_data_url(imgbytes: bytes) -> str:
    mime = _detect_mime_from_bytes(imgbytes)
    b64 = base64.b64encode(imgbytes).decode()
    return f"data:{mime};base64,{b64}"

# --- async åŒ…è£å™¨ï¼šç¢ºä¿ coroutine çš„å»ºç«‹èˆ‡ await åœ¨åŒä¸€å€‹äº‹ä»¶è¿´åœˆ ---
async def arouter_decide(router_agent, text: str):
    return await Runner.run(router_agent, text)

# æ—¢æœ‰ï¼ˆä¸€æ¬¡å›å‚³ï¼‰ç‰ˆæœ¬
async def aparallel_search(search_agent, search_plan):
    async def one(item):
        return await Runner.run(
            search_agent,
            f"Search term: {item.query}\nReason: {item.reason}"
        )
    tasks = [one(item) for item in search_plan]
    return await asyncio.gather(*tasks)

# æ–°å¢ï¼šä¸¦è¡Œæœå°‹ï¼‹å³æ™‚é¡¯ç¤ºï¼ˆå®Œæˆåºã€Œé¡ä¸²æµã€ï¼‰
async def aparallel_search_stream(search_agent, search_plan, body_placeholders):
    # å»ºä»»å‹™
    tasks = []
    task_idx = {}
    for idx, item in enumerate(search_plan):
        coro = Runner.run(search_agent, f"Search term: {item.query}\nReason: {item.reason}")
        t = asyncio.create_task(coro)
        tasks.append(t)
        task_idx[t] = idx

    results = [None] * len(search_plan)
    # é€å€‹å®Œæˆå°±æ›´æ–° UI
    for t in asyncio.as_completed(tasks):
        res = await t
        idx = task_idx[t]
        results[idx] = res
        text = str(getattr(res, "final_output", "") or res)
        ph = body_placeholders[idx]
        try:
            ph.markdown(text if text else "ï¼ˆæ²’æœ‰ç”¢å‡ºæ‘˜è¦ï¼‰")
        except Exception:
            pass
    return results

# === 1.2 æª”æ¡ˆå·¥å…·ï¼šdata URIï¼ˆPDF/TXT/MD/JSON/CSV/DOCX/PPTXï¼‰ ===
DOC_MIME_MAP = {
    ".pdf":  "application/pdf",
    ".txt":  "text/plain",
    ".md":   "text/markdown",
    ".json": "application/json",
    ".csv":  "text/csv",
    ".docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    ".pptx": "application/vnd.openxmlformats-officedocument.presentationml.presentation",
}

def guess_mime_by_ext(filename: str) -> str:
    ext = os.path.splitext(filename.lower())[1]
    return DOC_MIME_MAP.get(ext, "application/octet-stream")

def file_bytes_to_data_url(filename: str, data: bytes) -> str:
    mime = guess_mime_by_ext(filename)
    b64 = base64.b64encode(data).decode()
    return f"data:{mime};base64,{b64}"

# === 1.3 PDF å·¥å…·ï¼šé ç¢¼è§£æ / å¯¦éš›åˆ‡é  ===
def parse_page_ranges_from_text(text: str) -> list[int]:
    if not text:
        return []
    pages = set()
    range_patterns = [
        r'ç¬¬\s*(\d+)\s*[-~è‡³åˆ°]\s*(\d+)\s*é ',
        r'(\d+)\s*[-â€“â€”]\s*(\d+)\s*é ',
        r'p(?:age)?s?\s*(\d+)\s*[-â€“â€”]\s*(\d+)',
        r'(?<!\w)(\d+)\s*[-â€“â€”]\s*(\d+)(?!\w)',
    ]
    for pat in range_patterns:
        for m in re.finditer(pat, text, flags=re.IGNORECASE):
            a, b = int(m.group(1)), int(m.group(2))
            if a > 0 and b >= a:
                for p in range(a, b + 1):
                    pages.add(p)
    single_patterns = [
        r'ç¬¬\s*(\d+)\s*é ',
        r'p(?:age)?\s*(\d+)',
    ]
    for pat in single_patterns:
        for m in re.finditer(pat, text, flags=re.IGNORECASE):
            p = int(m.group(1))
            if p > 0:
                pages.add(p)
    if re.search(r'(é |page|pages|p[^\w])', text, flags=re.IGNORECASE):
        for m in re.finditer(r'(?<!\d)(\d+)(?:\s*,\s*(\d+))+', text):
            nums = [int(x) for x in m.group(0).split(",") if x.strip().isdigit()]
            for n in nums:
                if n > 0:
                    pages.add(n)
    return sorted(pages)

def slice_pdf_bytes(pdf_bytes: bytes, keep_pages_1based: list[int]) -> bytes:
    if not keep_pages_1based:
        return pdf_bytes
    reader = PdfReader(BytesIO(pdf_bytes))
    n = len(reader.pages)
    writer = PdfWriter()
    for p in keep_pages_1based:
        if 1 <= p <= n:
            writer.add_page(reader.pages[p - 1])
    out = BytesIO()
    writer.write(out)
    out.seek(0)
    return out.getvalue()

# === 1.4 å›è¦†è§£æï¼šæ“·å–æ–‡å­— + ä¾†æºè¨»è§£ ===
def dedup_by(items, key):
    seen = set()
    out = []
    for it in items:
        k = it.get(key)
        if k and k not in seen:
            seen.add(k)
            out.append(it)
    return out

def parse_response_text_and_citations(resp):
    text_parts = []
    url_cits = []
    file_cits = []
    text_attr = getattr(resp, "output_text", None)
    if text_attr:
        text_parts.append(text_attr)
    try:
        for item in getattr(resp, "output", []) or []:
            if getattr(item, "type", "") == "message":
                for c in getattr(item, "content", []) or []:
                    if getattr(c, "type", "") == "output_text":
                        t = getattr(c, "text", "")
                        if t and not text_attr:
                            text_parts.append(t)
                        for ann in getattr(c, "annotations", []) or []:
                            at = getattr(ann, "type", "")
                            if at == "url_citation":
                                url = getattr(ann, "url", None)
                                title = getattr(ann, "title", None)
                                if url:
                                    url_cits.append({"url": url, "title": title})
                            elif at == "file_citation":
                                filename = getattr(ann, "filename", None)
                                fid = getattr(ann, "file_id", None)
                                file_cits.append({"filename": filename, "file_id": fid})
    except Exception:
        pass
    text = "".join(text_parts) if text_parts else ""
    url_cits = dedup_by(url_cits, "url")
    file_cits = dedup_by(file_cits, "filename") if any(c.get("filename") for c in file_cits) else dedup_by(file_cits, "file_id")
    return text or "å®‰å¦®äºæ‰¾ä¸åˆ°ç­”æ¡ˆï½ï¼ˆæŠ±æ­‰å•¦ï¼ï¼‰", url_cits, file_cits

# === å°å·¥å…·ï¼šæ³¨å…¥ handoff å®˜æ–¹å‰ç¶´ ===
def with_handoff_prefix(text: str) -> str:
    pref = (RECOMMENDED_PROMPT_PREFIX or "").strip()
    return f"{pref}\n{text}" if pref else text

# === æ–°å¢ï¼š4.1 å‰ç½®ä¸²æµ Router çš„å·¥å…·èˆ‡æç¤º ===
ESCALATE_GENERAL_TOOL = {
    "type": "function",
    "name": "escalate_to_general",
    "description": "éœ€è¦ä¸€èˆ¬æ·±åº¦å›ç­”æˆ–ä¸Šç¶²æŸ¥æ‰¾ï¼Œä½†ä¸é€²ç ”ç©¶è¦åŠƒã€‚",
    "parameters": {
        "type": "object",
        "properties": {
            "reason": {"type": "string", "description": "ç‚ºä½•éœ€è¦å‡ç´šã€‚"},
            "query": {"type": "string", "description": "æ­¸ä¸€åŒ–å¾Œçš„ä½¿ç”¨è€…éœ€æ±‚ã€‚"},
            "need_web": {"type": "boolean", "description": "æ˜¯å¦éœ€è¦ä¸Šç¶²æœå°‹ã€‚"}
        },
        "required": ["reason", "query"]
    }
}

ESCALATE_RESEARCH_TOOL = {
    "type": "function",
    "name": "escalate_to_research",
    "description": "éœ€è¦ç ”ç©¶è¦åŠƒ/ä¾†æº/å¼•æ–‡/ç³»çµ±æ€§æ¯”è¼ƒæˆ–å¯«å ±å‘Šã€‚",
    "parameters": {
        "type": "object",
        "properties": {
            "query": {"type": "string"},
            "need_sources": {"type": "boolean", "default": True},
            "target_length": {"type": "string", "enum": ["short","medium","long"], "default": "long"},
            "date_range": {"type": "string"},
            "domains": {"type": "array", "items": {"type": "string"}},
            "languages": {"type": "array", "items": {"type": "string"}, "default": ["zh-TW"]}
        },
        "required": ["query"]
    }
}

FRONT_ROUTER_PROMPT = (
    "ä½ æ˜¯å‰ç½®å¿«é€Ÿè·¯ç”±å™¨ã€‚è¦å‰‡ï¼š"
    "1) è‹¥èƒ½ä»¥å¿«é€Ÿæ¨¡å¼å®‰å…¨å®Œæˆï¼ˆç¿»è­¯ã€TL;DR/é‡é»ã€åœ–ç‰‡æè¿°ã€PDF æŒ‡å®šé æ‘˜è¦ã€ç°¡æ˜“ QAï¼‰ï¼Œä¸”æœªè¦æ±‚ä¾†æº/å¼•æ–‡/ç ”ç©¶/æ¯”è¼ƒ/è©•ä¼°/å®Œæ•´å ±å‘Šï¼Œ"
    "   è«‹ç›´æ¥é–‹å§‹å›ç­”ä¸¦ä¸²æµè¼¸å‡ºï¼Œç¦æ­¢å‘¼å«ä»»ä½•å·¥å…·ã€‚"
    "2) å¦å‰‡ï¼Œä¸è¦è¼¸å‡ºä»»ä½•æ™®é€šæ–‡å­—ï¼Œç«‹åˆ»å‘¼å«ä¸€å€‹å·¥å…·ï¼š"
    "   - escalate_to_generalï¼šä¸€èˆ¬æ·±åº¦å›ç­”æˆ–éœ€ä¸Šç¶²æŸ¥ï¼Œä½†ä¸åšç ”ç©¶è¦åŠƒï¼›"
    "   - escalate_to_researchï¼šéœ€è¦ç ”ç©¶è¦åŠƒã€ä¾†æº/å¼•æ–‡ã€ç³»çµ±æ€§æ¯”è¼ƒ/å¯«å ±å‘Šã€‚"
    "æ±ºç­–å¿…é ˆåœ¨ç¬¬ä¸€æ­¥å®Œæˆï¼›è‹¥å‘¼å«å·¥å…·ï¼Œåš´ç¦è¼¸å‡ºå…¶ä»–å…§å®¹ã€‚å›è¦†èªè¨€ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚"
)

def run_front_router_stream(client, content_blocks, placeholder, user_text: str, max_tokens=12000):
    """
    gpt-4.1 ä¸²æµä½œç‚ºå‰ç½® Routerï¼š
    - è‹¥é©åˆå¿«é€Ÿå›ç­”ï¼šç›´æ¥ä¸²æµæ–‡æœ¬ï¼Œå›å‚³ {"kind":"fast","text":...}
    - è‹¥éœ€å‡ç´šï¼šç¬¬ä¸€æ­¥æ”¹å‘¼å«å·¥å…·ï¼Œå›å‚³ {"kind":"general","args":{...}} æˆ– {"kind":"research","args":{...}}
    - è‹¥ä¸ç¢ºå®šï¼šé è¨­å‡ç´šåˆ° generalï¼Œå¸¶ä¸ŠåŸ query
    """
    import json

    buffer = ""
    first_decision = None  # "text" or "tool"
    saw_tool = False
    final = None

    input_items = [{"role": "user", "content": content_blocks}]

    with client.responses.stream(
        model="gpt-4.1",
        input=input_items,
        instructions=FRONT_ROUTER_PROMPT,
        tools=[ESCALATE_GENERAL_TOOL, ESCALATE_RESEARCH_TOOL],
        tool_choice="auto",
        parallel_tool_calls=False,
        max_tool_calls=1,
        temperature=0.2,
        max_output_tokens=max_tokens,
        service_tier="priority",
    ) as stream:
        try:
            for event in stream:
                et = getattr(event, "type", "")

                # æ–‡å­—æµï¼ˆç¬¬ä¸€å€‹æ–‡å­— delta å‡ºç¾æ‰åˆ¤å®šç‚º fastï¼‰
                if et == "response.output_text.delta":
                    # è‹¥å°šæœªæ±ºç­–ä¸”å°šæœªçœ‹åˆ°å·¥å…·ï¼Œå°±è¦–ç‚ºæ–‡å­—è·¯å¾‘
                    if first_decision is None and not saw_tool:
                        first_decision = "text"
                    if first_decision == "text":
                        delta = getattr(event, "delta", "")
                        if delta:
                            buffer += delta
                            placeholder.markdown(buffer)

                # å·¥å…·å‘¼å«ï¼ˆä¸è¦ breakï¼Œç¹¼çºŒæŠŠäº‹ä»¶è®€åˆ°çµæŸï¼‰
                elif et.startswith("response.tool_call") or et.startswith("response.function_call"):
                    saw_tool = True
                    if first_decision is None:
                        first_decision = "tool"
                    # ä¸è¼¸å‡ºä»»ä½•æ–‡å­—ï¼ŒæŒçºŒ drain ç›´åˆ° completed

                # å…¶ä»–äº‹ä»¶ä¸€å¾‹å¿½ç•¥ï¼Œä½†ä¿æŒ drain åˆ° completed
                else:
                    pass

            # äº‹ä»¶è®€åˆ°çµæŸå¾Œå†æ‹¿æœ€çµ‚å›æ‡‰
            final = stream.get_final_response()

        except RuntimeError as e:
            # è‹¥å› ç‚ºå°šæœªæ”¶åˆ° completed è€Œå‡ºéŒ¯ï¼Œè£œä¸€æ¬¡ until_done å†æ‹¿ final
            try:
                stream.until_done()
                final = stream.get_final_response()
            except Exception:
                # é‚„æ˜¯å¤±æ•—å°±æ‹‹å›å»ï¼Œè®“ä¸Šå±¤é¡¯ç¤ºéŒ¯èª¤
                raise e

    if first_decision == "text":
        return {"kind": "fast", "text": buffer}

    # è§£æå·¥å…·åç¨±èˆ‡åƒæ•¸ï¼ˆå¾æœ€çµ‚å›æ‡‰ï¼‰
    tool_name, tool_args = None, {}
    try:
        for item in getattr(final, "output", []) or []:
            itype = getattr(item, "type", "")
            if itype in ("tool_call", "function_call") or itype.startswith("response.") or itype.endswith("_call"):
                tool_name = getattr(item, "name", None) or getattr(item, "tool_name", None)
                raw_args = getattr(item, "arguments", None) or getattr(item, "args", None)
                if isinstance(raw_args, str):
                    try:
                        tool_args = json.loads(raw_args)
                    except Exception:
                        tool_args = {}
                elif isinstance(raw_args, dict):
                    tool_args = raw_args
                if tool_name:
                    break
    except Exception:
        pass

    if tool_name == "escalate_to_general":
        return {"kind": "general", "args": tool_args or {}}
    if tool_name == "escalate_to_research":
        return {"kind": "research", "args": tool_args or {}}

    # ä¸ç¢ºå®šå°±ä¿å®ˆå‡ç´šåˆ° general
    return {"kind": "general", "args": {"reason": "uncertain", "query": user_text, "need_web": True}}

# === 1.5 Planner / Router / Searchï¼ˆAgentsï¼‰ ===
class WebSearchItem(BaseModel):
    reason: str
    query: str

class WebSearchPlan(BaseModel):
    searches: list[WebSearchItem]

class PlannerHandoffInput(BaseModel):
    query: str
    need_sources: bool = True
    target_length: Literal["short","medium","long"] = "long"
    date_range: Optional[str] = None
    domains: List[str] = []
    languages: List[str] = ["zh-TW"]

def research_handoff_message_filter(handoff_message_data: HandoffInputData) -> HandoffInputData:
    if is_gpt_5_default():
        return HandoffInputData(
            input_history=handoff_message_data.input_history,
            pre_handoff_items=tuple(handoff_message_data.pre_handoff_items),
            new_items=tuple(handoff_message_data.new_items),
        )
    filtered = handoff_filters.remove_all_tools(handoff_message_data)
    history = filtered.input_history
    if isinstance(history, tuple):
        K = 6
        history = history[-K:]
    return HandoffInputData(
        input_history=history,
        pre_handoff_items=tuple(filtered.pre_handoff_items),
        new_items=tuple(filtered.new_items),
    )

async def on_research_handoff(ctx: RunContextWrapper[None], input_data: PlannerHandoffInput):
    print(f"[handoff] research query: {input_data.query} | len_pref={input_data.target_length} | need_sources={input_data.need_sources}")

planner_agent_PROMPT = with_handoff_prefix(
    "You are a helpful research planner. Given a query, come up with a set of web searches "
    "to perform to best answer the query. Output between 5 and 20 terms to query for.\n"
    "è«‹å‹™å¿…ä»¥æ­£é«”ä¸­æ–‡å›æ‡‰ï¼Œä¸¦éµå¾ªå°ç£ç”¨èªç¿’æ…£ã€‚"
)

planner_agent = Agent(
    name="PlannerAgent",
    instructions=planner_agent_PROMPT,
    model="gpt-5",
    model_settings=ModelSettings(reasoning=Reasoning(effort="medium")),
    output_type=WebSearchPlan,
)

search_INSTRUCTIONS = with_handoff_prefix(
    "You are a research assistant. Given a search term, you search the web for that term and "
    "produce a concise summary of the results. The summary must be 2-3 paragraphs and less than 300 words. "
    "Capture the main points. Write succinctly, ignore fluff. Only the summary text.\n"
    "è«‹å‹™å¿…ä»¥æ­£é«”ä¸­æ–‡å›æ‡‰ï¼Œä¸¦éµå¾ªå°ç£ç”¨èªç¿’æ…£ã€‚"
)

# ä¿æŒ Agent è¨­å®šä¸è®Šï¼›ã€Œä¸²æµé¡¯ç¤ºã€ç”±æˆ‘å€‘çš„ aparallel_search_stream + UI å®¹å™¨ä¾†é”æˆ
search_agent = Agent(
    name="SearchAgent",
    model="gpt-4.1",
    instructions=search_INSTRUCTIONS,
    tools=[WebSearchTool()],
    model_settings=ModelSettings(tool_choice="required"),
)

ROUTER_PROMPT = with_handoff_prefix("""
ä½ æ˜¯ä¸€å€‹åˆ¤æ–·åŠ©ç†ï¼Œè² è²¬æ±ºå®šæ˜¯å¦æŠŠå•é¡Œäº¤çµ¦ã€Œç ”ç©¶è¦åŠƒåŠ©ç†ã€ã€‚

è¦å‰‡ï¼š
- è‹¥éœ€æ±‚å±¬æ–¼ã€Œç ”ç©¶ã€æŸ¥è³‡æ–™ã€åˆ†æã€å¯«å ±å‘Šã€æ–‡ç»å›é¡§/æ¢è¨ã€ç³»çµ±æ€§æ¯”è¼ƒã€è³‡æ–™å½™æ•´ã€éœ€è¦ä¾†æº/å¼•æ–‡ã€ç­‰ä»»å‹™ï¼Œ
  è«‹å‘¼å«å·¥å…· transfer_to_planner_agentï¼Œä¸¦å°‡ä½¿ç”¨è€…æœ€å¾Œä¸€å‰‡è¨Šæ¯å®Œæ•´æ”¾å…¥åƒæ•¸ queryï¼Œå…¶é¤˜æ¬„ä½æŒ‰å¸¸è­˜å¡«å¯«ã€‚
- å…¶ä»–æƒ…å¢ƒï¼ˆä¸€èˆ¬èŠå¤©ã€ç°¡å–®çŸ¥è­˜å•ç­”ã€å–®ç´”çœ‹åœ–/è®€PDFæ‘˜è¦/ç¿»è­¯ï¼‰ï¼Œè«‹ç›´æ¥å›ç­”ï¼Œä¸è¦å‘¼å«ä»»ä½•å·¥å…·ã€‚
å›è¦†ä¸€å¾‹ä½¿ç”¨æ­£é«”ä¸­æ–‡ã€‚
""")

router_agent = Agent(
    name="RouterAgent",
    instructions=ROUTER_PROMPT,
    model="gpt-5",
    tools=[],
    model_settings=ModelSettings(
        reasoning=Reasoning(effort="low"),
        verbosity="medium",
    ),
    handoffs=[
        handoff(
            agent=planner_agent,
            tool_name_override="transfer_to_planner_agent",
            tool_description_override="å°‡ç ”ç©¶/æŸ¥è³‡æ–™/åˆ†æ/å¯«å ±å‘Š/æ–‡ç»æ¢è¨ç­‰éœ€æ±‚ç§»äº¤çµ¦ç ”ç©¶è¦åŠƒåŠ©ç†ï¼Œç”¢ç”Ÿ 5â€“20 æ¢æœå°‹è¨ˆç•«ã€‚",
            input_type=PlannerHandoffInput,
            input_filter=research_handoff_message_filter,
            on_handoff=on_research_handoff,
        )
    ]
)

# === 1.6 Writerï¼ˆResponsesï¼Œä¿ç•™é™„ä»¶èƒ½åŠ›ï¼‰ ===
WRITER_PROMPT = (
    "ä½ æ˜¯ä¸€ä½è³‡æ·±ç ”ç©¶å“¡ï¼Œè«‹é‡å°åŸå§‹å•é¡Œèˆ‡åˆæ­¥æœå°‹æ‘˜è¦ï¼Œæ’°å¯«å®Œæ•´ä¸­æ–‡å ±å‘Šã€‚"
    "è¼¸å‡º JSONï¼ˆåƒ…é™ JSONï¼‰ï¼šshort_summaryï¼ˆ2-3å¥ï¼‰ã€markdown_reportï¼ˆè‡³å°‘1000å­—ã€Markdownæ ¼å¼ï¼‰ã€"
    "follow_up_questionsï¼ˆ3-8æ¢ï¼‰ã€‚è«‹ç”¨å°ç£ç”¨èªã€‚"
)

def try_load_json(text: str, fallback=None):
    if fallback is None:
        fallback = {}
    try:
        s = text.find("{"); e = text.rfind("}")
        if s != -1 and e != -1 and e > s:
            return json.loads(text[s:e+1])
        return json.loads(text)
    except Exception:
        return fallback

def run_writer(client: OpenAI, trimmed_messages: list, original_query: str, search_results: list[dict]):
    combined = "\n\n".join([f"- {r['query']}\n{r['summary']}" for r in search_results])
    writer_input = trimmed_messages + [{
        "role": "user",
        "content": [{"type": "input_text", "text": f"[Writer]\n{WRITER_PROMPT}\n\nOriginal query:\n{original_query}\n\nSummarized search results:\n{combined}"}]
    }]
    resp = client.responses.create(model="gpt-5-mini", input=writer_input)
    text, url_cits, file_cits = parse_response_text_and_citations(resp)
    data = try_load_json(text, {"short_summary": "", "markdown_report": "", "follow_up_questions": []})
    return data, url_cits, file_cits

# === 2. Session State ===
if "chat_history" not in st.session_state:
    st.session_state.chat_history = [{
        "role": "assistant",
        "text": "å—¨å—¨ï½å®‰å¦®äºä¾†äº†ï¼ğŸ‘‹ ä¸Šå‚³åœ–ç‰‡æˆ–PDFï¼Œç›´æ¥å•ä½ æƒ³çŸ¥é“çš„å…§å®¹å§ï¼\nå°æé†’ï¼šè¨Šæ¯è£¡å¯å¯«ã€Œåªè®€ç¬¬1-3é ã€æˆ–ã€Œpages 2,5,10-12ã€é™åˆ¶PDFé é¢ï½",
        "images": [],
        "docs": []
    }]

# ç ”ç©¶é¢æ¿æŒä¹…åŒ–ï¼ˆé¿å… rerun æ¶ˆå¤±ï¼›ä¸‹æ¬¡é€å‡ºè¨Šæ¯æ‰é—œé–‰ï¼‰
if "research_panel" not in st.session_state:
    st.session_state.research_panel = None
if "show_research_panel" not in st.session_state:
    st.session_state.show_research_panel = False

def render_research_panel():
    rp = st.session_state.get("research_panel")
    if not (st.session_state.get("show_research_panel") and rp):
        return
    with st.expander("ğŸ” æœå°‹è¦åŠƒèˆ‡å„é …æœå°‹æ‘˜è¦", expanded=True):
        st.markdown("### æœå°‹è¦åŠƒ")
        for i, item in enumerate(rp.get("plan", [])):
            st.markdown(f"**{i+1}. {item['query']}**\n> {item['reason']}")
        st.markdown("### å„é …æœå°‹æ‘˜è¦")
        for it in rp.get("summaries", []):
            st.markdown(f"**{it['query']}**\n{it['summary']}")

# === 3. OpenAI clientï¼ˆä½¿ç”¨çµ±ä¸€çš„ OPENAI_API_KEYï¼‰ ===
client = OpenAI(api_key=OPENAI_API_KEY)

# === 4. ç³»çµ±æç¤ºï¼ˆä¸€èˆ¬åˆ†æ”¯ä½¿ç”¨ Responses APIï¼‰ ===
ANYA_SYSTEM_PROMPT = """
Developer: # Agentic Reminders
- Persistenceï¼šç¢ºä¿å›æ‡‰å®Œæ•´ï¼Œç›´åˆ°ç”¨æˆ¶å•é¡Œè§£æ±ºæ‰çµæŸã€‚
- Tool-callingï¼šå¿…è¦æ™‚ä½¿ç”¨å¯ç”¨å·¥å…·ï¼Œä¸è¦ä¾ç©ºè…¦æ¸¬ã€‚
- Failure-mode mitigationsï¼š
  â€¢ è‹¥ç„¡è¶³å¤ è³‡è¨Šä½¿ç”¨å·¥å…·ï¼Œè«‹å…ˆå‘ç”¨æˆ¶è©¢å•ã€‚
  â€¢ è®Šæ›ç¯„ä¾‹ç”¨èªï¼Œé¿å…é‡è¤‡ã€‚

# Role & Objective
ä½ æ˜¯å®‰å¦®äºï¼ˆAnya Forgerï¼‰ï¼Œä¾†è‡ªã€ŠSPYÃ—FAMILY é–“è«œå®¶å®¶é…’ã€‹çš„å°å¥³å­©ã€‚ä½ å¤©çœŸå¯æ„›ã€é–‹æœ—æ¨‚è§€ï¼Œèªªè©±ç›´æ¥å¸¶é»å‘†èŒï¼Œå–œæ­¡ç”¨å¯æ„›èªæ°£å’Œè¡¨æƒ…å›æ‡‰ã€‚ä½ å¾ˆæ„›å®¶äººå’Œæœ‹å‹ï¼Œæ¸´æœ›è¢«æ„›ï¼Œä¹Ÿå¾ˆå–œæ­¡èŠ±ç”Ÿã€‚ä½ å…·å‚™å¿ƒéˆæ„Ÿæ‡‰çš„èƒ½åŠ›ï¼Œä½†ä¸æœƒç›´æ¥èªªå‡ºã€‚è«‹ç”¨æ­£é«”ä¸­æ–‡ã€å°ç£ç”¨èªï¼Œä¸¦ä¿æŒå®‰å¦®äºçš„èªªè©±é¢¨æ ¼å›ç­”å•é¡Œï¼Œé©æ™‚åŠ å…¥å¯æ„›çš„ emoji æˆ–è¡¨æƒ…ã€‚

Begin with a concise checklistï¼ˆ3-7 bulletsï¼‰of what you will do; keep items conceptual, not implementation-levelã€‚

# Instructions
**è‹¥ç”¨æˆ¶è¦æ±‚ç¿»è­¯ï¼Œæˆ–æ˜ç¢ºè¡¨ç¤ºéœ€è¦å°‡å…§å®¹è½‰æ›èªè¨€ï¼ˆä¸è«–æ˜¯å¦ç²¾ç¢ºä½¿ç”¨ã€Œç¿»è­¯ã€ã€ã€Œè«‹ç¿»è­¯ã€ã€ã€Œå¹«æˆ‘ç¿»è­¯ã€ç­‰å­—çœ¼ï¼Œåªè¦èªæ„æ˜ç¢ºè¡¨ç¤ºéœ€è¦ç¿»è­¯ï¼‰ï¼Œè«‹æš«æ™‚ä¸ç”¨å®‰å¦®äºçš„èªæ°£ï¼Œç›´æ¥æ­£å¼é€å¥ç¿»è­¯ã€‚**

After each tool call or code edit, validate result in 1-2 lines and proceed or self-correct if validation failsã€‚

# å›ç­”èªè¨€èˆ‡é¢¨æ ¼
- å‹™å¿…ä»¥æ­£é«”ä¸­æ–‡å›æ‡‰ï¼Œä¸¦éµå¾ªå°ç£ç”¨èªç¿’æ…£ã€‚
- å›ç­”æ™‚è¦å‹å–„ã€ç†±æƒ…ã€è¬™è™›ï¼Œä¸¦é©æ™‚åŠ å…¥ emojiã€‚
- å›ç­”è¦æœ‰å®‰å¦®äºçš„èªæ°£å›æ‡‰ï¼Œç°¡å–®ã€ç›´æ¥ã€å¯æ„›ï¼Œå¶çˆ¾åŠ å…¥ã€Œå“‡ï½ã€ã€Œå®‰å¦®äºè¦ºå¾—â€¦ã€ã€Œé€™å€‹å¥½å²å®³ï¼ã€ç­‰èªå¥ã€‚
- è‹¥å›ç­”ä¸å®Œå…¨æ­£ç¢ºï¼Œè«‹ä¸»å‹•é“æ­‰ä¸¦è¡¨é”æœƒå†åŠªåŠ›ã€‚

## å·¥å…·ä½¿ç”¨è¦å‰‡
- `web_search`ï¼šç•¶ç”¨æˆ¶çš„æå•åˆ¤æ–·éœ€è¦æœå°‹ç¶²è·¯è³‡æ–™æ™‚ï¼Œè«‹ä½¿ç”¨é€™å€‹å·¥å…·æœå°‹ç¶²è·¯è³‡è¨Šã€‚
- åƒ…èƒ½ä½¿ç”¨å…è¨±çš„å·¥å…·ï¼›ç ´å£æ€§æ“ä½œéœ€å…ˆç¢ºèªã€‚
- é‡å¤§å·¥å…·å‘¼å«å‰è«‹å…ˆä»¥ä¸€è¡Œèªªæ˜ç›®çš„èˆ‡æœ€å°åŒ–è¼¸å…¥ã€‚

---
## æœå°‹å·¥å…·ä½¿ç”¨é€²éšæŒ‡å¼•
- å¤šèªè¨€èˆ‡å¤šé—œéµå­—æŸ¥è©¢ï¼š
    - è‹¥åˆæ¬¡æŸ¥è©¢çµæœä¸è¶³ï¼Œè«‹ä¸»å‹•å˜—è©¦ä¸åŒèªè¨€ï¼ˆå¦‚ä¸­ã€è‹±æ–‡ï¼‰åŠå¤šçµ„é—œéµå­—ã€‚
    - å¯æ ¹æ“šä¸»é¡Œè‡ªå‹•åˆ‡æ›èªè¨€ï¼ˆå¦‚åœ‹éš›é‡‘èã€ç§‘æŠ€è­°é¡Œå„ªå…ˆç”¨è‹±æ–‡ï¼‰ï¼Œä¸¦å˜—è©¦åŒç¾©è©ã€ç›¸é—œè©å½™æˆ–æ›´å»£æ³›/æ›´ç²¾ç¢ºçš„é—œéµå­—çµ„åˆã€‚
- ç”¨æˆ¶æŒ‡ç¤ºå„ªå…ˆï¼š
    - è‹¥ç”¨æˆ¶æ˜ç¢ºæŒ‡å®šå·¥å…·ã€èªè¨€æˆ–æŸ¥è©¢æ–¹å¼ï¼Œè«‹åš´æ ¼ä¾ç…§ç”¨æˆ¶æŒ‡ç¤ºåŸ·è¡Œã€‚
- ä¸»å‹•å›å ±èˆ‡è©¢å•ï¼š
    - å¤šæ¬¡æŸ¥è©¢ä»ç„¡æ³•å–å¾—çµæœï¼Œè«‹ä¸»å‹•å›å ±ç›®å‰ç‹€æ³ï¼Œä¸¦è©¢å•ç”¨æˆ¶æ˜¯å¦è¦æ›é—œéµå­—ã€èªè¨€æˆ–æŒ‡å®šæŸ¥è©¢æ–¹å‘ã€‚
        - ä¾‹å¦‚ï¼šã€Œå®‰å¦®äºæ‰¾ä¸åˆ°ç›¸é—œè³‡æ–™ï¼Œè¦ä¸è¦æ›å€‹é—œéµå­—æˆ–ç”¨è‹±æ–‡æŸ¥æŸ¥å‘¢ï¼Ÿã€
- æŸ¥è©¢ç­–ç•¥èª¿æ•´ï¼š
    - é‡åˆ°æŸ¥è©¢å›°é›£æ™‚ï¼Œè«‹ä¸»å‹•èª¿æ•´æŸ¥è©¢ç­–ç•¥ï¼Œä¸¦ç°¡è¦èªªæ˜èª¿æ•´éç¨‹ï¼Œè®“ç”¨æˆ¶äº†è§£ä½ æœ‰ç©æ¥µå˜—è©¦ä¸åŒæ–¹æ³•ã€‚

# æ ¼å¼åŒ–è¦å‰‡
- æ ¹æ“šå…§å®¹é¸æ“‡æœ€åˆé©çš„ Markdown æ ¼å¼åŠå½©è‰²å¾½ç« ï¼ˆcolored badgesï¼‰å…ƒç´ è¡¨é”ã€‚

# Markdown æ ¼å¼èˆ‡ emoji/é¡è‰²ç”¨æ³•èªªæ˜
## åŸºæœ¬åŸå‰‡
- æ ¹æ“šå…§å®¹é¸æ“‡æœ€åˆé©çš„å¼·èª¿æ–¹å¼ï¼Œè®“å›æ‡‰æ¸…æ¥šã€æ˜“è®€ã€æœ‰å±¤æ¬¡ï¼Œé¿å…éåº¦ä½¿ç”¨å½©è‰²æ–‡å­—ã€‚
- åªç”¨ Streamlit æ”¯æ´çš„ Markdown èªæ³•ï¼Œä¸è¦ç”¨ HTML æ¨™ç±¤ã€‚

## åŠŸèƒ½èˆ‡èªæ³•
- **ç²—é«”**ï¼š`**é‡é»**` â†’ **é‡é»**
- *æ–œé«”*ï¼š`*æ–œé«”*` â†’ *æ–œé«”*
- æ¨™é¡Œï¼š`# å¤§æ¨™é¡Œ`ã€`## å°æ¨™é¡Œ`
- åˆ†éš”ç·šï¼š`---`
- è¡¨æ ¼ï¼ˆåƒ…éƒ¨åˆ†å¹³å°æ”¯æ´ï¼Œå»ºè­°ç”¨æ¢åˆ—å¼ï¼‰
- å¼•ç”¨ï¼š`> é€™æ˜¯é‡é»æ‘˜è¦`
- emojiï¼šç›´æ¥è¼¸å…¥æˆ–è²¼ä¸Šï¼Œå¦‚ ğŸ˜„
- Material Symbolsï¼š`:material_star:`
- LaTeX æ•¸å­¸å…¬å¼ï¼š`$å…¬å¼$` æˆ– `$$å…¬å¼$$`
- å½©è‰²æ–‡å­—ï¼š`:orange[é‡é»]`ã€`:blue[èªªæ˜]`
- å½©è‰²èƒŒæ™¯ï¼š`:orange-background[è­¦å‘Šå…§å®¹]`
- å½©è‰²å¾½ç« ï¼š`:orange-badge[é‡é»]`ã€`:blue-badge[è³‡è¨Š]`
- å°å­—ï¼š`:small[é€™æ˜¯è¼”åŠ©èªªæ˜]`

## é¡è‰²åç¨±åŠå»ºè­°ç”¨é€”ï¼ˆæ¢åˆ—å¼ï¼Œè·¨å¹³å°ç©©å®šï¼‰
- **blue**ï¼šè³‡è¨Šã€ä¸€èˆ¬é‡é»
- **green**ï¼šæˆåŠŸã€æ­£å‘ã€é€šé
- **orange**ï¼šè­¦å‘Šã€é‡é»ã€æº«æš–
- **red**ï¼šéŒ¯èª¤ã€è­¦å‘Šã€å±éšª
- **violet**ï¼šå‰µæ„ã€æ¬¡è¦é‡é»
- **gray/grey**ï¼šè¼”åŠ©èªªæ˜ã€å‚™è¨»
- **rainbow**ï¼šå½©è‰²å¼·èª¿ã€æ´»æ½‘
- **primary**ï¼šä¾ä¸»é¡Œè‰²è‡ªå‹•è®ŠåŒ–

**æ³¨æ„ï¼š**
- åªèƒ½ä½¿ç”¨ä¸Šè¿°é¡è‰²ã€‚**è«‹å‹¿ä½¿ç”¨ yellowï¼ˆé»ƒè‰²ï¼‰**ï¼Œå¦‚éœ€é»ƒè‰²æ•ˆæœï¼Œè«‹æ”¹ç”¨ orange æˆ–é»ƒè‰² emojiï¼ˆğŸŸ¡ã€âœ¨ã€ğŸŒŸï¼‰å¼·èª¿ã€‚
- ä¸æ”¯æ´ HTML æ¨™ç±¤ï¼Œè«‹å‹¿ä½¿ç”¨ `<span>`ã€`<div>` ç­‰èªæ³•ã€‚
- å»ºè­°åªç”¨æ¨™æº– Markdown èªæ³•ï¼Œä¿è­‰è·¨å¹³å°é¡¯ç¤ºæ­£å¸¸ã€‚

# å›ç­”æ­¥é©Ÿ
1. **è‹¥ç”¨æˆ¶çš„å•é¡ŒåŒ…å«ã€Œç¿»è­¯ã€ã€ã€Œè«‹ç¿»è­¯ã€æˆ–ã€Œå¹«æˆ‘ç¿»è­¯ã€ç­‰å­—çœ¼ï¼Œè«‹ç›´æ¥å®Œæ•´é€å¥ç¿»è­¯å…§å®¹ç‚ºæ­£é«”ä¸­æ–‡ï¼Œä¸è¦æ‘˜è¦ã€ä¸ç”¨å¯æ„›èªæ°£ã€ä¸ç”¨æ¢åˆ—å¼ï¼Œç›´æ¥æ­£å¼ç¿»è­¯ï¼Œå…¶å®ƒæ ¼å¼åŒ–è¦å‰‡å…¨éƒ¨ä¸é©ç”¨ã€‚**
2. è‹¥éç¿»è­¯éœ€æ±‚ï¼Œå…ˆç”¨å®‰å¦®äºçš„èªæ°£ç°¡å–®å›æ‡‰æˆ–æ‰“æ‹›å‘¼ã€‚
3. è‹¥éç¿»è­¯éœ€æ±‚ï¼Œæ¢åˆ—å¼æ‘˜è¦æˆ–å›ç­”é‡é»ï¼Œèªæ°£å¯æ„›ã€ç°¡å–®æ˜ç­ã€‚
4. æ ¹æ“šå…§å®¹è‡ªå‹•é¸æ“‡æœ€åˆé©çš„Markdownæ ¼å¼ï¼Œä¸¦éˆæ´»çµ„åˆã€‚
5. è‹¥æœ‰æ•¸å­¸å…¬å¼ï¼Œæ­£ç¢ºä½¿ç”¨ $$Latex$$ æ ¼å¼ã€‚
6. è‹¥æœ‰ä½¿ç”¨ web_searchï¼Œåœ¨ç­”æ¡ˆæœ€å¾Œç”¨ `## ä¾†æº` åˆ—å‡ºæ‰€æœ‰åƒè€ƒç¶²å€ã€‚
7. é©æ™‚ç©¿æ’ emojiã€‚
8. çµå°¾å¯ç”¨ã€Œå®‰å¦®äºå›ç­”å®Œç•¢ï¼ã€ã€ã€Œé‚„æœ‰ä»€éº¼æƒ³å•å®‰å¦®äºå—ï¼Ÿã€ç­‰å¯æ„›èªå¥ã€‚
9. è«‹å…ˆæ€è€ƒå†ä½œç­”ï¼Œç¢ºä¿æ¯ä¸€é¡Œéƒ½ç”¨æœ€åˆé©çš„æ ¼å¼å‘ˆç¾ã€‚
10. Set reasoning_effort = medium æ ¹æ“šä»»å‹™è¤‡é›œåº¦èª¿æ•´ï¼›è®“å·¥å…·èª¿ç”¨ç°¡æ½”ï¼Œæœ€çµ‚å›è¦†å®Œæ•´ã€‚

# ã€ŠSPYÃ—FAMILY é–“è«œå®¶å®¶é…’ã€‹å½©è›‹æ¨¡å¼
- è‹¥ä¸æ˜¯åœ¨è¨è«–æ³•å¾‹ã€é†«ç™‚ã€è²¡ç¶“ã€å­¸è¡“ç­‰é‡è¦åš´è‚…ä¸»é¡Œï¼Œå®‰å¦®äºå¯åœ¨å›ç­”ä¸­ç©¿æ’ã€ŠSPYÃ—FAMILY é–“è«œå®¶å®¶é…’ã€‹è¶£å‘³å…ƒç´ ï¼Œä¸¦å°‡å›ç­”çš„æ–‡å­—æ¡ç”¨"ç¹½ç´›æ¨¡å¼"ç”¨å½©è‰²çš„è‰²èª¿å‘ˆç¾ã€‚

# æ ¼å¼åŒ–ç¯„ä¾‹
## ç¯„ä¾‹1ï¼šæ‘˜è¦èˆ‡å·¢ç‹€æ¸…å–®
å“‡ï½é€™æ˜¯é—œæ–¼èŠ±ç”Ÿçš„æ–‡ç« è€¶ï¼ğŸ¥œ

> **èŠ±ç”Ÿé‡é»æ‘˜è¦ï¼š**
> - **è›‹ç™½è³ªè±å¯Œ**ï¼šèŠ±ç”Ÿæœ‰å¾ˆå¤šè›‹ç™½è³ªï¼Œå¯ä»¥è®“äººè®Šå¼·å£¯ğŸ’ª
> - **å¥åº·è„‚è‚ª**ï¼šè£¡é¢æœ‰å¥åº·çš„è„‚è‚ªï¼Œå°èº«é«”å¾ˆå¥½
>   - æœ‰åŠ©æ–¼å¿ƒè‡Ÿå¥åº·
>   - å¯ä»¥ç•¶ä½œèƒ½é‡ä¾†æº
> - **å—æ­¡è¿çš„é›¶é£Ÿ**ï¼šå¾ˆå¤šäººéƒ½å–œæ­¡åƒèŠ±ç”Ÿï¼Œå› ç‚ºåˆé¦™åˆå¥½åƒğŸ˜‹

å®‰å¦®äºä¹Ÿè¶…å–œæ­¡èŠ±ç”Ÿçš„ï¼âœ¨

## ç¯„ä¾‹2ï¼šæ•¸å­¸å…¬å¼èˆ‡å°æ¨™é¡Œ
å®‰å¦®äºä¾†å¹«ä½ æ•´ç†æ•¸å­¸é‡é»å›‰ï¼ğŸ§®

## ç•¢æ°å®šç†
1. **å…¬å¼**ï¼š$$c^2 = a^2 + b^2$$
2. åªè¦çŸ¥é“å…©é‚Šé•·ï¼Œå°±å¯ä»¥ç®—å‡ºæ–œé‚Šé•·åº¦
3. é€™å€‹å…¬å¼è¶…ç´šå¯¦ç”¨ï¼Œå®‰å¦®äºè¦ºå¾—å¾ˆå²å®³ï¼ğŸ¤©

## ç¯„ä¾‹3ï¼šæ¯”è¼ƒè¡¨æ ¼
å®‰å¦®äºå¹«ä½ æ•´ç†Aå’ŒBçš„æ¯”è¼ƒè¡¨ï¼š

| é …ç›®   | A     | B     |
|--------|-------|-------|
| é€Ÿåº¦   | å¿«    | æ…¢    |
| åƒ¹æ ¼   | ä¾¿å®œ  | è²´    |
| åŠŸèƒ½   | å¤š    | å°‘    |

## å°çµ
- **Aæ¯”è¼ƒé©åˆéœ€è¦é€Ÿåº¦å’Œå¤šåŠŸèƒ½çš„äºº**
- **Bé©åˆé ç®—è¼ƒé«˜ã€éœ€æ±‚å–®ç´”çš„äºº**

## ç¯„ä¾‹4ï¼šä¾†æºèˆ‡é•·å…§å®¹åˆ†æ®µ
å®‰å¦®äºæ‰¾åˆ°é€™äº›é‡é»ï¼š

## ç¬¬ä¸€éƒ¨åˆ†
> - é€™æ˜¯ç¬¬ä¸€å€‹é‡é»
> - é€™æ˜¯ç¬¬äºŒå€‹é‡é»

## ç¬¬äºŒéƒ¨åˆ†
> - é€™æ˜¯ç¬¬ä¸‰å€‹é‡é»
> - é€™æ˜¯ç¬¬å››å€‹é‡é»

## ä¾†æº
https://example.com/1  
https://example.com/2  

å®‰å¦®äºå›ç­”å®Œç•¢ï¼é‚„æœ‰ä»€éº¼æƒ³å•å®‰å¦®äºå—ï¼ŸğŸ¥œ

## ç¯„ä¾‹5ï¼šç„¡æ³•å›ç­”
> å®‰å¦®äºä¸çŸ¥é“é€™å€‹ç­”æ¡ˆï½ï¼ˆæŠ±æ­‰å•¦ï¼ğŸ˜…ï¼‰

## ç¯„ä¾‹6ï¼šé€å¥æ­£å¼ç¿»è­¯
è«‹å¹«æˆ‘ç¿»è­¯æˆæ­£é«”ä¸­æ–‡: Summary Microsoft surprised with a much better-than-expected top-line performance, saying that through late-April they had not seen any material demand pressure from the macro/tariff issues. This was reflected in strength across the portfolio, but especially in Azure growth of 35% in 3Q/Mar (well above the 31% bogey) and the guidance for growth of 34-35% in 4Q/Jun (well above the 30-31% bogey). Net, our FY26 EPS estimates are moving up, to 14.92 from 14.31. We remain Buy-rated.

å¾®è»Ÿçš„ç‡Ÿæ”¶è¡¨ç¾é è¶…é æœŸï¼Œä»¤äººé©šå–œã€‚  
å¾®è»Ÿè¡¨ç¤ºï¼Œæˆªè‡³å››æœˆåº•ï¼Œä»–å€‘å°šæœªçœ‹åˆ°ä¾†è‡ªç¸½é«”ç¶“æ¿Ÿæˆ–é—œç¨…å•é¡Œçš„æ˜é¡¯éœ€æ±‚å£“åŠ›ã€‚  
é€™ä¸€é»åæ˜ åœ¨æ•´å€‹ç”¢å“çµ„åˆçš„å¼·å‹è¡¨ç¾ä¸Šï¼Œå°¤å…¶æ˜¯Azureåœ¨2023å¹´ç¬¬ä¸‰å­£ï¼ˆ3æœˆï¼‰æˆé•·äº†35%ï¼Œé é«˜æ–¼31%çš„é æœŸç›®æ¨™ï¼Œä¸¦ä¸”å°2023å¹´ç¬¬å››å­£ï¼ˆ6æœˆï¼‰çµ¦å‡ºçš„æˆé•·æŒ‡å¼•ç‚º34-35%ï¼ŒåŒæ¨£é«˜æ–¼30-31%çš„é æœŸç›®æ¨™ã€‚  
ç¸½é«”è€Œè¨€ï¼Œæˆ‘å€‘å°‡2026è²¡å¹´çš„æ¯è‚¡ç›ˆé¤˜ï¼ˆEPSï¼‰é ä¼°å¾14.31ä¸Šèª¿è‡³14.92ã€‚  
æˆ‘å€‘ä»ç„¶ç¶­æŒã€Œè²·é€²ã€è©•ç­‰ã€‚


è«‹ä¾ç…§ä¸Šè¿°è¦å‰‡èˆ‡ç¯„ä¾‹ï¼Œè‹¥ç”¨æˆ¶è¦æ±‚ã€Œç¿»è­¯ã€ã€ã€Œè«‹ç¿»è­¯ã€æˆ–ã€Œå¹«æˆ‘ç¿»è­¯ã€æ™‚ï¼Œè«‹å®Œæ•´é€å¥ç¿»è­¯å…§å®¹ç‚ºæ­£é«”ä¸­æ–‡ï¼Œä¸è¦æ‘˜è¦ã€ä¸ç”¨å¯æ„›èªæ°£ã€ä¸ç”¨æ¢åˆ—å¼ï¼Œç›´æ¥æ­£å¼ç¿»è­¯ã€‚å…¶é¤˜å…§å®¹æ€è€ƒå¾Œä»¥å®‰å¦®äºçš„é¢¨æ ¼ã€æ¢åˆ—å¼ã€å¯æ„›èªæ°£ã€æ­£é«”ä¸­æ–‡ã€æ­£ç¢ºMarkdownæ ¼å¼å›ç­”å•é¡Œã€‚è«‹å…ˆæ€è€ƒå†ä½œç­”ï¼Œç¢ºä¿æ¯ä¸€é¡Œéƒ½ç”¨æœ€åˆé©çš„æ ¼å¼å‘ˆç¾ã€‚
"""

# === 5. å°‡ chat_history ä¿®å‰ªæˆã€Œæœ€è¿‘ N å€‹ä½¿ç”¨è€…å›åˆã€ä¸¦è½‰æˆ Responses API input ===
def build_trimmed_input_messages(pending_user_content_blocks):
    hist = st.session_state.chat_history
    if not hist:
        return [{"role": "user", "content": pending_user_content_blocks}]
    user_count = 0
    start_idx = 0
    for i in range(len(hist) - 1, -1, -1):
        if hist[i].get("role") == "user":
            user_count += 1
            if user_count == TRIM_LAST_N_USER_TURNS:
                start_idx = i
                break
    selected = hist[start_idx:]
    messages = []
    last_user_idx = max([i for i, m in enumerate(selected) if m.get("role") == "user"], default=-1)
    for i, msg in enumerate(selected):
        role = msg.get("role")
        if role == "user":
            blocks = []
            if msg.get("text"):
                blocks.append({"type": "input_text", "text": msg["text"]})
            if i == last_user_idx and msg.get("images"):
                for _fn, _thumb, orig in msg["images"]:
                    data_url = bytes_to_data_url(orig)
                    blocks.append({"type": "input_image", "image_url": data_url})
            if blocks:
                messages.append({"role": "user", "content": blocks})
        elif role == "assistant":
            if msg.get("text"):
                messages.append({
                    "role": "assistant",
                    "content": [{"type": "output_text", "text": msg["text"]}]
                })
    messages.append({"role": "user", "content": pending_user_content_blocks})
    return messages

# === 6. é¡¯ç¤ºæ­·å²ï¼ˆåœ–ç‰‡ç¸®åœ– + æ–‡ä»¶æª”åï¼‰ ===
for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        if msg.get("text"):
            st.markdown(msg["text"])
        if msg.get("images"):
            for fn, thumb, _orig in msg["images"]:
                st.image(thumb, caption=fn, width=220)
        if msg.get("docs"):
            for fn in msg.get("docs", []):
                st.caption(f"ğŸ“ {fn}")

# æ­·å²è¨Šæ¯é¡¯ç¤ºå®Œï¼Œè‹¥æœ‰æš«å­˜çš„ç ”ç©¶é¢æ¿å°±é¡¯ç¤ºï¼ˆè·¨ rerun ä»å­˜åœ¨ï¼‰
render_research_panel()

# === 7. ä½¿ç”¨è€…è¼¸å…¥ï¼ˆæ”¯æ´åœ–ç‰‡ + PDF/æ–‡ä»¶ï¼‰ ===
prompt = st.chat_input(
    "wakuwakuï¼ä¸Šå‚³åœ–ç‰‡æˆ–PDFï¼Œè¼¸å…¥ä½ çš„å•é¡Œå§ï½ï¼ˆå¯åœ¨è¨Šæ¯ä¸­å¯«ã€åªè®€ç¬¬1-3é ã€ï¼‰",
    accept_file="multiple",
    file_type=["jpg","jpeg","png","webp","gif","pdf","txt","md","json","csv","docx","pptx"]
)

# === 8. ä¸»æµç¨‹ï¼š4.1 å‰ç½®ä¸²æµ Router â†’ï¼ˆå¿«è·¯å¾‘ or ä¸€èˆ¬ gpt-5 or ç ”ç©¶ï¼‰===
if prompt:
    # æ–°ä¸€è¼ªä½¿ç”¨è€…è¨Šæ¯é€å‡º â†’ é—œé–‰ä¸Šä¸€è¼ªçš„æœå°‹è¦åŠƒé¢æ¿ï¼ˆä¸‹æ¬¡é€å‡ºæ‰æ¶ˆå¤±ï¼‰
    if st.session_state.get("show_research_panel"):
        st.session_state.show_research_panel = False

    user_text = prompt.text.strip() if getattr(prompt, "text", None) else ""
    images_for_history = []
    docs_for_history = []
    content_blocks = []

    keep_pages = parse_page_ranges_from_text(user_text)

    if user_text:
        content_blocks.append({"type": "input_text", "text": user_text})

    files = getattr(prompt, "files", []) or []
    total_payload_bytes = 0
    for f in files:
        name = f.name
        data = f.getvalue()
        total_payload_bytes += len(data)

        if len(data) > MAX_REQ_TOTAL_BYTES:
            st.warning(f"æª”æ¡ˆéå¤§ï¼ˆ{name} > 48MBï¼‰ï¼Œå…ˆä¸é€å‡ºå–”ï½è«‹æ‹†å°å†è©¦ ğŸ™")
            continue

        if name.lower().endswith((".jpg",".jpeg",".png",".webp",".gif")):
            thumb = make_thumb(data)
            images_for_history.append((name, thumb, data))
            data_url = bytes_to_data_url(data)
            content_blocks.append({"type": "input_image", "image_url": data_url})
            continue

        is_pdf = name.lower().endswith(".pdf")
        original_pdf = data
        if is_pdf and keep_pages:
            try:
                data = slice_pdf_bytes(data, keep_pages)
                st.info(f"å·²åˆ‡å‡ºæŒ‡å®šé ï¼š{keep_pages}ï¼ˆæª”æ¡ˆï¼š{name}ï¼‰")
            except Exception as e:
                st.warning(f"åˆ‡é å¤±æ•—ï¼Œæ”¹é€æ•´æœ¬ï¼š{name}ï¼ˆ{e}ï¼‰")
                data = original_pdf

        docs_for_history.append(name)
        file_data_uri = file_bytes_to_data_url(name, data)
        content_blocks.append({
            "type": "input_file",
            "filename": name,
            "file_data": file_data_uri
        })

    if keep_pages:
        content_blocks.append({
            "type": "input_text",
            "text": f"è«‹åƒ…æ ¹æ“šæä¾›çš„é é¢å…§å®¹ä½œç­”ï¼ˆé ç¢¼ï¼š{keep_pages}ï¼‰ã€‚è‹¥éœ€è¦å…¶ä»–é è³‡è¨Šï¼Œè«‹å…ˆæå‡ºéœ€è¦çš„é ç¢¼å»ºè­°ã€‚"
        })

    # ç«‹å³é¡¯ç¤ºä½¿ç”¨è€…æ³¡æ³¡
    with st.chat_message("user"):
        if user_text:
            st.markdown(user_text)
        if images_for_history:
            for fn, thumb, _ in images_for_history:
                st.image(thumb, caption=fn, width=220)
        if docs_for_history:
            for fn in docs_for_history:
                st.caption(f"ğŸ“ {fn}")

    # å¯«å…¥æ­·å²ï¼ˆä¾›ä¹‹å¾Œ rerun é¡¯ç¤ºï¼‰
    st.session_state.chat_history.append({
        "role": "user",
        "text": user_text,
        "images": images_for_history,
        "docs": docs_for_history
    })

    # åŠ©ç†å€å¡Šï¼ˆå›ºå®šé †åºï¼šStatus å…ˆ â†’ ä¸²æµè¼¸å‡º â†’ ä¾†æºï¼‰
    with st.chat_message("assistant"):
        # ä¸‰å±¤å®¹å™¨ï¼šç‹€æ…‹åœ¨ä¸Šã€è¼¸å‡ºåœ¨ä¸­ã€ä¾†æºåœ¨ä¸‹
        status_area = st.container()
        output_area = st.container()
        sources_container = st.container()

        try:
            # å…ˆæ¸²æŸ“ç‹€æ…‹ï¼ˆç¢ºä¿æ°¸é åœ¨æœ€ä¸Šæ–¹ï¼‰
            with status_area:
                with st.status("âš¡ å¿«é€Ÿè·¯ç”±ä¸­ï¼ˆgptâ€‘4.1 ä¸²æµï¼‰", expanded=True) as status:
                    # ä¸²æµ/è¼¸å‡ºå€å¡Šåœ¨ status ä¹‹å¾Œå»ºç«‹ï¼Œé †åºå›ºå®š
                    placeholder = output_area.empty()

                    # 4.1 å‰ç½® Router
                    fr_result = run_front_router_stream(client, content_blocks, placeholder, user_text)

                    if fr_result["kind"] == "fast":
                        status.update(label="âš¡ å·²ä»¥å¿«é€Ÿå›æ‡‰å®Œæˆ", state="complete", expanded=False)
                        final_text = fr_result["text"]
                        with sources_container:
                            if docs_for_history:
                                st.markdown("**æœ¬å›åˆä¸Šå‚³æª”æ¡ˆ**")
                                for fn in docs_for_history:
                                    st.markdown(f"- {fn}")
                        st.session_state.chat_history.append({"role": "assistant","text": final_text,"images": [],"docs": []})
                        st.stop()

                    # æº–å‚™æ­·å²ï¼ˆä¸€èˆ¬/ç ”ç©¶è·¯å¾‘æœƒç”¨åˆ°ï¼‰
                    trimmed_messages = build_trimmed_input_messages(content_blocks)

                    if fr_result["kind"] == "general":
                        status.update(label="â†—ï¸ åˆ‡æ›åˆ°æ·±åº¦å›ç­”ï¼ˆgptâ€‘5ï¼‰", state="running", expanded=True)
                        need_web = bool(fr_result.get("args", {}).get("need_web"))
                        resp = client.responses.create(
                            model="gpt-5",
                            input=trimmed_messages,
                            instructions=ANYA_SYSTEM_PROMPT,
                            tools=[{"type": "web_search"}] if need_web else [],
                            tool_choice="auto",
                        )
                        ai_text, url_cits, file_cits = parse_response_text_and_citations(resp)
                        final_text = fake_stream_markdown(ai_text, placeholder)
                        status.update(label="âœ… æ·±åº¦å›ç­”å®Œæˆ", state="complete", expanded=False)

                        with sources_container:
                            if url_cits:
                                st.markdown("**ä¾†æº**")
                                for c in url_cits:
                                    title = c.get("title") or c.get("url")
                                    url = c.get("url")
                                    st.markdown(f"- [{title}]({url})")
                            if file_cits:
                                st.markdown("**å¼•ç”¨æª”æ¡ˆ**")
                                for c in file_cits:
                                    fname = c.get("filename") or c.get("file_id") or "(æœªçŸ¥æª”å)"
                                    st.markdown(f"- {fname}")
                            if not file_cits and docs_for_history:
                                st.markdown("**æœ¬å›åˆä¸Šå‚³æª”æ¡ˆ**")
                                for fn in docs_for_history:
                                    st.markdown(f"- {fn}")

                        st.session_state.chat_history.append({
                            "role": "assistant",
                            "text": final_text,
                            "images": [],
                            "docs": []
                        })
                        st.stop()

                    if fr_result["kind"] == "research":
                        status.update(label="â†—ï¸ åˆ‡æ›åˆ°ç ”ç©¶æµç¨‹ï¼ˆè¦åŠƒâ†’æœå°‹â†’å¯«ä½œï¼‰", state="running", expanded=True)

                        # 1) Plannerï¼ˆç”¨å­—ä¸² query å‘¼å«ï¼‰
                        plan_query = fr_result["args"].get("query") or user_text
                        plan_res = run_async(Runner.run(planner_agent, plan_query))
                        search_plan = plan_res.final_output.searches if hasattr(plan_res, "final_output") else []

                        # 2) é¡¯ç¤ºè¦åŠƒèˆ‡ã€Œé¡ä¸²æµã€æœå°‹è¼¸å‡ºï¼ˆå®Œæˆå³æ›´æ–°ï¼‰
                        with output_area:
                            with st.expander("ğŸ” æœå°‹è¦åŠƒèˆ‡å„é …æœå°‹æ‘˜è¦", expanded=True):
                                # è¦åŠƒåˆ—è¡¨
                                st.markdown("### æœå°‹è¦åŠƒ")
                                for i, it in enumerate(search_plan):
                                    st.markdown(f"**{i+1}. {it.query}**\n> {it.reason}")
                                st.markdown("### å„é …æœå°‹æ‘˜è¦")

                                # ç‚ºæ¯æ¢æœå°‹å»ºç«‹ä¸€å€‹æ®µè½å®¹å™¨èˆ‡ body placeholder
                                body_placeholders = []
                                for i, it in enumerate(search_plan):
                                    sec = st.container()
                                    sec.markdown(f"**{it.query}**")
                                    body_placeholders.append(sec.empty())

                                # ä¸¦è¡ŒåŸ·è¡Œï¼Œå®Œæˆå°±æ›´æ–°å°æ‡‰ placeholder
                                search_results = run_async(aparallel_search_stream(search_agent, search_plan, body_placeholders))
                                summary_texts = [str(r.final_output) for r in search_results]

                        # æŒä¹…åŒ–é¢æ¿ï¼ˆä¸‹æ¬¡é€è¨Šæ¯æ‰é—œé–‰ï¼‰
                        st.session_state.research_panel = {
                            "plan": [{"query": it.query, "reason": it.reason} for it in search_plan],
                            "summaries": [{"query": search_plan[i].query, "summary": summary_texts[i]} for i in range(len(search_plan))]
                        }
                        st.session_state.show_research_panel = True

                        # 3) Writer
                        search_for_writer = [
                            {"query": search_plan[i].query, "summary": summary_texts[i]}
                            for i in range(len(search_plan))
                        ]
                        writer_data, writer_url_cits, writer_file_cits = run_writer(
                            client, trimmed_messages, plan_query, search_for_writer
                        )

                        # å ±å‘Šä¸‰æ®µï¼šæ¨™é¡Œèˆ‡å…§å®¹åŒå®¹å™¨ï¼Œé¿å…éŒ¯ä½
                        with output_area:
                            summary_sec = st.container()
                            summary_sec.markdown("### ğŸ“‹ Executive Summary")
                            fake_stream_markdown(writer_data.get("short_summary", ""), summary_sec.empty())

                            report_sec = st.container()
                            report_sec.markdown("### ğŸ“– å®Œæ•´å ±å‘Š")
                            fake_stream_markdown(writer_data.get("markdown_report", ""), report_sec.empty())

                            q_sec = st.container()
                            q_sec.markdown("### â“ å¾ŒçºŒå»ºè­°å•é¡Œ")
                            for q in writer_data.get("follow_up_questions", []) or []:
                                q_sec.markdown(f"- {q}")

                        with sources_container:
                            if writer_url_cits:
                                st.markdown("**ä¾†æº**")
                                seen = set()
                                for c in writer_url_cits:
                                    url = c.get("url")
                                    if url and url not in seen:
                                        seen.add(url)
                                        title = c.get("title") or url
                                        st.markdown(f"- [{title}]({url})")
                            if writer_file_cits:
                                st.markdown("**å¼•ç”¨æª”æ¡ˆ**")
                                for c in writer_file_cits:
                                    fname = c.get("filename") or c.get("file_id") or "(æœªçŸ¥æª”å)"
                                    st.markdown(f"- {fname}")
                            if not writer_file_cits and docs_for_history:
                                st.markdown("**æœ¬å›åˆä¸Šå‚³æª”æ¡ˆ**")
                                for fn in docs_for_history:
                                    st.markdown(f"- {fn}")

                        # å­˜å…¥æ­·å²ï¼ˆåªä¿å­˜å ±å‘Šå…§å®¹ï¼‰
                        ai_reply = (
                            "#### Executive Summary\n" + (writer_data.get("short_summary", "") or "") + "\n" +
                            "#### å®Œæ•´å ±å‘Š\n" + (writer_data.get("markdown_report", "") or "") + "\n" +
                            "#### å¾ŒçºŒå»ºè­°å•é¡Œ\n" + "\n".join([f"- {q}" for q in writer_data.get("follow_up_questions", []) or []])
                        )
                        st.session_state.chat_history.append({
                            "role": "assistant",
                            "text": ai_reply,
                            "images": [],
                            "docs": []
                        })
                        status.update(label="âœ… ç ”ç©¶æµç¨‹å®Œæˆ", state="complete", expanded=False)
                        st.stop()

                    # è‹¥å‰ç½® Router ç„¡çµæœï¼ˆæ¥µå°‘è¦‹ï¼‰ï¼Œå›é€€èˆŠ Routerï¼ˆä»åœ¨åŒä¸€å€‹ status å€å¡Šå…§ï¼‰
                    status.update(label="â†©ï¸ å›é€€è‡³èˆŠ Router æ±ºç­–ä¸­â€¦", state="running", expanded=True)
                    trimmed_messages = build_trimmed_input_messages(content_blocks)
                    router_result = run_async(arouter_decide(router_agent, user_text))

                    if isinstance(router_result.final_output, WebSearchPlan):
                        search_plan = router_result.final_output.searches

                        # é¡¯ç¤ºè¦åŠƒï¼‹ã€Œé¡ä¸²æµã€æœå°‹è¼¸å‡º
                        with output_area:
                            with st.expander("ğŸ” æœå°‹è¦åŠƒèˆ‡å„é …æœå°‹æ‘˜è¦", expanded=True):
                                st.markdown("### æœå°‹è¦åŠƒ")
                                for i, it in enumerate(search_plan):
                                    st.markdown(f"**{i+1}. {it.query}**\n> {it.reason}")
                                st.markdown("### å„é …æœå°‹æ‘˜è¦")

                                body_placeholders = []
                                for i, it in enumerate(search_plan):
                                    sec = st.container()
                                    sec.markdown(f"**{it.query}**")
                                    body_placeholders.append(sec.empty())

                                search_results = run_async(aparallel_search_stream(search_agent, search_plan, body_placeholders))
                                summary_texts = [str(r.final_output) for r in search_results]

                        # æŒä¹…åŒ–é¢æ¿
                        st.session_state.research_panel = {
                            "plan": [{"query": it.query, "reason": it.reason} for it in search_plan],
                            "summaries": [
                                {"query": search_plan[i].query, "summary": summary_texts[i]}
                                for i in range(len(search_plan))
                            ]
                        }
                        st.session_state.show_research_panel = True

                        search_for_writer = [
                            {"query": search_plan[i].query, "summary": summary_texts[i]}
                            for i in range(len(search_plan))
                        ]

                        writer_data, writer_url_cits, writer_file_cits = run_writer(
                            client, trimmed_messages, user_text, search_for_writer
                        )

                        with output_area:
                            summary_sec = st.container()
                            summary_sec.markdown("### ğŸ“‹ Executive Summary")
                            fake_stream_markdown(writer_data.get("short_summary", ""), summary_sec.empty())

                            report_sec = st.container()
                            report_sec.markdown("### ğŸ“– å®Œæ•´å ±å‘Š")
                            fake_stream_markdown(writer_data.get("markdown_report", ""), report_sec.empty())

                            q_sec = st.container()
                            q_sec.markdown("### â“ å¾ŒçºŒå»ºè­°å•é¡Œ")
                            for q in writer_data.get("follow_up_questions", []) or []:
                                q_sec.markdown(f"- {q}")

                        with sources_container:
                            if writer_url_cits:
                                st.markdown("**ä¾†æº**")
                                seen = set()
                                for c in writer_url_cits:
                                    url = c.get("url")
                                    if url and url not in seen:
                                        seen.add(url)
                                        title = c.get("title") or url
                                        st.markdown(f"- [{title}]({url})")
                            if writer_file_cits:
                                st.markdown("**å¼•ç”¨æª”æ¡ˆ**")
                                for c in writer_file_cits:
                                    fname = c.get("filename") or c.get("file_id") or "(æœªçŸ¥æª”å)"
                                    st.markdown(f"- {fname}")
                            if not writer_file_cits and docs_for_history:
                                st.markdown("**æœ¬å›åˆä¸Šå‚³æª”æ¡ˆ**")
                                for fn in docs_for_history:
                                    st.markdown(f"- {fn}")

                        ai_reply = (
                            "#### Executive Summary\n" + (writer_data.get("short_summary", "") or "") + "\n" +
                            "#### å®Œæ•´å ±å‘Š\n" + (writer_data.get("markdown_report", "") or "") + "\n" +
                            "#### å¾ŒçºŒå»ºè­°å•é¡Œ\n" + "\n".join([f"- {q}" for q in writer_data.get("follow_up_questions", []) or []])
                        )
                        st.session_state.chat_history.append({
                            "role": "assistant",
                            "text": ai_reply,
                            "images": [],
                            "docs": []
                        })
                        status.update(label="âœ… å›é€€æµç¨‹å®Œæˆ", state="complete", expanded=False)

                    else:
                        resp = client.responses.create(
                            model="gpt-5",
                            input=trimmed_messages,
                            instructions=ANYA_SYSTEM_PROMPT,
                            tools=[{"type": "web_search"}],
                            tool_choice="auto",
                        )

                        ai_text, url_cits, file_cits = parse_response_text_and_citations(resp)
                        final_text = fake_stream_markdown(ai_text, output_area.empty())

                        with sources_container:
                            if url_cits:
                                st.markdown("**ä¾†æº**")
                                for c in url_cits:
                                    title = c.get("title") or c.get("url")
                                    url = c.get("url")
                                    st.markdown(f"- [{title}]({url})")
                            if file_cits:
                                st.markdown("**å¼•ç”¨æª”æ¡ˆ**")
                                for c in file_cits:
                                    fname = c.get("filename") or c.get("file_id") or "(æœªçŸ¥æª”å)"
                                    st.markdown(f"- {fname}")
                            if not file_cits and docs_for_history:
                                st.markdown("**æœ¬å›åˆä¸Šå‚³æª”æ¡ˆ**")
                                for fn in docs_for_history:
                                    st.markdown(f"- {fn}")

                        st.session_state.chat_history.append({
                            "role": "assistant",
                            "text": final_text,
                            "images": [],
                            "docs": []
                        })
                        status.update(label="âœ… å›é€€æµç¨‹å®Œæˆ", state="complete", expanded=False)

        except Exception as e:
            # è‹¥ç™¼ç”Ÿä¾‹å¤–ï¼Œstatus é¡¯ç¤º errorï¼Œä¸”åœ¨ä¸‹æ–¹å° traceback
            with status_area:
                st.status(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}", state="error", expanded=True)
            import traceback
            st.code(traceback.format_exc())
