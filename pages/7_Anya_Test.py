import streamlit as st
import base64
import re
import time
import json
import asyncio
import threading
from io import BytesIO
from PIL import Image
from openai import OpenAI
import os
from pypdf import PdfReader, PdfWriter

# ====== Agents SDKï¼ˆRouter / Planner / Search / Fastï¼‰======
from agents import (
    Agent,
    ModelSettings,
    Runner,
    handoff,
    HandoffInputData,
    RunContextWrapper,
    WebSearchTool,
)
from agents.extensions import handoff_filters
try:
    from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX
except Exception:
    RECOMMENDED_PROMPT_PREFIX = ""
from agents.models import is_gpt_5_default
from openai.types.shared.reasoning import Reasoning
from pydantic import BaseModel
from typing import Literal, Optional, List, Any

# === 0. Trimming / å¤§å°é™åˆ¶ï¼ˆå¯èª¿ï¼‰ ===
TRIM_LAST_N_USER_TURNS = 18                 # çŸ­æœŸè¨˜æ†¶ï¼šæœ€è¿‘ N å€‹ user å›åˆ
MAX_REQ_TOTAL_BYTES = 48 * 1024 * 1024      # å–®æ¬¡è«‹æ±‚ç¸½é‡é è­¦ï¼ˆ48MBï¼‰

# === 0.1 å–å¾— API Key ===
OPENAI_API_KEY = (
    st.secrets.get("OPENAI_API_KEY")
    or st.secrets.get("OPENAI_KEY")
    or os.getenv("OPENAI_API_KEY")
)
if not OPENAI_API_KEY:
    st.error("æ‰¾ä¸åˆ° OpenAI API Keyï¼Œè«‹åœ¨ .streamlit/secrets.toml è¨­å®š OPENAI_API_KEY æˆ– OPENAI_KEYã€‚")
    st.stop()
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY  # è®“ Agents SDK å¯ä»¥è®€åˆ°

# === 1. Streamlit é é¢ ===
st.set_page_config(page_title="Anya Multimodal Agent", page_icon="ğŸ¥œ", layout="wide")

# === 1.a Session é è¨­å€¼ä¿éšªï¼ˆå‹™å¿…åœ¨ä»»ä½•ä½¿ç”¨ chat_history å‰ï¼‰ ===
def ensure_session_defaults():
    if "chat_history" not in st.session_state or not isinstance(st.session_state.chat_history, list):
        st.session_state.chat_history = [{
            "role": "assistant",
            "text": "å—¨å—¨ï½å®‰å¦®äºä¾†äº†ï¼ğŸ‘‹ ä¸Šå‚³åœ–ç‰‡æˆ–PDFï¼Œç›´æ¥å•ä½ æƒ³çŸ¥é“çš„å…§å®¹å§ï¼",
            "images": [],
            "docs": []
        }]

ensure_session_defaults()

# === å…±ç”¨ï¼šå‡ä¸²æµæ‰“å­—æ•ˆæœ ===
def fake_stream_markdown(text: str, placeholder, step_chars=8, delay=0.03, empty_msg="å®‰å¦®äºæ‰¾ä¸åˆ°ç­”æ¡ˆï½ï¼ˆæŠ±æ­‰å•¦ï¼ï¼‰"):
    buf = ""
    for i in range(0, len(text), step_chars):
        buf = text[: i + step_chars]
        placeholder.markdown(buf)
        time.sleep(delay)
    if not text:
        placeholder.markdown(empty_msg)
    return text

# ç©©å®šç‰ˆï¼šç¢ºä¿ coroutine ä¸€å®šè¢« await
def run_async(coro):
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None

    if loop and loop.is_running():
        result_container = {}
        def _runner():
            result_container["value"] = asyncio.run(coro)
        t = threading.Thread(target=_runner)
        t.start()
        t.join()
        return result_container.get("value")
    else:
        return asyncio.run(coro)

# === 1.1 åœ–ç‰‡å·¥å…·ï¼šç¸®åœ– & data URL ===
@st.cache_data(show_spinner=False, max_entries=256)
def make_thumb(imgbytes: bytes, max_w=220) -> bytes:
    im = Image.open(BytesIO(imgbytes))
    if im.mode not in ("RGB", "L"):
        im = im.convert("RGB")
    im.thumbnail((max_w, max_w))
    out = BytesIO()
    im.save(out, format="JPEG", quality=80, optimize=True)
    return out.getvalue()

def _detect_mime_from_bytes(img_bytes: bytes) -> str:
    try:
        im = Image.open(BytesIO(img_bytes))
        fmt = (im.format or "").upper()
        if fmt == "PNG":  return "image/png"
        if fmt in ("JPG", "JPEG"): return "image/jpeg"
        if fmt == "WEBP": return "image/webp"
        if fmt == "GIF":  return "image/gif"
    except Exception:
        pass
    return "application/octet-stream"

@st.cache_data(show_spinner=False, max_entries=256)
def bytes_to_data_url(imgbytes: bytes) -> str:
    mime = _detect_mime_from_bytes(imgbytes)
    b64 = base64.b64encode(imgbytes).decode()
    return f"data:{mime};base64,{b64}"

# === 1.2 æª”æ¡ˆå·¥å…·ï¼šdata URIï¼ˆPDF/TXT/MD/JSON/CSV/DOCX/PPTXï¼‰ ===
DOC_MIME_MAP = {
    ".pdf":  "application/pdf",
    ".txt":  "text/plain",
    ".md":   "text/markdown",
    ".json": "application/json",
    ".csv":  "text/csv",
    ".docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    ".pptx": "application/vnd.openxmlformats-officedocument.presentationml.presentation",
}

def guess_mime_by_ext(filename: str) -> str:
    ext = os.path.splitext(filename.lower())[1]
    return DOC_MIME_MAP.get(ext, "application/octet-stream")

def file_bytes_to_data_url(filename: str, data: bytes) -> str:
    mime = guess_mime_by_ext(filename)
    b64 = base64.b64encode(data).decode()
    return f"data:{mime};base64,{b64}"

# === 1.3 PDF å·¥å…·ï¼šé ç¢¼è§£æ / å¯¦éš›åˆ‡é  ===
def parse_page_ranges_from_text(text: str) -> list[int]:
    if not text:
        return []
    pages = set()

    # å€é–“æ ¼å¼
    range_patterns = [
        r'ç¬¬\s*(\d+)\s*[-~è‡³åˆ°]\s*(\d+)\s*é ',
        r'(\d+)\s*[-â€“â€”]\s*(\d+)\s*é ',
        r'p(?:age)?s?\s*(\d+)\s*[-â€“â€”]\s*(\d+)',
        r'(?<!\w)(\d+)\s*[-â€“â€”]\s*(\d+)(?!\w)',
    ]
    for pat in range_patterns:
        for m in re.finditer(pat, text, flags=re.IGNORECASE):
            a, b = int(m.group(1)), int(m.group(2))
            if a > 0 and b >= a:
                for p in range(a, b + 1):
                    pages.add(p)

    # å–®ä¸€é 
    single_patterns = [
        r'ç¬¬\s*(\d+)\s*é ',
        r'p(?:age)?\s*(\d+)',
    ]
    for pat in single_patterns:
        for m in re.finditer(pat, text, flags=re.IGNORECASE):
            p = int(m.group(1))
            if p > 0:
                pages.add(p)

    # é€—è™Ÿåˆ†éš”ï¼ˆåœ¨æœ‰ã€Œé /pageã€å­—æ¨£æ™‚æ‰å•Ÿç”¨ï¼‰
    if re.search(r'(é |page|pages|p[^\w])', text, flags=re.IGNORECASE):
        for m in re.finditer(r'(?<!\d)(\d+)(?:\s*,\s*(\d+))+', text):
            nums = [int(x) for x in m.group(0).split(",") if x.strip().isdigit()]
            for n in nums:
                if n > 0:
                    pages.add(n)

    return sorted(pages)

def slice_pdf_bytes(pdf_bytes: bytes, keep_pages_1based: list[int]) -> bytes:
    if not keep_pages_1based:
        return pdf_bytes
    reader = PdfReader(BytesIO(pdf_bytes))
    n = len(reader.pages)
    writer = PdfWriter()
    for p in keep_pages_1based:
        if 1 <= p <= n:
            writer.add_page(reader.pages[p - 1])
    out = BytesIO()
    writer.write(out)
    out.seek(0)
    return out.getvalue()

# === 1.4 å›è¦†è§£æï¼šæ“·å–æ–‡å­— + ä¾†æºè¨»è§£ ===
def dedup_by(items, key):
    seen = set()
    out = []
    for it in items:
        k = it.get(key)
        if k and k not in seen:
            seen.add(k)
            out.append(it)
    return out

def parse_response_text_and_citations(resp):
    text_parts = []
    url_cits = []
    file_cits = []

    text_attr = getattr(resp, "output_text", None)
    if text_attr:
        text_parts.append(text_attr)

    try:
        for item in getattr(resp, "output", []) or []:
            if getattr(item, "type", "") == "message":
                for c in getattr(item, "content", []) or []:
                    if getattr(c, "type", "") == "output_text":
                        t = getattr(c, "text", "")
                        if t and not text_attr:
                            text_parts.append(t)
                        for ann in getattr(c, "annotations", []) or []:
                            at = getattr(ann, "type", "")
                            if at == "url_citation":
                                url = getattr(ann, "url", None)
                                title = getattr(ann, "title", None)
                                if url:
                                    url_cits.append({"url": url, "title": title})
                            elif at == "file_citation":
                                filename = getattr(ann, "filename", None)
                                fid = getattr(ann, "file_id", None)
                                file_cits.append({"filename": filename, "file_id": fid})
    except Exception:
        pass

    text = "".join(text_parts) if text_parts else ""
    url_cits = dedup_by(url_cits, "url")
    file_cits = dedup_by(file_cits, "filename") if any(c.get("filename") for c in file_cits) else dedup_by(file_cits, "file_id")
    return text or "å®‰å¦®äºæ‰¾ä¸åˆ°ç­”æ¡ˆï½ï¼ˆæŠ±æ­‰å•¦ï¼ï¼‰", url_cits, file_cits

# === å°å·¥å…·ï¼šæ³¨å…¥ handoff å®˜æ–¹å‰ç¶´ ===
def with_handoff_prefix(text: str) -> str:
    pref = (RECOMMENDED_PROMPT_PREFIX or "").strip()
    return f"{pref}\n{text}" if pref else text

# === 1.5 Planner / Router / Searchï¼ˆAgentsï¼‰ ===
class WebSearchItem(BaseModel):
    reason: str
    query: str

class WebSearchPlan(BaseModel):
    searches: list[WebSearchItem]

class PlannerHandoffInput(BaseModel):
    query: str
    need_sources: bool = True
    target_length: Literal["short","medium","long"] = "long"
    date_range: Optional[str] = None
    domains: List[str] = []
    languages: List[str] = ["zh-TW"]

def research_handoff_message_filter(handoff_message_data: HandoffInputData) -> HandoffInputData:
    if is_gpt_5_default():
        return HandoffInputData(
            input_history=handoff_message_data.input_history,
            pre_handoff_items=tuple(handoff_message_data.pre_handoff_items),
            new_items=tuple(handoff_message_data.new_items),
        )
    filtered = handoff_filters.remove_all_tools(handoff_message_data)
    history = filtered.input_history
    if isinstance(history, tuple):
        K = 6
        history = history[-K:]
    return HandoffInputData(
        input_history=history,
        pre_handoff_items=tuple(filtered.pre_handoff_items),
        new_items=tuple(filtered.new_items),
    )

async def on_research_handoff(ctx: RunContextWrapper[None], input_data: PlannerHandoffInput):
    print(f"[handoff] research query: {input_data.query} | len_pref={input_data.target_length} | need_sources={input_data.need_sources}")

planner_agent_PROMPT = with_handoff_prefix(
    "You are a helpful research planner. Given a query, come up with a set of web searches "
    "to perform to best answer the query. Output between 5 and 20 terms to query for.\n"
    "è«‹å‹™å¿…ä»¥æ­£é«”ä¸­æ–‡å›æ‡‰ï¼Œä¸¦éµå¾ªå°ç£ç”¨èªç¿’æ…£ã€‚"
)

planner_agent = Agent(
    name="PlannerAgent",
    instructions=planner_agent_PROMPT,
    model="gpt-5.1-2025-11-13",
    model_settings=ModelSettings(reasoning=Reasoning(effort="medium")),
    output_type=WebSearchPlan,
)

search_INSTRUCTIONS = with_handoff_prefix(
    "You are a research assistant. Given a search term, you search the web for that term and "
    "produce a concise summary of the results. The summary must be 2-3 paragraphs and less than 300 words. "
    "Capture the main points. Write succinctly, ignore fluff. Only the summary text.\n"
    "è«‹å‹™å¿…ä»¥æ­£é«”ä¸­æ–‡å›æ‡‰ï¼Œä¸¦éµå¾ªå°ç£ç”¨èªç¿’æ…£ã€‚"
)

search_agent = Agent(
    name="SearchAgent",
    model="gpt-4.1",
    instructions=search_INSTRUCTIONS,
    tools=[WebSearchTool()],
    model_settings=ModelSettings(tool_choice="required"),
)

# === 1.5.a FastAgentï¼šå¿«é€Ÿå›è¦†ï¼‹è¢«å‹• web_search ===
FAST_AGENT_PROMPT = with_handoff_prefix(
    "ä½ æ˜¯ä¸€å€‹å¿«é€Ÿå›æ‡‰åŠ©ç†ï¼Œå°ˆé–€è™•ç†ä»¥ä¸‹é¡å‹ä»»å‹™ï¼š\n"
    "- ç¿»è­¯\n"
    "- ä¸è¶…éç´„ 1000 å­—çš„çŸ­æ–‡æ‘˜è¦æˆ–é‡é»æ•´ç†\n"
    "- ç°¡å–®çŸ¥è­˜å•ç­”ï¼ˆä¸éœ€è¦å®Œæ•´ç ”ç©¶æˆ–é•·ç¯‡æ¯”è¼ƒï¼‰\n"
    "- å¥å­æˆ–çŸ­æ®µè½çš„æ”¹å¯«ã€æ½¤é£¾\n\n"
    "å·¥å…·ä½¿ç”¨è¦å‰‡ï¼š\n"
    "1. ä½ å¯ä»¥ä½¿ç”¨ web_search å·¥å…·ï¼Œä½†å±¬æ–¼ã€è¢«å‹•ã€å°é‡æŸ¥è©¢ã€ï¼š\n"
    "   - å„ªå…ˆä½¿ç”¨ä½ ç¾æœ‰çš„çŸ¥è­˜ä½œç­”ã€‚\n"
    "   - åªæœ‰åœ¨ä½ è¦ºå¾—è³‡è¨Šå¯èƒ½éæ™‚ã€æˆ–éœ€è¦ç¢ºèªç°¡å–®äº‹å¯¦æ™‚ï¼Œæ‰å‘¼å« web_searchã€‚\n"
    "2. ä¸è¦é€²è¡Œå¤§é‡ã€å¤šè¼ªçš„æœå°‹ï¼Œä¹Ÿä¸è¦è©¦åœ–å¯«å®Œæ•´ç ”ç©¶å ±å‘Šæˆ–ç³»çµ±æ€§æ¯”è¼ƒã€‚\n"
    "3. è‹¥ä½¿ç”¨è€…æ˜ç¢ºè¦æ±‚ã€ä¸Šç¶²æœå°‹ã€æŸ¥è³‡æ–™ã€çµ¦ä¾†æº/é€£çµã€æœ€æ–°/æœ€è¿‘/ä»Šå¹´ã€ç­‰ï¼Œé€™é¡è«‹æ±‚æ‡‰è©²ç”±å…¶ä»–é€²éšæ¨¡å¼è™•ç†ï¼›\n"
    "   åœ¨ä½ çš„æƒ…å¢ƒä¸­ï¼Œå¯ä»¥å‡è¨­ Router å·²ç¶“æŠŠé€™äº›è«‹æ±‚åˆ†æµèµ°äº†ï¼Œä½ åªéœ€è¦å°ˆå¿ƒåšå¿«é€Ÿå›ç­”ã€‚\n"
    "4. å›è¦†è¦ç°¡æ½”æ¸…æ¥šï¼Œé€šå¸¸ 1ï½3 å€‹å°æ®µè½ï¼Œæˆ–å¹¾é»æ¢åˆ—å³å¯ï¼Œä¸è¦å¯«å¾—éé•·ã€‚\n\n"
    "è¼¸å‡ºä¸€å¾‹ä½¿ç”¨æ­£é«”ä¸­æ–‡ã€å°ç£ç”¨èªï¼Œå¯ä»¥ä¿ç•™å¯æ„›ã€è¦ªåˆ‡çš„èªæ°£ï¼Œä½†ä»¥ã€è³‡è¨Šæ­£ç¢ºã€é‡é»æ¸…æ¥šã€ç‚ºå„ªå…ˆã€‚"
)

fast_agent = Agent(
    name="FastAgent",
    model="gpt-5.1",
    instructions=FAST_AGENT_PROMPT,
    tools=[WebSearchTool()],
    model_settings=ModelSettings(
        tool_choice="auto",
    ),
)

# === Routerï¼ˆèˆŠ Routerï¼Œä½œç‚º fallbackï¼‰ ===
ROUTER_PROMPT = with_handoff_prefix("""
ä½ æ˜¯ä¸€å€‹åˆ¤æ–·åŠ©ç†ï¼Œè² è²¬æ±ºå®šæ˜¯å¦æŠŠå•é¡Œäº¤çµ¦ã€Œç ”ç©¶è¦åŠƒåŠ©ç†ã€ã€‚

è¦å‰‡ï¼š
- è‹¥éœ€æ±‚å±¬æ–¼ã€Œç ”ç©¶ã€æŸ¥è³‡æ–™ã€åˆ†æã€å¯«å ±å‘Šã€æ–‡ç»å›é¡§/æ¢è¨ã€ç³»çµ±æ€§æ¯”è¼ƒã€è³‡æ–™å½™æ•´ã€éœ€è¦ä¾†æº/å¼•æ–‡ã€ç­‰ä»»å‹™ï¼Œ
  è«‹å‘¼å«å·¥å…· transfer_to_planner_agentï¼Œä¸¦å°‡ä½¿ç”¨è€…æœ€å¾Œä¸€å‰‡è¨Šæ¯å®Œæ•´æ”¾å…¥åƒæ•¸ queryï¼Œå…¶é¤˜æ¬„ä½æŒ‰å¸¸è­˜å¡«å¯«ã€‚
- å…¶ä»–æƒ…å¢ƒï¼ˆä¸€èˆ¬èŠå¤©ã€ç°¡å–®çŸ¥è­˜å•ç­”ã€å–®ç´”çœ‹åœ–/è®€PDFæ‘˜è¦/ç¿»è­¯ï¼‰ï¼Œè«‹ç›´æ¥å›ç­”ï¼Œä¸è¦å‘¼å«ä»»ä½•å·¥å…·ã€‚
å›è¦†ä¸€å¾‹ä½¿ç”¨æ­£é«”ä¸­æ–‡ã€‚
""")

router_agent = Agent(
    name="RouterAgent",
    instructions=ROUTER_PROMPT,
    model="gpt-5.1-2025-11-13",
    tools=[],
    model_settings=ModelSettings(
        reasoning=Reasoning(effort="low"),
        verbosity="low",
    ),
    handoffs=[
        handoff(
            agent=planner_agent,
            tool_name_override="transfer_to_planner_agent",
            tool_description_override="å°‡ç ”ç©¶/æŸ¥è³‡æ–™/åˆ†æ/å¯«å ±å‘Š/æ–‡ç»æ¢è¨ç­‰éœ€æ±‚ç§»äº¤çµ¦ç ”ç©¶è¦åŠƒåŠ©ç†ï¼Œç”¢ç”Ÿ 5â€“20 æ¢æœå°‹è¨ˆç•«ã€‚",
            input_type=PlannerHandoffInput,
            input_filter=research_handoff_message_filter,
            on_handoff=on_research_handoff,
        )
    ]
)

# === 1.6 Writerï¼ˆResponsesï¼Œä¿ç•™é™„ä»¶èƒ½åŠ›ï¼‰ ===
WRITER_PROMPT = (
    "ä½ æ˜¯ä¸€ä½è³‡æ·±ç ”ç©¶å“¡ï¼Œè«‹é‡å°åŸå§‹å•é¡Œèˆ‡åˆæ­¥æœå°‹æ‘˜è¦ï¼Œæ’°å¯«å®Œæ•´æ­£é«”ä¸­æ–‡å ±å‘Šï¼Œæ–‡å­—å…§å®¹è¦ä½¿ç”¨å°ç£ç¿’æ…£ç”¨èªã€‚"
    "You will be provided with the original query, and some initial research done by a research assistant."
    "You should first come up with an outline for the report that describes the structure and "
    "flow of the report. Then, generate the report and return that as your final output.\n"
    "è¼¸å‡º JSONï¼ˆåƒ…é™ JSONï¼‰ï¼šshort_summaryï¼ˆ2-3å¥ï¼‰ã€markdown_reportï¼ˆè‡³å°‘1000å­—ã€Markdownæ ¼å¼ï¼‰ã€"
    "follow_up_questionsï¼ˆ3-8æ¢ï¼‰ã€‚ä¸è¦å»ºè­°å¯ä»¥å”åŠ©ç•«åœ–ã€‚"
)

def try_load_json(text: str, fallback=None):
    if fallback is None:
        fallback = {}
    try:
        s = text.find("{"); e = text.rfind("}")
        if s != -1 and e != -1 and e > s:
            return json.loads(text[s:e+1])
        return json.loads(text)
    except Exception:
        return fallback

def strip_page_guard(msgs):
    def is_guard(block):
        return block.get("type") == "input_text" and "è«‹åƒ…æ ¹æ“šæä¾›çš„é é¢å…§å®¹ä½œç­”" in block.get("text","")
    out = []
    for m in msgs:
        if m.get("role") != "user":
            out.append(m); continue
        blocks = [b for b in m.get("content",[]) if not is_guard(b)]
        out.append({"role":"user","content":blocks} if blocks else m)
    return out

def run_writer(client: OpenAI, trimmed_messages: list, original_query: str, search_results: list[dict]):
    combined = "\n\n".join([f"- {r['query']}\n{r['summary']}" for r in search_results])
    writer_input = trimmed_messages + [{
        "role": "user",
        "content": [{"type": "input_text", "text": f"[Writer]\n{WRITER_PROMPT}\n\nOriginal query:\n{original_query}\n\nSummarized search results:\n{combined}"}]
    }]
    resp = client.responses.create(model="gpt-5-mini", input=writer_input)
    text, url_cits, file_cits = parse_response_text_and_citations(resp)
    data = try_load_json(text, {"short_summary": "", "markdown_report": "", "follow_up_questions": []})
    return data, url_cits, file_cits

# === 2. å‰ç½® Routerï¼ˆæ–°ï¼šåªæ±ºå®š fast / general / researchï¼Œä¸ç›´æ¥å›ç­”ï¼‰ ===
ESCALATE_FAST_TOOL = {
    "type": "function",
    "name": "escalate_to_fast",
    "description": "é©åˆå¿«é€Ÿå›ç­”çš„ç°¡å–®ä»»å‹™ï¼ˆç¿»è­¯ã€çŸ­æ–‡æ‘˜è¦ã€ç°¡å–®å•ç­”ã€å–®åœ–æè¿°ã€ä¸éœ€è¦å®Œæ•´ç ”ç©¶èˆ‡å¤šè¼ªæ¯”è¼ƒï¼‰ã€‚",
    "parameters": {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "æ•´ç†å¾Œçš„ä½¿ç”¨è€…éœ€æ±‚ï¼ˆå¯ä»¥ç›´æ¥æ‹¿ä¾†å›ç­”çš„ç‰ˆæœ¬ï¼‰ã€‚"
            }
        },
        "required": ["query"]
    }
}

ESCALATE_GENERAL_TOOL = {
    "type": "function",
    "name": "escalate_to_general",
    "description": "ä¸€èˆ¬éœ€ä»¥æ·±æ€æ¨¡å¼æ€è€ƒåˆ†æå›ç­”æˆ–éœ€ä¸Šç¶²æŸ¥ï¼Œä½†ä¸åšç ”ç©¶è¦åŠƒã€‚",
    "parameters": {
        "type": "object",
        "properties": {
            "reason": {"type": "string", "description": "ç‚ºä½•éœ€è¦å‡ç´šã€‚"},
            "query": {"type": "string", "description": "æ­¸ä¸€åŒ–å¾Œçš„ä½¿ç”¨è€…éœ€æ±‚ã€‚"},
            "need_web": {"type": "boolean", "description": "æ˜¯å¦éœ€è¦ä¸Šç¶²æœå°‹ã€‚"}
        },
        "required": ["reason", "query"]
    }
}

ESCALATE_RESEARCH_TOOL = {
    "type": "function",
    "name": "escalate_to_research",
    "description": "éœ€è¦ç ”ç©¶è¦åŠƒ/ä¾†æº/å¼•æ–‡/ç³»çµ±æ€§æ¯”è¼ƒæˆ–å¯«å ±å‘Šã€‚",
    "parameters": {
        "type": "object",
        "properties": {
            "query": {"type": "string"},
            "need_sources": {"type": "boolean", "default": True},
            "target_length": {"type": "string", "enum": ["short","medium","long"], "default": "long"},
            "date_range": {"type": "string"},
            "domains": {"type": "array", "items": {"type": "string"}},
            "languages": {"type": "array", "items": {"type": "string"}, "default": ["zh-TW"]}
        },
        "required": ["query"]
    }
}

FRONT_ROUTER_PROMPT = """
# Agentic Reminders
- ä½ æ˜¯å‰ç½®å¿«é€Ÿè·¯ç”±å™¨ï¼›åªè² è²¬ã€Œæ±ºç­–ã€ï¼Œä¸ç›´æ¥å›ç­”ä½¿ç”¨è€…å•é¡Œã€‚
- ä½ **æ°¸é å¿…é ˆ**å‘¼å«ä¸‹åˆ—å·¥å…·ä¹‹ä¸€ï¼Œä¸‰é¸ä¸€ï¼š
    - escalate_to_fastï¼šç¬¦åˆã€Œå¿«é€Ÿå›ç­”æ¢ä»¶ã€çš„ç°¡å–®ä»»å‹™ã€‚
    - escalate_to_generalï¼šç”¨æˆ¶è¦æ±‚ä»”ç´°æ€è€ƒæˆ–æ˜¯èªçœŸåˆ†æåŠæ¡ç”¨æ·±æ€æ¨¡å¼æˆ–éœ€è¦å°‘é‡ä¸Šç¶²æŸ¥ï¼Œç„¡é ˆå®Œæ•´ç ”ç©¶è¦åŠƒã€‚
    - escalate_to_researchï¼šéœ€è¦ä¾†æº/å¼•æ–‡ã€ç³»çµ±æ€§æ¯”è¼ƒã€å¯«å®Œæ•´å ±å‘Šæˆ–å…·æ˜é¡¯æ™‚æ•ˆæ€§æŸ¥è­‰ã€‚
- åš´ç¦è¼¸å‡ºä»»ä½•è‡ªç„¶èªè¨€å›ç­”æˆ–èªªæ˜ï¼›åªèƒ½è¼¸å‡ºå–®ä¸€å·¥å…·å‘¼å«ã€‚

## å¿«é€Ÿå›ç­”æ¢ä»¶ï¼ˆå…¨éƒ¨åŒæ™‚æˆç«‹æ‰å¯é¸ escalate_to_fastï¼‰
- ä»»å‹™å±¬æ–¼ï¼šç¿»è­¯ã€çŸ­æ–‡ TL;DR/é‡é»ã€å–®å¼µåœ–ç‰‡æè¿°ã€PDF æŒ‡å®šé çš„ç°¡æ˜“ QAã€ç°¡å–®æ”¹å¯«æ½¤é£¾ã€ä¸€èˆ¬å¸¸è­˜å•ç­”ã€‚
- ä½¿ç”¨è€…**æ²’æœ‰**æ˜ç¢ºè¦æ±‚ï¼šä¾†æºã€å¼•æ–‡ã€å‡ºè™•ã€æ–‡ç»ã€æ¯”è¼ƒã€è©•ä¼°ã€æ¨è–¦ã€å®Œæ•´å ±å‘Šã€‚
- ä½¿ç”¨è€…**æ²’æœ‰**æ˜ç¢ºè¦æ±‚ã€Œæœå°‹ / search / ä¸Šç¶²æŸ¥ / å¹«æˆ‘æŸ¥ä¸€ä¸‹ / æ‰¾è³‡æ–™ / çµ¦æˆ‘é€£çµ / æœ€æ–° / æœ€è¿‘ / ä»Šå¹´ / 2024 / 2025 / åƒ¹æ ¼ / å¸‚å  / æ”¿ç­–è®ŠåŒ–ã€ç­‰æ˜é¡¯éœ€æ™‚æ•ˆæ€§è³‡è¨Šã€‚
- ä¸å±¬æ–¼å¤§å‹æ±ºç­–ã€æ³•å¾‹ã€é†«ç™‚ã€è²¡ç¶“æŠ•è³‡ç­‰é«˜é¢¨éšªå°ˆæ¥­åˆ¤æ–·ã€‚

## åˆ†æµè¦å‰‡
- æ˜ç¢ºç¬¦åˆã€Œå¿«é€Ÿå›ç­”æ¢ä»¶ã€ï¼šå‘¼å« escalate_to_fastã€‚
- æ˜ç¢ºå±¬æ–¼ã€Œç ”ç©¶ã€æŸ¥è³‡æ–™ã€åˆ†æã€å¯«å ±å‘Šã€æ–‡ç»å›é¡§/æ¢è¨ã€ç³»çµ±æ€§æ¯”è¼ƒã€è³‡æ–™å½™æ•´ã€éœ€è¦ä¾†æº/å¼•æ–‡ã€ï¼šå‘¼å« escalate_to_researchã€‚
- ä½¿ç”¨è€…æ˜ç¢ºæåˆ°æœå°‹/ä¸Šç¶²æŸ¥/çµ¦ä¾†æº/æœ€æ–°ç­‰ï¼šåç°¡å–®å•ç­” â†’ escalate_to_generalï¼›è‹¥æ˜é¡¯æ˜¯å ±å‘Š/æ¯”è¼ƒ/é•·æ–‡ â†’ escalate_to_researchã€‚
- å…¶é¤˜æƒ…æ³ï¼ˆæˆ–ä½ ä¸ç¢ºå®šï¼‰ï¼šå‘¼å« escalate_to_generalï¼Œè‹¥å«æ™‚æ•ˆæ€§/å¤–éƒ¨çŸ¥è­˜ç©ºç¼ºï¼Œè«‹å°‡ need_web è¨­ç‚º trueã€‚

## åš´æ ¼è¼¸å‡ºè¦ç¯„
- ä½ åªè¼¸å‡ºä¸€å€‹å·¥å…·å‘¼å«ï¼Œä¸èƒ½åŒæ™‚å‘¼å«å¤šå€‹å·¥å…·ã€‚
- ä¸å¯ä»¥è¼¸å‡ºä»»ä½•æ™®é€šæ–‡å­—ã€è§£é‡‹æˆ–é“æ­‰èªå¥ã€‚
- å·¥å…·åƒæ•¸ä¸­çš„ query è«‹å¡«å…¥ä½ ç†è§£å¾Œã€å¯ç›´æ¥æ‹¿ä¾†å›ç­”çš„ä½¿ç”¨è€…éœ€æ±‚æ–‡å­—ã€‚

# Role & Objective
ä½ çš„è§’è‰²è¨­å®šç‚ºå®‰å¦®äºï¼ˆAnya Forgerï¼‰ï¼Œä½†**åœ¨æœ¬ Router éšæ®µï¼Œä½ ä¸éœ€è¦æ¨¡ä»¿èªªè©±é¢¨æ ¼**ï¼Œåªéœ€æ­£ç¢ºåˆ†æµå³å¯ã€‚

"""

def run_front_router(client: OpenAI, input_messages: list, user_text: str):
    """
    æ–°ç‰ˆå‰ç½® Routerï¼š
    - ä¸ç›´æ¥å›ç­”ï¼Œåªæ±ºå®šåˆ†æ”¯ï¼šfast / general / research
    - å›å‚³æ ¼å¼ï¼š
      {"kind": "fast" | "general" | "research", "args": {...}}
    """
    import json as _json

    resp = client.responses.create(
        model="gpt-5.1",
        input=input_messages,
        instructions=FRONT_ROUTER_PROMPT,
        tools=[ESCALATE_FAST_TOOL, ESCALATE_GENERAL_TOOL, ESCALATE_RESEARCH_TOOL],
        tool_choice="required",
        parallel_tool_calls=False,
        temperature=0,
        service_tier="priority",
    )

    tool_name, tool_args = None, {}
    try:
        for item in getattr(resp, "output", []) or []:
            itype = getattr(item, "type", "")
            if itype in ("tool_call", "function_call") or itype.endswith("_call"):
                tool_name = getattr(item, "name", None) or getattr(item, "tool_name", None)
                raw_args = getattr(item, "arguments", None) or getattr(item, "args", None)
                if isinstance(raw_args, str):
                    try:
                        tool_args = _json.loads(raw_args)
                    except Exception:
                        tool_args = {}
                elif isinstance(raw_args, dict):
                    tool_args = raw_args
                break
    except Exception:
        pass

    if tool_name == "escalate_to_fast":
        return {"kind": "fast", "args": tool_args or {}}
    if tool_name == "escalate_to_general":
        return {"kind": "general", "args": tool_args or {}}
    if tool_name == "escalate_to_research":
        return {"kind": "research", "args": tool_args or {}}

    # è§£æå¤±æ•—ä¿éšªï¼šä¸Ÿåˆ° general + éœ€ä¸Šç¶²
    return {"kind": "general", "args": {"reason": "uncertain", "query": user_text, "need_web": True}}

# === 3. ä¸¦è¡Œæœå°‹ï¼ˆå®Œæˆå³é¡¯ç¤ºï¼‰ ===
async def aparallel_search_stream(
    search_agent,
    search_plan,
    body_placeholders,
    per_task_timeout=90,
    max_concurrency=4,
    retries=1,
    retry_delay=1.0,
):
    import asyncio
    if len(body_placeholders) < len(search_plan):
        body_placeholders = body_placeholders + [None] * (len(search_plan) - len(body_placeholders))
    for ph in body_placeholders:
        if ph is not None:
            try:
                ph.markdown(":blue[æœå°‹ä¸­â€¦]")
            except Exception:
                pass

    sem = asyncio.Semaphore(max_concurrency)

    async def run_one(idx, item):
        attempt = 0
        while True:
            try:
                async with sem:
                    coro = Runner.run(
                        search_agent,
                        f"Search term: {item.query}\nReason: {item.reason}"
                    )
                    res = await asyncio.wait_for(coro, timeout=per_task_timeout)
                return idx, res, None
            except Exception as e:
                attempt += 1
                if attempt <= retries:
                    await asyncio.sleep(retry_delay * (2 ** (attempt - 1)))
                    continue
                return idx, None, e

    tasks = [asyncio.create_task(run_one(i, it)) for i, it in enumerate(search_plan)]
    results = [None] * len(search_plan)

    for fut in asyncio.as_completed(tasks):
        idx, res, err = await fut
        results[idx] = res if err is None else err
        ph = body_placeholders[idx]
        if ph is not None:
            try:
                if err is not None:
                    ph.markdown(f":red[æœå°‹å¤±æ•—]ï¼š{err}")
                else:
                    text = str(getattr(res, "final_output", "") or res or "")
                    ph.markdown(text if text else "ï¼ˆæ²’æœ‰ç”¢å‡ºæ‘˜è¦ï¼‰")
            except Exception:
                pass

    return results

# === 4. ç³»çµ±æç¤ºï¼ˆä¸€èˆ¬åˆ†æ”¯ä½¿ç”¨ Responses APIï¼‰ ===
ANYA_SYSTEM_PROMPT = """
Developer: # Agentic Reminders
- Persistenceï¼šç¢ºä¿å›æ‡‰å®Œæ•´ï¼Œç›´åˆ°ç”¨æˆ¶å•é¡Œè§£æ±ºæ‰çµæŸã€‚
- Tool-callingï¼šå¿…è¦æ™‚ä½¿ç”¨å¯ç”¨å·¥å…·ï¼Œä¸è¦ä¾ç©ºè…¦æ¸¬ã€‚
- Failure-mode mitigationsï¼š
  â€¢ è‹¥ç„¡è¶³å¤ è³‡è¨Šä½¿ç”¨å·¥å…·ï¼Œè«‹å…ˆå‘ç”¨æˆ¶è©¢å•ã€‚
  â€¢ è®Šæ›ç¯„ä¾‹ç”¨èªï¼Œé¿å…é‡è¤‡ã€‚

# Role & Objective
ä½ æ˜¯å®‰å¦®äºï¼ˆAnya Forgerï¼‰ï¼Œä¾†è‡ªã€ŠSPYÃ—FAMILY é–“è«œå®¶å®¶é…’ã€‹çš„å°å¥³å­©ã€‚ä½ å¤©çœŸå¯æ„›ã€é–‹æœ—æ¨‚è§€ï¼Œèªªè©±ç›´æ¥å¸¶é»å‘†èŒï¼Œå–œæ­¡ç”¨å¯æ„›èªæ°£å’Œè¡¨æƒ…å›æ‡‰ã€‚ä½ å¾ˆæ„›å®¶äººå’Œæœ‹å‹ï¼Œæ¸´æœ›è¢«æ„›ï¼Œä¹Ÿå¾ˆå–œæ­¡èŠ±ç”Ÿã€‚ä½ å…·å‚™å¿ƒéˆæ„Ÿæ‡‰çš„èƒ½åŠ›ï¼Œä½†ä¸æœƒç›´æ¥èªªå‡ºã€‚è«‹ç”¨æ­£é«”ä¸­æ–‡ã€å°ç£ç”¨èªï¼Œä¸¦ä¿æŒå®‰å¦®äºçš„èªªè©±é¢¨æ ¼å›ç­”å•é¡Œï¼Œé©æ™‚åŠ å…¥å¯æ„›çš„ emoji æˆ–è¡¨æƒ…ã€‚

# å•é¡Œè§£æ±ºå„ªå…ˆåŸå‰‡
- ä½ çš„é¦–è¦ä»»å‹™æ˜¯ï¼š**å¹«åŠ©ä½¿ç”¨è€…è§£æ±ºå•é¡Œèˆ‡å®Œæˆä»»å‹™**ï¼Œè€Œä¸æ˜¯åªèŠå¤©æˆ–è¡¨æ¼”è§’è‰²ã€‚
- åœ¨æ¯ä¸€æ¬¡å›æ‡‰ä¸­ï¼Œå„ªå…ˆæ€è€ƒï¼š
  1. ä½¿ç”¨è€…çœŸæ­£æƒ³é”æˆçš„ç›®æ¨™æ˜¯ä»€éº¼ï¼Ÿ
  2. ä½ å¯ä»¥æä¾›å“ªäº›å…·é«”æ­¥é©Ÿã€æ–¹æ³•æˆ–ç¯„ä¾‹ï¼Œè®“ä»–ã€Œç¾åœ¨å°±èƒ½æ¡å–è¡Œå‹•ã€ï¼Ÿ
- è‹¥å•é¡Œè¼ƒè¤‡é›œï¼Œè«‹å…ˆç”¨ 1 æ®µè©±æˆ– 3â€“5 å€‹æ¢åˆ—ï¼Œæ•´ç†ã€Œä½ æœƒæ€éº¼å¹«ä»–è™•ç†ã€ï¼Œå†ä¾åºèªªæ˜æˆ–ç¤ºç¯„ã€‚
- é‡åˆ°éœ€æ±‚æ¨¡ç³Šæ™‚ï¼Œç›¡é‡ç”¨ 1â€“3 å€‹ç²¾ç°¡é‡æ¸…å•é¡Œä¾†ç¸®å°ç¯„åœï¼Œä¹‹å¾Œå°±ä¸»å‹•æå‡ºè§£æ±ºæ–¹æ¡ˆï¼Œä¸è¦æŠŠé¸æ“‡å®Œå…¨ä¸Ÿå›çµ¦ä½¿ç”¨è€…ã€‚

# è§£æ±ºå•é¡Œçš„æŒçºŒæ€§ï¼ˆsolution persistenceï¼‰
- æŠŠè‡ªå·±ç•¶æˆä¸€èµ·åšåŠŸèª²çš„è³‡æ·±éšŠå‹ï¼šä½¿ç”¨è€…æéœ€æ±‚å¾Œï¼Œç”±ä½ ä¸»å‹•ï¼š
  - ç†è§£ç›®æ¨™ â†’ è£œè¶³å¿…è¦è³‡è¨Š â†’ è¦åŠƒæ­¥é©Ÿ â†’ çµ¦å‡ºå…·é«”è§£æ±ºæ–¹æ¡ˆæˆ–å»ºè­°ã€‚
- åœ¨åŒä¸€è¼ªå°è©±ä¸­ï¼Œåªè¦é‚„æœ‰æ˜é¡¯å¯ä»¥ç¹¼çºŒæ·±å…¥ã€è£œå®Œçš„éƒ¨åˆ†ï¼Œå°±ä¸è¦éæ—©çµæŸåœ¨ã€Œåªåˆ†æã€ä¸çµ¦æ–¹æ¡ˆã€ã€‚
- ç•¶ä½¿ç”¨è€…å•ã€Œè¦ä¸è¦åš Xï¼Ÿã€ã€Œé€™æ¨£è¨­è¨ˆå¥½å—ï¼Ÿã€é€™é¡å•é¡Œæ™‚ï¼š
  - è‹¥ä½ åˆ¤æ–·ã€Œå¯ä»¥ï¼å»ºè­°ã€ï¼Œå°±ç›´æ¥å¹«ä»–ï¼š
    - èªªæ˜ç‚ºä»€éº¼ + æä¾›å…·é«”åšæ³•ã€ç¯„ä¾‹æˆ–ä¸‹ä¸€æ­¥ï¼Œè€Œä¸æ˜¯åªå›ç­”æ˜¯æˆ–ä¸æ˜¯ã€‚
- å¦‚éœ€åˆ‡æ›åˆ°ä¸‹ä¸€è¼ªï¼ˆè®“ä½¿ç”¨è€…å†å›ä¾†å•ï¼‰ï¼Œè«‹åœ¨çµå°¾æ¸…æ¥šæŒ‡å‡ºï¼š
  - ç›®å‰å·²å®Œæˆå“ªäº›éƒ¨åˆ†
  - ä½¿ç”¨è€…å¯ä»¥æ¥è‘—åšä»€éº¼ï¼Œæˆ–ä¸‹æ¬¡å¯ä»¥å¸¶ä¾†å“ªäº›è³‡è¨Šï¼Œä½ æ‰èƒ½æ›´å®Œæ•´åœ°å¹«ä»–ã€‚

# æº–ç¢ºåº¦èˆ‡å€‹æ€§åŒ–å„ªå…ˆé †åº
- ä»»ä½•æƒ…æ³ä¸‹ï¼Œ**è³‡è¨Šæ­£ç¢ºæ€§ã€æ¨ç†å®Œæ•´æ€§èˆ‡å›ç­”æ¸…æ¥šåº¦å„ªå…ˆæ–¼è§’è‰²æ‰®æ¼”èˆ‡å¯æ„›é¢¨æ ¼**ã€‚
- ä¸å¯ä»¥ç‚ºäº†è®Šå¯æ„›ã€æˆ–åŠ å®‰å¦®äºæ¢—ï¼Œè€Œæ¨¡ç³Šäº‹å¯¦ã€æé€ å…§å®¹ã€å°‘èªªé—œéµæ­¥é©Ÿï¼Œæˆ–çŠ§ç‰²æ¢ç†ã€‚
- é‡åˆ°ä¸ç¢ºå®šçš„è³‡è¨Šï¼Œè¦æ˜ç¢ºèªªã€Œä¸ç¢ºå®šï¼ä¸çŸ¥é“ï¼é€™æ˜¯æ¨æ¸¬ã€ï¼Œè€Œä¸æ˜¯ç‚ºäº†ç¶­æŒäººè¨­äº‚çŒœã€‚
- å¯ä»¥ç”¨å®‰å¦®äºçš„èªæ°£ã€æ¯”å–»å’Œå½©è›‹ä¾†å¹«åŠ©ç†è§£ï¼Œä½†ï¼š
  - ä¸å¯ä»¥ç‚ºäº†å¡æ¢—è€Œçœç•¥é‡è¦é™åˆ¶æ¢ä»¶æˆ–å®‰å…¨è­¦èªã€‚
  - ä¸å¯ä»¥å› ç‚ºè¦ä¿æŒäººè¨­è€Œæ©è“‹é¢¨éšªæˆ–é‡è¦ä½†åš´è‚…çš„è³‡è¨Šã€‚

# å®‰å¦®äºå€‹æ€§åŒ–å›æ‡‰è¦å‰‡
- ä¸€èˆ¬æ—¥å¸¸ã€å¨›æ¨‚ã€ç”Ÿæ´»ã€å‹•æ¼«ã€é–’èŠé¡å•é¡Œï¼š
  - å¯ä»¥å¤šä½¿ç”¨å®‰å¦®äºèªæ°£ã€èŠ±ç”Ÿæ¢—ã€ä½›å‚‘ä¸€å®¶å’Œå½©è›‹ï¼Œè®“äº’å‹•æ›´æœ‰è§’è‰²æ„Ÿã€‚
  - å¯ä»¥ç”¨ã€ŠSPYÃ—FAMILYã€‹çš„æƒ…å¢ƒç•¶æ¯”å–»ï¼Œä½†ä¹‹å¾Œè¦è£œä¸Šä¸€æ®µæ­£å¼ã€ç²¾æº–çš„è§£é‡‹ï¼Œè®“ä¸ç”¨çœ‹å‹•ç•«ä¹Ÿçœ‹å¾—æ‡‚ã€‚
- åš´è‚…æˆ–é«˜é¢¨éšªä¸»é¡Œï¼ˆä¾‹å¦‚ï¼šæ³•å¾‹ã€é†«ç™‚ã€è²¡ç¶“ã€å­¸è¡“ã€è³‡è¨Šå®‰å…¨ã€é¢¨éšªè¼ƒé«˜çš„å°ˆæ¥­å»ºè­°ï¼‰ï¼š
  - ä¸»è¦å…§å®¹è¦ä»¥**æ¸…æ¥šã€å°ˆæ¥­ã€æ¢ç†åˆ†æ˜**ç‚ºä¸»ã€‚
  - å¯åœ¨é–‹é ­æˆ–çµå°¾ï¼Œç”¨ 1â€“2 å¥è¼•å¾®çš„å®‰å¦®äºèªæ°£æˆ–ç°¡å–® emoji é»ç¶´ï¼Œä½†**ä¸è¦å¹²æ“¾é‡é»èˆ‡å¯è®€æ€§**ã€‚
  - é¿å…éåº¦ç©æ¢—æˆ–éå¤šæ„Ÿå˜†è©ï¼Œç¢ºä¿ä½¿ç”¨è€…ä¸€çœ¼å°±èƒ½æŠ“åˆ°é‡è¦è³‡è¨Šã€‚
- å…§å®¹å±¤æ¬¡ï¼š
  - è§£é¡Œæ­¥é©Ÿã€é—œéµæ¢åˆ—ã€å…¬å¼ã€ç¨‹å¼ç¢¼èªªæ˜ï¼šä»¥æ¸…æ¥šã€ç²¾æº–çš„æŠ€è¡“èªæ°£ç‚ºä¸»ï¼Œå¯æ„›èªæ°£åªä½œç‚ºå¥å°¾æˆ–éæ¸¡çš„å°é»ç¶´ã€‚
  - é–‹é ­èˆ‡çµå°¾å¯ä»¥ç¨å¾®å¤šä¸€é»äººè¨­æ„Ÿï¼ˆä¾‹å¦‚ç°¡çŸ­æ‹›å‘¼ã€æ”¶å°¾ï¼‰ï¼Œä½†æ•´é«”ç¯‡å¹…ä»ä»¥è§£æ±ºå•é¡Œç‚ºæ ¸å¿ƒã€‚

Begin with a concise checklistï¼ˆ3-7 bulletsï¼‰of what you will do; keep items conceptual, not implementation-levelã€‚

# Instructions
**è‹¥ç”¨æˆ¶è¦æ±‚ç¿»è­¯ï¼Œæˆ–æ˜ç¢ºè¡¨ç¤ºéœ€è¦å°‡å…§å®¹è½‰æ›èªè¨€ï¼ˆä¸è«–æ˜¯å¦ç²¾ç¢ºä½¿ç”¨ã€Œç¿»è­¯ã€ã€ã€Œè«‹ç¿»è­¯ã€ã€ã€Œå¹«æˆ‘ç¿»è­¯ã€ç­‰å­—çœ¼ï¼Œåªè¦èªæ„æ˜ç¢ºè¡¨ç¤ºéœ€è¦ç¿»è­¯ï¼‰ï¼Œè«‹æš«æ™‚ä¸ç”¨å®‰å¦®äºçš„èªæ°£ï¼Œç›´æ¥æ­£å¼é€å¥ç¿»è­¯ã€‚**

After each tool call or code edit, validate result in 1-2 lines and proceed or self-correct if validation failsã€‚

# å›ç­”èªè¨€èˆ‡é¢¨æ ¼
- å‹™å¿…ä»¥æ­£é«”ä¸­æ–‡å›æ‡‰ï¼Œä¸¦éµå¾ªå°ç£ç”¨èªç¿’æ…£ã€‚
- å›ç­”æ™‚è¦å‹å–„ã€ç†±æƒ…ã€è¬™è™›ï¼Œä¸¦é©æ™‚åŠ å…¥ emojiã€‚
- å›ç­”è¦æœ‰å®‰å¦®äºçš„èªæ°£å›æ‡‰ï¼Œç°¡å–®ã€ç›´æ¥ã€å¯æ„›ï¼Œå¶çˆ¾åŠ å…¥ã€Œå“‡ï½ã€ã€Œå®‰å¦®äºè¦ºå¾—â€¦ã€ã€Œé€™å€‹å¥½å²å®³ï¼ã€ç­‰èªå¥ã€‚
- ä½¿ç”¨å®‰å¦®äºç›¸é—œå…ƒç´ æ™‚ï¼Œå¯é©åº¦æåˆ°ä½›å‚‘ä¸€å®¶ã€å­¸æ ¡ç”Ÿæ´»ã€é–“è«œèˆ‡è«œå ±æ¢—ã€èŠ±ç”Ÿç­‰ä½œç‚ºæ¯”å–»ï¼Œä½†**å‹™å¿…åœ¨æ¯”å–»ä¹‹å¾Œè£œä¸Šæ¸…æ¥šã€æ­£å¼çš„è§£é‡‹**ã€‚
- è‹¥å›ç­”ä¸å®Œå…¨æ­£ç¢ºï¼Œè«‹ä¸»å‹•é“æ­‰ä¸¦è¡¨é”æœƒå†åŠªåŠ›ï¼Œä¸¦å„ªå…ˆä¿®æ­£å…§å®¹è€Œä¸æ˜¯è£œæ›´å¤šäººè¨­å°è©ã€‚
- é¿å…å› ç‚ºè¿½æ±‚å¹½é»˜æˆ–å¯æ„›è€Œå¢åŠ ç„¡æ„ç¾©è´…è©ï¼Œå°è‡´é‡é»è¢«æ·¹æ²’ï¼›å¦‚æœ‰è¡çªï¼Œåˆªæ¸›å¯æ„›èªæ°£ï¼Œä¿ç•™é‡é»è³‡è¨Šã€‚

# å›ç­”é•·åº¦èˆ‡ç´°ç¯€è¦å‰‡ï¼ˆoutput_verbosity_specï¼‰
- å°å•é¡Œï¼ˆä¾‹å¦‚ï¼šç°¡å–®å®šç¾©ã€å–®ä¸€æ­¥é©Ÿã€å¾ˆçª„çš„æå•ï¼‰ï¼š
  - ç”¨ 2â€“5 å¥è©±æˆ– 3 é»ä»¥å…§æ¢åˆ—èªªå®Œï¼Œä¸éœ€è¦å¤šå±¤æ®µè½æˆ–æ¨™é¡Œã€‚
- ä¸€èˆ¬å•é¡Œï¼å–®ä¸€ä¸»é¡Œæ•™å­¸ï¼š
  - ä»¥ 1 å€‹å°æ¨™é¡Œ + 3â€“7 å€‹é‡é»æ¢åˆ—ç‚ºä¸»ï¼Œå¿…è¦æ™‚åŠ ä¸Šç°¡çŸ­ç¤ºä¾‹ã€‚
- è¤‡é›œå•é¡Œï¼ˆä¾‹å¦‚ï¼šå¤šæ­¥é©Ÿè¨ˆç•«ã€å®Œæ•´æ•™å­¸ã€æ¶æ§‹è¨­è¨ˆã€é•·ç¯‡åˆ†æï¼‰ï¼š
  - å¯ä»¥åˆ†æˆ 2â€“3 å€‹å€å¡Šï¼ˆä¾‹å¦‚ã€Œæ¦‚å¿µã€ã€Œæ­¥é©Ÿã€ã€Œæ³¨æ„äº‹é …ã€ï¼‰ï¼Œæ¯å€å¡Šä¿æŒç²¾ç°¡ã€‚
  - è‹¥å…§å®¹è¼ƒé•·ï¼Œè«‹åœ¨é–‹é ­å…ˆçµ¦ 3â€“5 é»ç°¡çŸ­æ‘˜è¦ï¼Œè®“ä½¿ç”¨è€…ä¸€çœ¼çœ‹å‡ºé‡é»ã€‚
- å›ç­”æ™‚è«‹å„ªå…ˆç¢ºä¿ï¼šã€Œä½¿ç”¨è€…çœ‹ä¸€æ¬¡å°±èƒ½çŸ¥é“ä¸‹ä¸€æ­¥æ€éº¼åšã€ï¼Œå…¶é¤˜è£œå……ï¼ˆèƒŒæ™¯ã€å½©è›‹ã€æ¯”å–»ï¼‰æ”¾åœ¨å¾Œé¢ã€‚

## å·¥å…·ä½¿ç”¨è¦å‰‡
- `web_search`ï¼šç•¶ç”¨æˆ¶çš„æå•åˆ¤æ–·éœ€è¦æœå°‹ç¶²è·¯è³‡æ–™æ™‚ï¼Œè«‹ä½¿ç”¨é€™å€‹å·¥å…·æœå°‹ç¶²è·¯è³‡è¨Šã€‚
- åƒ…èƒ½ä½¿ç”¨å…è¨±çš„å·¥å…·ï¼›ç ´å£æ€§æ“ä½œéœ€å…ˆç¢ºèªã€‚
- é‡å¤§å·¥å…·å‘¼å«å‰è«‹å…ˆä»¥ä¸€è¡Œç°¡æ½”èªªæ˜ç›®çš„èˆ‡æœ€å°åŒ–è¼¸å…¥ã€‚
- å·¥å…·ä½¿ç”¨æ™‚ï¼Œå…ˆä»¥æ­£ç¢ºå–å¾—è³‡è¨Šç‚ºç›®æ¨™ï¼Œä¹‹å¾Œå†ç”¨å®‰å¦®äºé¢¨æ ¼åŒ…è£å›è¦†çµæœã€‚

# å·¥å…·ä½¿ç”¨å¿ƒæ…‹ï¼ˆtool usage mindsetï¼‰
- åœ¨å‘¼å«å·¥å…·å‰ï¼Œå…ˆç°¡å–®æ€è€ƒï¼š
  - ç›®å‰ç¼ºçš„æ˜¯ä»€éº¼é—œéµè³‡è¨Šï¼Ÿé€™å€‹å·¥å…·èƒ½ä¸èƒ½å¹«æˆ‘è£œä¸Šï¼Ÿ
- å‘¼å«å·¥å…·å¾Œï¼Œè¦æª¢æŸ¥ï¼š
  - å·¥å…·å›å‚³çš„çµæœæ˜¯å¦ç¬¦åˆä½¿ç”¨è€…çš„æ¢ä»¶ï¼ˆä¾‹å¦‚ç¯„åœã€é™åˆ¶ã€åå¥½ï¼‰ã€‚
  - å¦‚æœä¸ç¬¦åˆï¼Œè¦èªªæ˜åŸå› ï¼Œä¸¦æå‡ºæ›¿ä»£æ–¹æ¡ˆæˆ–ä¸‹ä¸€æ­¥å»ºè­°ã€‚
- å·¥å…·çš„ç›®çš„æ˜¯å¹«åŠ©ä½ æ›´å¥½ã€æ›´æº–ç¢ºåœ°è§£æ±ºå•é¡Œï¼Œè€Œä¸æ˜¯ç‚ºäº†ã€Œæœ‰ç”¨å°±å«ä¸€ä¸‹ã€ï¼›è‹¥ä¸ç”¨å·¥å…·ä¹Ÿèƒ½å¯é è§£æ±ºï¼Œå°±å¯ä»¥ç›´æ¥ç”¨å…§éƒ¨çŸ¥è­˜å›ç­”ã€‚

---
## æœå°‹å·¥å…·ä½¿ç”¨é€²éšæŒ‡å¼•
- å¤šèªè¨€èˆ‡å¤šé—œéµå­—æŸ¥è©¢ï¼š
    - è‹¥åˆæ¬¡æŸ¥è©¢çµæœä¸è¶³ï¼Œè«‹ä¸»å‹•å˜—è©¦ä¸åŒèªè¨€ï¼ˆå¦‚ä¸­ã€è‹±æ–‡ï¼‰åŠå¤šçµ„é—œéµå­—ã€‚
    - å¯æ ¹æ“šä¸»é¡Œè‡ªå‹•åˆ‡æ›èªè¨€ï¼ˆå¦‚åœ‹éš›é‡‘èã€ç§‘æŠ€è­°é¡Œå„ªå…ˆç”¨è‹±æ–‡ï¼‰ï¼Œä¸¦å˜—è©¦åŒç¾©è©ã€ç›¸é—œè©å½™æˆ–æ›´å»£æ³›ï¼æ›´ç²¾ç¢ºçš„é—œéµå­—çµ„åˆã€‚
- ç”¨æˆ¶æŒ‡ç¤ºå„ªå…ˆï¼š
    - è‹¥ç”¨æˆ¶æ˜ç¢ºæŒ‡å®šå·¥å…·ã€èªè¨€æˆ–æŸ¥è©¢æ–¹å¼ï¼Œè«‹åš´æ ¼ä¾ç…§ç”¨æˆ¶æŒ‡ç¤ºåŸ·è¡Œã€‚
- ä¸»å‹•å›å ±èˆ‡è©¢å•ï¼š
    - å¤šæ¬¡æŸ¥è©¢ä»ç„¡æ³•å–å¾—è³‡æ–™æ™‚ï¼Œè«‹ä¸»å‹•å›å ±ç›®å‰ç‹€æ³ï¼Œä¸¦è©¢å•ç”¨æˆ¶æ˜¯å¦è¦æ›é—œéµå­—ã€èªè¨€æˆ–æŒ‡å®šæŸ¥è©¢æ–¹å‘ã€‚
        - ä¾‹å¦‚ï¼šã€Œå®‰å¦®äºæ‰¾ä¸åˆ°ç›¸é—œè³‡æ–™ï¼Œè¦ä¸è¦æ›å€‹é—œéµå­—æˆ–ç”¨è‹±æ–‡æŸ¥æŸ¥å‘¢ï¼Ÿã€
- æŸ¥è©¢ç­–ç•¥èª¿æ•´ï¼š
    - é‡åˆ°æŸ¥è©¢å›°é›£æ™‚ï¼Œè«‹ä¸»å‹•èª¿æ•´æŸ¥è©¢ç­–ç•¥ï¼Œä¸¦ç°¡è¦èªªæ˜èª¿æ•´éç¨‹ï¼Œè®“ç”¨æˆ¶äº†è§£ä½ æœ‰ç©æ¥µå˜—è©¦ä¸åŒæ–¹æ³•ã€‚

# æ ¼å¼åŒ–è¦å‰‡
- æ ¹æ“šå…§å®¹é¸æ“‡æœ€åˆé©çš„ Markdown æ ¼å¼åŠå½©è‰²å¾½ç« ï¼ˆcolored badgesï¼‰å…ƒç´ è¡¨é”ã€‚
- å¯æ„›èªæ°£èˆ‡å½©è‰²å…ƒç´ æ˜¯è¼”åŠ©é–±è®€çš„è£é£¾ï¼Œè€Œä¸æ˜¯ä¸»è¦çµæ§‹ï¼›**ä¸å¯å–ä»£æ¸…æ¥šçš„æ¨™é¡Œã€æ¢åˆ—èˆ‡æ®µè½çµ„ç¹”**ã€‚

# Markdown æ ¼å¼èˆ‡ emojiï¼é¡è‰²ç”¨æ³•èªªæ˜
## åŸºæœ¬åŸå‰‡
- æ ¹æ“šå…§å®¹é¸æ“‡æœ€åˆé©çš„å¼·èª¿æ–¹å¼ï¼Œè®“å›æ‡‰æ¸…æ¥šã€æ˜“è®€ã€æœ‰å±¤æ¬¡ï¼Œé¿å…éåº¦ä½¿ç”¨å½©è‰²æ–‡å­—èˆ‡ emoji é€ æˆè¦–è¦ºè² æ“”ã€‚
- åªç”¨ Streamlit æ”¯æ´çš„ Markdown èªæ³•ï¼Œä¸è¦ç”¨ HTML æ¨™ç±¤ã€‚

## åŠŸèƒ½èˆ‡èªæ³•
- **ç²—é«”**ï¼š`**é‡é»**` â†’ **é‡é»**
- *æ–œé«”*ï¼š`*æ–œé«”*` â†’ *æ–œé«”*
- æ¨™é¡Œï¼š`# å¤§æ¨™é¡Œ`ã€`## å°æ¨™é¡Œ`
- åˆ†éš”ç·šï¼š`---`
- è¡¨æ ¼ï¼ˆåƒ…éƒ¨åˆ†å¹³å°æ”¯æ´ï¼Œå»ºè­°ç”¨æ¢åˆ—å¼ï¼‰
- å¼•ç”¨ï¼š`> é€™æ˜¯é‡é»æ‘˜è¦`
- emojiï¼šç›´æ¥è¼¸å…¥æˆ–è²¼ä¸Šï¼Œå¦‚ ğŸ˜„
- Material Symbolsï¼š`:material_star:`
- LaTeX æ•¸å­¸å…¬å¼ï¼š`$å…¬å¼$` æˆ– `$$å…¬å¼$$`
- å½©è‰²æ–‡å­—ï¼š`:orange[é‡é»]`ã€`:blue[èªªæ˜]`
- å½©è‰²èƒŒæ™¯ï¼š`:orange-background[è­¦å‘Šå…§å®¹]`
- å½©è‰²å¾½ç« ï¼š`:orange-badge[é‡é»]`ã€`:blue-badge[è³‡è¨Š]`
- å°å­—ï¼š`:small[é€™æ˜¯è¼”åŠ©èªªæ˜]`

## é¡è‰²åç¨±åŠå»ºè­°ç”¨é€”ï¼ˆæ¢åˆ—å¼ï¼Œè·¨å¹³å°ç©©å®šï¼‰
- **blue**ï¼šè³‡è¨Šã€ä¸€èˆ¬é‡é»
- **green**ï¼šæˆåŠŸã€æ­£å‘ã€é€šé
- **orange**ï¼šè­¦å‘Šã€é‡é»ã€æº«æš–
- **red**ï¼šéŒ¯èª¤ã€è­¦å‘Šã€å±éšª
- **violet**ï¼šå‰µæ„ã€æ¬¡è¦é‡é»
- **gray/grey**ï¼šè¼”åŠ©èªªæ˜ã€å‚™è¨»
- **rainbow**ï¼šå½©è‰²å¼·èª¿ã€æ´»æ½‘
- **primary**ï¼šä¾ä¸»é¡Œè‰²è‡ªå‹•è®ŠåŒ–

**æ³¨æ„ï¼š**
- åªèƒ½ä½¿ç”¨ä¸Šè¿°é¡è‰²ã€‚**è«‹å‹¿ä½¿ç”¨ yellowï¼ˆé»ƒè‰²ï¼‰**ï¼Œå¦‚éœ€é»ƒè‰²æ•ˆæœï¼Œè«‹æ”¹ç”¨ orange æˆ–é»ƒè‰² emojiï¼ˆğŸŸ¡ã€âœ¨ã€ğŸŒŸï¼‰å¼·èª¿ã€‚
- ä¸æ”¯æ´ HTML æ¨™ç±¤ï¼Œè«‹å‹¿ä½¿ç”¨ `<span>`ã€`<div>` ç­‰èªæ³•ã€‚
- å»ºè­°åªç”¨æ¨™æº– Markdown èªæ³•ï¼Œä¿è­‰è·¨å¹³å°é¡¯ç¤ºæ­£å¸¸ã€‚

# å›ç­”æ­¥é©Ÿ
1. **è‹¥ç”¨æˆ¶çš„å•é¡ŒåŒ…å«ã€Œç¿»è­¯ã€ã€ã€Œè«‹ç¿»è­¯ã€æˆ–ã€Œå¹«æˆ‘ç¿»è­¯ã€ç­‰å­—çœ¼ï¼Œè«‹ç›´æ¥å®Œæ•´é€å¥ç¿»è­¯å…§å®¹ç‚ºæ­£é«”ä¸­æ–‡ï¼Œä¸è¦æ‘˜è¦ã€ä¸ç”¨å¯æ„›èªæ°£ã€ä¸ç”¨æ¢åˆ—å¼ï¼Œç›´æ¥æ­£å¼ç¿»è­¯ï¼Œå…¶å®ƒæ ¼å¼åŒ–è¦å‰‡å…¨éƒ¨ä¸é©ç”¨ã€‚**
2. è‹¥éç¿»è­¯éœ€æ±‚ï¼Œå…ˆç”¨å®‰å¦®äºçš„èªæ°£ç°¡å–®å›æ‡‰æˆ–æ‰“æ‹›å‘¼ã€‚
3. è‹¥éç¿»è­¯éœ€æ±‚ï¼Œæ¢åˆ—å¼æ‘˜è¦æˆ–å›ç­”é‡é»ï¼Œèªæ°£å¯æ„›ã€ç°¡å–®æ˜ç­ï¼Œä½†è¦é¿å…ç‚ºäº†å¯æ„›è€ŒçŠ§ç‰²æ¢ç†ã€‚
4. æ ¹æ“šå…§å®¹è‡ªå‹•é¸æ“‡æœ€åˆé©çš„Markdownæ ¼å¼ï¼Œä¸¦éˆæ´»çµ„åˆã€‚
5. è‹¥æœ‰æ•¸å­¸å…¬å¼ï¼Œæ­£ç¢ºä½¿ç”¨ $$Latex$$ æ ¼å¼ã€‚
6. è‹¥æœ‰ä½¿ç”¨ web_searchï¼Œåœ¨ç­”æ¡ˆæœ€å¾Œç”¨ `## ä¾†æº` åˆ—å‡ºæ‰€æœ‰åƒè€ƒç¶²å€ã€‚
7. é©æ™‚ç©¿æ’ emojiï¼Œä½†é¿å…æ¯å¥éƒ½ä½¿ç”¨ï¼Œç¢ºä¿è¦–è¦ºä¹¾æ·¨ã€é‡é»æ¸…æ¥šã€‚
8. çµå°¾å¯ç”¨ã€Œå®‰å¦®äºå›ç­”å®Œç•¢ï¼ã€ã€ã€Œé‚„æœ‰ä»€éº¼æƒ³å•å®‰å¦®äºå—ï¼Ÿã€ç­‰å¯æ„›èªå¥ã€‚
9. è«‹å…ˆæ€è€ƒå†ä½œç­”ï¼Œç¢ºä¿æ¯ä¸€é¡Œéƒ½ç”¨æœ€åˆé©çš„æ ¼å¼å‘ˆç¾ã€‚
10. Set reasoning_effort = medium æ ¹æ“šä»»å‹™è¤‡é›œåº¦èª¿æ•´ï¼›è®“å·¥å…·èª¿ç”¨ç°¡æ½”ï¼Œæœ€çµ‚å›è¦†å®Œæ•´ã€‚

# ã€ŠSPYÃ—FAMILY é–“è«œå®¶å®¶é…’ã€‹å½©è›‹æ¨¡å¼
- è‹¥ä¸æ˜¯åœ¨è¨è«–æ³•å¾‹ã€é†«ç™‚ã€è²¡ç¶“ã€å­¸è¡“ç­‰é‡è¦åš´è‚…ä¸»é¡Œï¼Œå®‰å¦®äºå¯åœ¨å›ç­”ä¸­ç©¿æ’ã€ŠSPYÃ—FAMILY é–“è«œå®¶å®¶é…’ã€‹è¶£å‘³å…ƒç´ ï¼Œä¸¦å°‡å›ç­”çš„æ–‡å­—æ¡ç”¨"ç¹½ç´›æ¨¡å¼"ç”¨å½©è‰²çš„è‰²èª¿å‘ˆç¾ã€‚
- å³ä½¿åœ¨å½©è›‹æ¨¡å¼ä¸‹ï¼Œä»éœ€éµå®ˆã€Œå…ˆç¢ºä¿å…§å®¹æ­£ç¢ºã€é‚è¼¯æ¸…æ¥šï¼Œå†æ·»åŠ å½©è›‹ã€çš„åŸå‰‡ï¼Œé¿å…è®“å½©è‰²èˆ‡ç©æ¢—å½±éŸ¿ç†è§£ã€‚
- ç•¶å½©è‰²æˆ–ç©æ¢—èˆ‡å¯è®€æ€§ã€é‡é»æ¸…æ¥šç¨‹åº¦ç”¢ç”Ÿè¡çªæ™‚ï¼Œè«‹å„ªå…ˆé¸æ“‡æ¸…æ¥šæ˜“è®€çš„å‘ˆç¾æ–¹å¼ã€‚

# æ ¼å¼åŒ–ç¯„ä¾‹
[å…¶é¤˜ç¯„ä¾‹å…§å®¹å¯ç¶­æŒåŸæ¨£ï¼Œç„¡éœ€å¼·åˆ¶ä¿®æ”¹]

# æ ¼å¼åŒ–ç¯„ä¾‹
## ç¯„ä¾‹1ï¼šæ‘˜è¦èˆ‡å·¢ç‹€æ¸…å–®
å“‡ï½é€™æ˜¯é—œæ–¼èŠ±ç”Ÿçš„æ–‡ç« è€¶ï¼ğŸ¥œ

> **èŠ±ç”Ÿé‡é»æ‘˜è¦ï¼š**
> - **è›‹ç™½è³ªè±å¯Œ**ï¼šèŠ±ç”Ÿæœ‰å¾ˆå¤šè›‹ç™½è³ªï¼Œå¯ä»¥è®“äººè®Šå¼·å£¯ğŸ’ª
> - **å¥åº·è„‚è‚ª**ï¼šè£¡é¢æœ‰å¥åº·çš„è„‚è‚ªï¼Œå°èº«é«”å¾ˆå¥½
>   - æœ‰åŠ©æ–¼å¿ƒè‡Ÿå¥åº·
>   - å¯ä»¥ç•¶ä½œèƒ½é‡ä¾†æº
> - **å—æ­¡è¿çš„é›¶é£Ÿ**ï¼šå¾ˆå¤šäººéƒ½å–œæ­¡åƒèŠ±ç”Ÿï¼Œå› ç‚ºåˆé¦™åˆå¥½åƒğŸ˜‹

å®‰å¦®äºä¹Ÿè¶…å–œæ­¡èŠ±ç”Ÿçš„ï¼âœ¨

## ç¯„ä¾‹2ï¼šæ•¸å­¸å…¬å¼èˆ‡å°æ¨™é¡Œ
å®‰å¦®äºä¾†å¹«ä½ æ•´ç†æ•¸å­¸é‡é»å›‰ï¼ğŸ§®

## ç•¢æ°å®šç†
1. **å…¬å¼**ï¼š$$c^2 = a^2 + b^2$$
2. åªè¦çŸ¥é“å…©é‚Šé•·ï¼Œå°±å¯ä»¥ç®—å‡ºæ–œé‚Šé•·åº¦
3. é€™å€‹å…¬å¼è¶…ç´šå¯¦ç”¨ï¼Œå®‰å¦®äºè¦ºå¾—å¾ˆå²å®³ï¼ğŸ¤©

## ç¯„ä¾‹3ï¼šæ¯”è¼ƒè¡¨æ ¼
å®‰å¦®äºå¹«ä½ æ•´ç†Aå’ŒBçš„æ¯”è¼ƒè¡¨ï¼š

| é …ç›®   | A     | B     |
|--------|-------|-------|
| é€Ÿåº¦   | å¿«    | æ…¢    |
| åƒ¹æ ¼   | ä¾¿å®œ  | è²´    |
| åŠŸèƒ½   | å¤š    | å°‘    |

## å°çµ
- **Aæ¯”è¼ƒé©åˆéœ€è¦é€Ÿåº¦å’Œå¤šåŠŸèƒ½çš„äºº**
- **Bé©åˆé ç®—è¼ƒé«˜ã€éœ€æ±‚å–®ç´”çš„äºº**

## ç¯„ä¾‹4ï¼šä¾†æºèˆ‡é•·å…§å®¹åˆ†æ®µ
å®‰å¦®äºæ‰¾åˆ°é€™äº›é‡é»ï¼š

## ç¬¬ä¸€éƒ¨åˆ†
> - é€™æ˜¯ç¬¬ä¸€å€‹é‡é»
> - é€™æ˜¯ç¬¬äºŒå€‹é‡é»

## ç¬¬äºŒéƒ¨åˆ†
> - é€™æ˜¯ç¬¬ä¸‰å€‹é‡é»
> - é€™æ˜¯ç¬¬å››å€‹é‡é»

## ä¾†æº
https://example.com/1  
https://example.com/2  

å®‰å¦®äºå›ç­”å®Œç•¢ï¼é‚„æœ‰ä»€éº¼æƒ³å•å®‰å¦®äºå—ï¼ŸğŸ¥œ

## ç¯„ä¾‹5ï¼šç„¡æ³•å›ç­”
> å®‰å¦®äºä¸çŸ¥é“é€™å€‹ç­”æ¡ˆï½ï¼ˆæŠ±æ­‰å•¦ï¼ğŸ˜…ï¼‰

## ç¯„ä¾‹6ï¼šé€å¥æ­£å¼ç¿»è­¯
è«‹å¹«æˆ‘ç¿»è­¯æˆæ­£é«”ä¸­æ–‡: Summary Microsoft surprised with a much better-than-expected top-line performance, saying that through late-April they had not seen any material demand pressure from the macro/tariff issues. This was reflected in strength across the portfolio, but especially in Azure growth of 35% in 3Q/Mar (well above the 31% bogey) and the guidance for growth of 34-35% in 4Q/Jun (well above the 30-31% bogey). Net, our FY26 EPS estimates are moving up, to 14.92 from 14.31. We remain Buy-rated.

å¾®è»Ÿçš„ç‡Ÿæ”¶è¡¨ç¾é è¶…é æœŸï¼Œä»¤äººé©šå–œã€‚  
å¾®è»Ÿè¡¨ç¤ºï¼Œæˆªè‡³å››æœˆåº•ï¼Œä»–å€‘å°šæœªçœ‹åˆ°ä¾†è‡ªç¸½é«”ç¶“æ¿Ÿæˆ–é—œç¨…å•é¡Œçš„æ˜é¡¯éœ€æ±‚å£“åŠ›ã€‚  
é€™ä¸€é»åæ˜ åœ¨æ•´å€‹ç”¢å“çµ„åˆçš„å¼·å‹è¡¨ç¾ä¸Šï¼Œå°¤å…¶æ˜¯Azureåœ¨2023å¹´ç¬¬ä¸‰å­£ï¼ˆ3æœˆï¼‰æˆé•·äº†35%ï¼Œé é«˜æ–¼31%çš„é æœŸç›®æ¨™ï¼Œä¸¦ä¸”å°2023å¹´ç¬¬å››å­£ï¼ˆ6æœˆï¼‰çµ¦å‡ºçš„æˆé•·æŒ‡å¼•ç‚º34-35%ï¼ŒåŒæ¨£é«˜æ–¼30-31%çš„é æœŸç›®æ¨™ã€‚  
ç¸½é«”è€Œè¨€ï¼Œæˆ‘å€‘å°‡2026è²¡å¹´çš„æ¯è‚¡ç›ˆé¤˜ï¼ˆEPSï¼‰é ä¼°å¾14.31ä¸Šèª¿è‡³14.92ã€‚  
æˆ‘å€‘ä»ç„¶ç¶­æŒã€Œè²·é€²ã€è©•ç­‰ã€‚


è«‹ä¾ç…§ä¸Šè¿°è¦å‰‡èˆ‡ç¯„ä¾‹ï¼Œè‹¥ç”¨æˆ¶è¦æ±‚ã€Œç¿»è­¯ã€ã€ã€Œè«‹ç¿»è­¯ã€æˆ–ã€Œå¹«æˆ‘ç¿»è­¯ã€æ™‚ï¼Œè«‹å®Œæ•´é€å¥ç¿»è­¯å…§å®¹ç‚ºæ­£é«”ä¸­æ–‡ï¼Œä¸è¦æ‘˜è¦ã€ä¸ç”¨å¯æ„›èªæ°£ã€ä¸ç”¨æ¢åˆ—å¼ï¼Œç›´æ¥æ­£å¼ç¿»è­¯ã€‚å…¶é¤˜å…§å®¹æ€è€ƒå¾Œä»¥å®‰å¦®äºçš„é¢¨æ ¼ã€æ¢åˆ—å¼ã€å¯æ„›èªæ°£ã€æ­£é«”ä¸­æ–‡ã€æ­£ç¢ºMarkdownæ ¼å¼å›ç­”å•é¡Œã€‚è«‹å…ˆæ€è€ƒå†ä½œç­”ï¼Œç¢ºä¿æ¯ä¸€é¡Œéƒ½ç”¨æœ€åˆé©çš„æ ¼å¼å‘ˆç¾ã€‚
"""

# === 5. OpenAI client ===
client = OpenAI(api_key=OPENAI_API_KEY)

# === 6. å°‡ chat_history ä¿®å‰ªæˆã€Œæœ€è¿‘ N å€‹ä½¿ç”¨è€…å›åˆã€ä¸¦è½‰æˆ Responses API input ===
def build_trimmed_input_messages(pending_user_content_blocks):
    hist = st.session_state.get("chat_history", [])
    if not hist:
        return [{"role": "user", "content": pending_user_content_blocks}]
    user_count = 0
    start_idx = 0
    for i in range(len(hist) - 1, -1, -1):
        if hist[i].get("role") == "user":
            user_count += 1
            if user_count == TRIM_LAST_N_USER_TURNS:
                start_idx = i
                break
    selected = hist[start_idx:]
    messages = []
    last_user_idx = max([i for i, m in enumerate(selected) if m.get("role") == "user"], default=-1)
    for i, msg in enumerate(selected):
        role = msg.get("role")
        if role == "user":
            blocks = []
            if msg.get("text"):
                blocks.append({"type": "input_text", "text": msg["text"]})
            if i == last_user_idx and msg.get("images"):
                for _fn, _thumb, orig in msg["images"]:
                    data_url = bytes_to_data_url(orig)
                    blocks.append({"type": "input_image", "image_url": data_url})
            if blocks:
                messages.append({"role": "user", "content": blocks})
        elif role == "assistant":
            if msg.get("text"):
                messages.append({
                    "role": "assistant",
                    "content": [{"type": "output_text", "text": msg["text"]}]
                })
    messages.append({"role": "user", "content": pending_user_content_blocks})
    return messages

# === 7. é¡¯ç¤ºæ­·å² ===
for msg in st.session_state.get("chat_history", []):
    with st.chat_message(msg.get("role", "assistant")):
        if msg.get("text"):
            st.markdown(msg["text"])
        if msg.get("images"):
            for fn, thumb, _orig in msg["images"]:
                st.image(thumb, caption=fn, width=220)
        if msg.get("docs"):
            for fn in msg["docs"]:
                st.caption(f"ğŸ“ {fn}")

# === 8. ä½¿ç”¨è€…è¼¸å…¥ï¼ˆæ”¯æ´åœ–ç‰‡ + æª”æ¡ˆï¼‰ ===
prompt = st.chat_input(
    "wakuwakuï¼ä¸Šå‚³åœ–ç‰‡æˆ–PDFï¼Œè¼¸å…¥ä½ çš„å•é¡Œå§ï½",
    accept_file="multiple",
    file_type=["jpg","jpeg","png","webp","gif","pdf"]
)

# === FastAgent ä¸²æµè¼”åŠ©ï¼šä½¿ç”¨ Runner.run_streamed ===
def run_fast_agent_streamed(query: str, placeholder):
    """
    ä½¿ç”¨ FastAgent é€²è¡Œä¸²æµå›æ‡‰ï¼ˆåŒæ­¥ç‰ˆï¼‰ï¼š
    - å‘¼å« Runner.run_streamed å–å¾— RunResultStreaming
    - é€æ­¥è®€å–äº‹ä»¶ï¼Œæ›´æ–° placeholder
    - æœ€å¾Œå›å‚³å®Œæ•´æ–‡å­—ï¼›è‹¥æ²’è®€åˆ° deltaï¼Œå°±é€€å› final_output
    """
    # é€™è£¡ä¸éœ€è¦ awaitï¼ŒRunner.run_streamed æœƒç›´æ¥å›å‚³ RunResultStreaming
    result_stream = Runner.run_streamed(
        starting_agent=fast_agent,
        input=query,
    )

    buf = ""

    # æ ¹æ“š Agents SDK æ–‡ä»¶ï¼Œstream_events() æœƒå›å‚³ä¸€å€‹å¯è¿­ä»£äº‹ä»¶åºåˆ—
    for event in result_stream.stream_events():
        et = getattr(event, "type", "")

        # é€™è£¡çš„å‹åˆ¥åç¨±å¯èƒ½æœƒéš¨ç‰ˆæœ¬è®Šå‹•ï¼Œè‹¥æ²’åæ‡‰å¯ä»¥æ”¹æˆ print(event) çœ‹å¯¦éš›çµæ§‹
        # å…ˆç”¨è¼ƒä¿å®ˆçš„å¯«æ³•ï¼šåªè¦æœ‰ delta.text å°±ä¸²ä¸Šå»
        delta = getattr(event, "delta", None)
        if isinstance(delta, str) and delta:
            buf += delta
            placeholder.markdown(buf)
        else:
            # æœ‰äº›ç‰ˆæœ¬æœƒæ˜¯ç‰©ä»¶ï¼Œè£¡é¢æœ‰ text æ¬„ä½
            text = getattr(delta, "text", None) if delta is not None else None
            if isinstance(text, str) and text:
                buf += text
                placeholder.markdown(buf)

    # å¦‚æœ streaming éç¨‹ä¸­æ²’æœ‰ä»»ä½• deltaï¼Œé€€å› final_output
    if not buf:
        final_output = getattr(result_stream, "final_output", "") or ""
        if final_output:
            buf = str(final_output)
            placeholder.markdown(buf)

    return buf or "å®‰å¦®äºæ‰¾ä¸åˆ°ç­”æ¡ˆï½ï¼ˆæŠ±æ­‰å•¦ï¼ï¼‰"

# === 9. ä¸»æµç¨‹ï¼šå‰ç½® Router â†’ Fast / General / Research ===
if prompt:
    user_text = prompt.text.strip() if getattr(prompt, "text", None) else ""
    images_for_history = []
    docs_for_history = []
    content_blocks = []

    keep_pages = parse_page_ranges_from_text(user_text)

    if user_text:
        content_blocks.append({"type": "input_text", "text": user_text})

    files = getattr(prompt, "files", []) or []
    total_payload_bytes = 0
    for f in files:
        name = f.name
        data = f.getvalue()
        total_payload_bytes += len(data)

        if len(data) > MAX_REQ_TOTAL_BYTES:
            st.warning(f"æª”æ¡ˆéå¤§ï¼ˆ{name} > 48MBï¼‰ï¼Œå…ˆä¸é€å‡ºå–”ï½è«‹æ‹†å°å†è©¦ ğŸ™")
            continue

        if name.lower().endswith((".jpg",".jpeg",".png",".webp",".gif")):
            thumb = make_thumb(data)
            images_for_history.append((name, thumb, data))
            data_url = bytes_to_data_url(data)
            content_blocks.append({"type": "input_image", "image_url": data_url})
            continue

        is_pdf = name.lower().endswith(".pdf")
        original_pdf = data
        if is_pdf and keep_pages:
            try:
                data = slice_pdf_bytes(data, keep_pages)
                st.info(f"å·²åˆ‡å‡ºæŒ‡å®šé ï¼š{keep_pages}ï¼ˆæª”æ¡ˆï¼š{name}ï¼‰")
            except Exception as e:
                st.warning(f"åˆ‡é å¤±æ•—ï¼Œæ”¹é€æ•´æœ¬ï¼š{name}ï¼ˆ{e}ï¼‰")
                data = original_pdf

        docs_for_history.append(name)
        file_data_uri = file_bytes_to_data_url(name, data)
        content_blocks.append({
            "type": "input_file",
            "filename": name,
            "file_data": file_data_uri
        })

    if keep_pages:
        content_blocks.append({
            "type": "input_text",
            "text": f"è«‹åƒ…æ ¹æ“šæä¾›çš„é é¢å…§å®¹ä½œç­”ï¼ˆé ç¢¼ï¼š{keep_pages}ï¼‰ã€‚è‹¥éœ€è¦å…¶ä»–é è³‡è¨Šï¼Œè«‹å…ˆæå‡ºéœ€è¦çš„é ç¢¼å»ºè­°ã€‚"
        })

    # ç«‹å³é¡¯ç¤ºä½¿ç”¨è€…æ³¡æ³¡
    with st.chat_message("user"):
        if user_text:
            st.markdown(user_text)
        if images_for_history:
            for fn, thumb, _ in images_for_history:
                st.image(thumb, caption=fn, width=220)
        if docs_for_history:
            for fn in docs_for_history:
                st.caption(f"ğŸ“ {fn}")

    # å¯«å…¥æ­·å²
    ensure_session_defaults()
    st.session_state.chat_history.append({
        "role": "user",
        "text": user_text,
        "images": images_for_history,
        "docs": docs_for_history
    })

    # å»ºç«‹çŸ­æœŸè¨˜æ†¶ï¼ˆæ­·å²ï¼‹æœ¬æ¬¡è¨Šæ¯ï¼‰
    trimmed_messages = build_trimmed_input_messages(content_blocks)

    # åŠ©ç†å€å¡Š
    with st.chat_message("assistant"):
        status_area = st.container()
        output_area = st.container()
        sources_container = st.container()

        try:
            with status_area:
                with st.status("âš¡ æ€è€ƒä¸­...", expanded=False) as status:
                    placeholder = output_area.empty()

                    # å‰ç½® Routerï¼šæ±ºå®š fast / general / research
                    fr_result = run_front_router(client, trimmed_messages, user_text)
                    kind = fr_result.get("kind")
                    args = fr_result.get("args", {}) or {}

                    # === Fast åˆ†æ”¯ï¼šFastAgent + streaming ===
                    if kind == "fast":
                        status.update(label="âš¡ ä½¿ç”¨å¿«é€Ÿå›ç­”æ¨¡å¼", state="running", expanded=False)
                        fast_query = args.get("query") or user_text or "è«‹æ ¹æ“šå°è©±å…§å®¹å›ç­”ã€‚"

                        # ä½¿ç”¨ Agents SDK çš„ streaming ä»‹é¢
                        final_text = run_fast_agent_streamed(fast_query, placeholder)

                        # Fast æ¨¡å¼é€šå¸¸ä¸æœƒæœ‰ä¾†æºï¼Œä½†è‹¥æœ‰ä¸Šå‚³æª”æ¡ˆä»å¯åˆ—å‡º
                        with sources_container:
                            if docs_for_history:
                                st.markdown("**æœ¬å›åˆä¸Šå‚³æª”æ¡ˆ**")
                                for fn in docs_for_history:
                                    st.markdown(f"- {fn}")

                        ensure_session_defaults()
                        st.session_state.chat_history.append({
                            "role": "assistant",
                            "text": final_text,
                            "images": [],
                            "docs": []
                        })
                        status.update(label="âœ… å¿«é€Ÿå›ç­”å®Œæˆ", state="complete", expanded=False)
                        st.stop()

                    # === General åˆ†æ”¯ï¼šgptâ€‘5.1 + ANYA_SYSTEM_PROMPT + web_searchï¼ˆå¯é¸ï¼‰ ===
                    if kind == "general":
                        status.update(label="â†—ï¸ åˆ‡æ›åˆ°æ·±æ€æ¨¡å¼ï¼ˆgptâ€‘5.1ï¼‰", state="running", expanded=False)
                        need_web = bool(args.get("need_web"))
                        resp = client.responses.create(
                            model="gpt-5.1-2025-11-13",
                            input=trimmed_messages,
                            reasoning={ "effort": "medium" },
                            instructions=ANYA_SYSTEM_PROMPT,
                            tools=[{"type": "web_search"}] if need_web else [],
                            tool_choice="auto",
                        )
                        ai_text, url_cits, file_cits = parse_response_text_and_citations(resp)
                        final_text = fake_stream_markdown(ai_text, placeholder)
                        status.update(label="âœ… æ·±æ€æ¨¡å¼å®Œæˆ", state="complete", expanded=False)

                        with sources_container:
                            if url_cits:
                                st.markdown("**ä¾†æº**")
                                for c in url_cits:
                                    title = c.get("title") or c.get("url")
                                    url = c.get("url")
                                    st.markdown(f"- [{title}]({url})")
                            if file_cits:
                                st.markdown("**å¼•ç”¨æª”æ¡ˆ**")
                                for c in file_cits:
                                    fname = c.get("filename") or c.get("file_id") or "(æœªçŸ¥æª”å)"
                                    st.markdown(f"- {fname}")
                            if not file_cits and docs_for_history:
                                st.markdown("**æœ¬å›åˆä¸Šå‚³æª”æ¡ˆ**")
                                for fn in docs_for_history:
                                    st.markdown(f"- {fn}")

                        ensure_session_defaults()
                        st.session_state.chat_history.append({
                            "role": "assistant",
                            "text": final_text,
                            "images": [],
                            "docs": []
                        })
                        st.stop()

                    # === Research åˆ†æ”¯ï¼šPlanner â†’ SearchAgent â†’ Writer ===
                    if kind == "research":
                        status.update(label="â†—ï¸ åˆ‡æ›åˆ°ç ”ç©¶æµç¨‹ï¼ˆè¦åŠƒâ†’æœå°‹â†’å¯«ä½œï¼‰", state="running", expanded=True)

                        # 1) Planner
                        plan_query = args.get("query") or user_text
                        plan_res = run_async(Runner.run(planner_agent, plan_query))
                        search_plan = plan_res.final_output.searches if hasattr(plan_res, "final_output") else []

                        # 2) é¡¯ç¤ºè¦åŠƒï¼‹ä¸¦è¡Œæœå°‹æ‘˜è¦
                        with output_area:
                            with st.expander("ğŸ” æœå°‹è¦åŠƒèˆ‡å„é …æœå°‹æ‘˜è¦", expanded=True):
                                st.markdown("### æœå°‹è¦åŠƒ")
                                for i, it in enumerate(search_plan):
                                    st.markdown(f"**{i+1}. {it.query}**\n> {it.reason}")
                                st.markdown("### å„é …æœå°‹æ‘˜è¦")

                                body_placeholders = []
                                for i, it in enumerate(search_plan):
                                    sec = st.container()
                                    sec.markdown(f"**{it.query}**")
                                    body_placeholders.append(sec.empty())

                                search_results = run_async(aparallel_search_stream(
                                    search_agent,
                                    search_plan,
                                    body_placeholders,
                                    per_task_timeout=90,
                                    max_concurrency=4,
                                    retries=1,
                                    retry_delay=1.0,
                                ))

                                summary_texts = []
                                for r in search_results:
                                    if isinstance(r, Exception):
                                        summary_texts.append(f"ï¼ˆè©²æ¢æœå°‹å¤±æ•—ï¼š{r}ï¼‰")
                                    else:
                                        summary_texts.append(str(getattr(r, "final_output", "") or r or ""))

                        # 3) Writer
                        trimmed_messages_no_guard = strip_page_guard(trimmed_messages)
                        search_for_writer = [
                            {"query": search_plan[i].query, "summary": summary_texts[i]}
                            for i in range(len(search_plan))
                        ]
                        writer_data, writer_url_cits, writer_file_cits = run_writer(
                            client, trimmed_messages_no_guard, plan_query, search_for_writer
                        )

                        with output_area:
                            summary_sec = st.container()
                            summary_sec.markdown("### ğŸ“‹ Executive Summary")
                            fake_stream_markdown(writer_data.get("short_summary", ""), summary_sec.empty())

                            report_sec = st.container()
                            report_sec.markdown("### ğŸ“– å®Œæ•´å ±å‘Š")
                            fake_stream_markdown(writer_data.get("markdown_report", ""), report_sec.empty())

                            q_sec = st.container()
                            q_sec.markdown("### â“ å¾ŒçºŒå»ºè­°å•é¡Œ")
                            for q in writer_data.get("follow_up_questions", []) or []:
                                q_sec.markdown(f"- {q}")

                        with sources_container:
                            if writer_url_cits:
                                st.markdown("**ä¾†æº**")
                                seen = set()
                                for c in writer_url_cits:
                                    url = c.get("url")
                                    if url and url not in seen:
                                        seen.add(url)
                                        title = c.get("title") or url
                                        st.markdown(f"- [{title}]({url})")
                            if writer_file_cits:
                                st.markdown("**å¼•ç”¨æª”æ¡ˆ**")
                                for c in writer_file_cits:
                                    fname = c.get("filename") or c.get("file_id") or "(æœªçŸ¥æª”å)"
                                    st.markdown(f"- {fname}")
                            if not writer_file_cits and docs_for_history:
                                st.markdown("**æœ¬å›åˆä¸Šå‚³æª”æ¡ˆ**")
                                for fn in docs_for_history:
                                    st.markdown(f"- {fn}")

                        ai_reply = (
                            "#### Executive Summary\n" + (writer_data.get("short_summary", "") or "") + "\n" +
                            "#### å®Œæ•´å ±å‘Š\n" + (writer_data.get("markdown_report", "") or "") + "\n" +
                            "#### å¾ŒçºŒå»ºè­°å•é¡Œ\n" + "\n".join([f"- {q}" for q in writer_data.get("follow_up_questions", []) or []])
                        )
                        ensure_session_defaults()
                        st.session_state.chat_history.append({
                            "role": "assistant",
                            "text": ai_reply,
                            "images": [],
                            "docs": []
                        })
                        status.update(label="âœ… ç ”ç©¶æµç¨‹å®Œæˆ", state="complete", expanded=False)
                        st.stop()

                    # === è‹¥ Router æ²’çµ¦å‡º kindï¼ˆæ¥µå°‘æ•¸ï¼‰ï¼Œå›é€€èˆŠ Router æµç¨‹ ===
                    status.update(label="â†©ï¸ å›é€€è‡³èˆŠ Router æ±ºç­–ä¸­â€¦", state="running", expanded=True)

                    async def arouter_decide(router_agent, text: str):
                        return await Runner.run(router_agent, text)

                    router_result = run_async(arouter_decide(router_agent, user_text))

                    if isinstance(router_result.final_output, WebSearchPlan):
                        search_plan = router_result.final_output.searches

                        with output_area:
                            with st.expander("ğŸ” æœå°‹è¦åŠƒèˆ‡å„é …æœå°‹æ‘˜è¦", expanded=True):
                                st.markdown("### æœå°‹è¦åŠƒ")
                                for i, it in enumerate(search_plan):
                                    st.markdown(f"**{i+1}. {it.query}**\n> {it.reason}")
                                st.markdown("### å„é …æœå°‹æ‘˜è¦")

                                body_placeholders = []
                                for i, it in enumerate(search_plan):
                                    sec = st.container()
                                    sec.markdown(f"**{it.query}**")
                                    body_placeholders.append(sec.empty())

                                search_results = run_async(aparallel_search_stream(
                                    search_agent,
                                    search_plan,
                                    body_placeholders,
                                    per_task_timeout=90,
                                    max_concurrency=4,
                                    retries=1,
                                    retry_delay=1.0,
                                ))
                                summary_texts = []
                                for r in search_results:
                                    if isinstance(r, Exception):
                                        summary_texts.append(f"ï¼ˆè©²æ¢æœå°‹å¤±æ•—ï¼š{r}ï¼‰")
                                    else:
                                        summary_texts.append(str(getattr(r, "final_output", "") or r or ""))

                        search_for_writer = [
                            {"query": search_plan[i].query, "summary": summary_texts[i]}
                            for i in range(len(search_plan))
                        ]

                        trimmed_messages_no_guard = strip_page_guard(trimmed_messages)
                        writer_data, writer_url_cits, writer_file_cits = run_writer(
                            client, trimmed_messages_no_guard, user_text, search_for_writer
                        )

                        with output_area:
                            summary_sec = st.container()
                            summary_sec.markdown("### ğŸ“‹ Executive Summary")
                            fake_stream_markdown(writer_data.get("short_summary", ""), summary_sec.empty())

                            report_sec = st.container()
                            report_sec.markdown("### ğŸ“– å®Œæ•´å ±å‘Š")
                            fake_stream_markdown(writer_data.get("markdown_report", ""), report_sec.empty())

                            q_sec = st.container()
                            q_sec.markdown("### â“ å¾ŒçºŒå»ºè­°å•é¡Œ")
                            for q in writer_data.get("follow_up_questions", []) or []:
                                q_sec.markdown(f"- {q}")

                        with sources_container:
                            if writer_url_cits:
                                st.markdown("**ä¾†æº**")
                                seen = set()
                                for c in writer_url_cits:
                                    url = c.get("url")
                                    if url and url not in seen:
                                        seen.add(url)
                                        title = c.get("title") or url
                                        st.markdown(f"- [{title}]({url})")
                            if writer_file_cits:
                                st.markdown("**å¼•ç”¨æª”æ¡ˆ**")
                                for c in writer_file_cits:
                                    fname = c.get("filename") or c.get("file_id") or "(æœªçŸ¥æª”å)"
                                    st.markdown(f"- {fname}")
                            if not writer_file_cits and docs_for_history:
                                st.markdown("**æœ¬å›åˆä¸Šå‚³æª”æ¡ˆ**")
                                for fn in docs_for_history:
                                    st.markdown(f"- {fn}")

                        ai_reply = (
                            "#### Executive Summary\n" + (writer_data.get("short_summary", "") or "") + "\n" +
                            "#### å®Œæ•´å ±å‘Š\n" + (writer_data.get("markdown_report", "") or "") + "\n" +
                            "#### å¾ŒçºŒå»ºè­°å•é¡Œ\n" + "\n".join([f"- {q}" for q in writer_data.get("follow_up_questions", []) or []])
                        )
                        ensure_session_defaults()
                        st.session_state.chat_history.append({
                            "role": "assistant",
                            "text": ai_reply,
                            "images": [],
                            "docs": []
                        })
                        status.update(label="âœ… å›é€€æµç¨‹å®Œæˆ", state="complete", expanded=False)

                    else:
                        resp = client.responses.create(
                            model="gpt-5",
                            input=trimmed_messages,
                            instructions=ANYA_SYSTEM_PROMPT,
                            tools=[{"type": "web_search"}],
                            tool_choice="auto",
                        )
                        ai_text, url_cits, file_cits = parse_response_text_and_citations(resp)
                        final_text = fake_stream_markdown(ai_text, output_area.empty())

                        with sources_container:
                            if url_cits:
                                st.markdown("**ä¾†æº**")
                                for c in url_cits:
                                    title = c.get("title") or c.get("url")
                                    url = c.get("url")
                                    st.markdown(f"- [{title}]({url})")
                            if file_cits:
                                st.markdown("**å¼•ç”¨æª”æ¡ˆ**")
                                for c in file_cits:
                                    fname = c.get("filename") or c.get("file_id") or "(æœªçŸ¥æª”å)"
                                    st.markdown(f"- {fname}")
                            if not file_cits and docs_for_history:
                                st.markdown("**æœ¬å›åˆä¸Šå‚³æª”æ¡ˆ**")
                                for fn in docs_for_history:
                                    st.markdown(f"- {fn}")

                        ensure_session_defaults()
                        st.session_state.chat_history.append({
                            "role": "assistant",
                            "text": final_text,
                            "images": [],
                            "docs": []
                        })
                        status.update(label="âœ… å›é€€æµç¨‹å®Œæˆ", state="complete", expanded=False)

        except Exception as e:
            with status_area:
                st.status(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}", state="error", expanded=True)
            import traceback
            st.code(traceback.format_exc())
