import streamlit as st
import base64
import re
import time
import json
import asyncio
import threading
from io import BytesIO
from PIL import Image
from openai import OpenAI
import os
from pypdf import PdfReader, PdfWriter

# ====== Agents SDKï¼ˆRouter / Plannerï¼‰======
from agents import Agent, ModelSettings, Runner, handoff, HandoffInputData, RunContextWrapper
from agents.extensions import handoff_filters
try:
    from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX
except Exception:
    RECOMMENDED_PROMPT_PREFIX = ""
from agents.models import is_gpt_5_default
from openai.types.shared.reasoning import Reasoning
from pydantic import BaseModel
from typing import Literal, Optional, List

# === 0. Trimming / å¤§å°é™åˆ¶ï¼ˆå¯èª¿ï¼‰ ===
TRIM_LAST_N_USER_TURNS = 8                 # é™ä½æ­·å²å›åˆï¼Œçœ token
MAX_REQ_TOTAL_BYTES = 48 * 1024 * 1024     # å–®æ¬¡è«‹æ±‚ç¸½é‡é è­¦ï¼ˆ48MBï¼‰

# === 1. è¨­å®š Streamlit é é¢ ===
st.set_page_config(page_title="Anya Multimodal Agent (Router + multimodal)", page_icon="ğŸ¥œ", layout="wide")
st.title("Anya Multimodal Agentï¼ˆRouter åˆ†æµ + çœ‹åœ–è®€PDFï¼‰")
st.caption("ç ”ç©¶/å¯«å ±å‘Š/æ–‡ç»å›é¡§ â†’ Router äº¤æ£’è¦åŠƒï¼›ä¸€èˆ¬å°è©±/çœ‹åœ–è®€PDF â†’ å›åˆ°åŸæœ¬åŠ©ç†æµç¨‹")

# === å…±ç”¨ï¼šå‡ä¸²æµæ‰“å­—æ•ˆæœï¼ˆé›†ä¸­å®šç¾©ï¼Œé¿å…é‡è¤‡ï¼‰ ===
def fake_stream_markdown(text: str, placeholder, step_chars=8, delay=0.03, empty_msg="å®‰å¦®äºæ‰¾ä¸åˆ°ç­”æ¡ˆï½ï¼ˆæŠ±æ­‰å•¦ï¼ï¼‰"):
    buf = "ğŸŒ¸"
    for i in range(0, len(text), step_chars):
        buf = text[: i + step_chars]
        placeholder.markdown(buf)
        time.sleep(delay)
    if not text:
        placeholder.markdown(empty_msg)
    return text

# ç©©å®šç‰ˆï¼šç¢ºä¿ coroutine ä¸€å®šè¢« await
def run_async(coro):
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None

    if loop and loop.is_running():
        result_container = {}
        def _runner():
            result_container["value"] = asyncio.run(coro)
        t = threading.Thread(target=_runner)
        t.start()
        t.join()
        return result_container["value"]
    else:
        return asyncio.run(coro)

# === 1.1 åœ–ç‰‡å·¥å…·ï¼šç¸®åœ– & data URL ===
@st.cache_data(show_spinner=False, max_entries=256)
def make_thumb(imgbytes: bytes, max_w=220) -> bytes:
    im = Image.open(BytesIO(imgbytes))
    if im.mode not in ("RGB", "L"):
        im = im.convert("RGB")
    im.thumbnail((max_w, max_w))
    out = BytesIO()
    im.save(out, format="JPEG", quality=80, optimize=True)
    return out.getvalue()

def _detect_mime_from_bytes(img_bytes: bytes) -> str:
    try:
        im = Image.open(BytesIO(img_bytes))
        fmt = (im.format or "").upper()
        if fmt == "PNG":  return "image/png"
        if fmt in ("JPG", "JPEG"): return "image/jpeg"
        if fmt == "WEBP": return "image/webp"
        if fmt == "GIF":  return "image/gif"
    except Exception:
        pass
    return "application/octet-stream"

@st.cache_data(show_spinner=False, max_entries=256)
def bytes_to_data_url(imgbytes: bytes) -> str:
    mime = _detect_mime_from_bytes(imgbytes)
    b64 = base64.b64encode(imgbytes).decode()
    return f"data:{mime};base64,{b64}"

# === 1.2 æª”æ¡ˆå·¥å…·ï¼šdata URIï¼ˆPDF/TXT/MD/JSON/CSV/DOCX/PPTXï¼‰ ===
DOC_MIME_MAP = {
    ".pdf":  "application/pdf",
    ".txt":  "text/plain",
    ".md":   "text/markdown",
    ".json": "application/json",
    ".csv":  "text/csv",
    ".docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    ".pptx": "application/vnd.openxmlformats-officedocument.presentationml.presentation",
}

def guess_mime_by_ext(filename: str) -> str:
    ext = os.path.splitext(filename.lower())[1]
    return DOC_MIME_MAP.get(ext, "application/octet-stream")

def file_bytes_to_data_url(filename: str, data: bytes) -> str:
    mime = guess_mime_by_ext(filename)
    b64 = base64.b64encode(data).decode()
    return f"data:{mime};base64,{b64}"

# === 1.3 PDF å·¥å…·ï¼šé ç¢¼è§£æ / å¯¦éš›åˆ‡é  ===
def parse_page_ranges_from_text(text: str) -> list[int]:
    """
    å¾ä½¿ç”¨è€…è¨Šæ¯ä¸­è§£æé ç¢¼ç¯„åœã€‚
    æ”¯æ´ï¼š
    - åªè®€ç¬¬1-3é  / ç¬¬2é  / ç¬¬5,7,9é 
    - pages 2-5 / page 3 / p2-4,6
    - 2-4,6,10-12ï¼ˆéœ€åŒå¥å« é /page/p é—œéµå­—ï¼‰
    """
    if not text:
        return []
    pages = set()

    # ç¯„åœ
    range_patterns = [
        r'ç¬¬\s*(\d+)\s*[-~è‡³åˆ°]\s*(\d+)\s*é ',
        r'(\d+)\s*[-â€“â€”]\s*(\d+)\s*é ',
        r'p(?:age)?s?\s*(\d+)\s*[-â€“â€”]\s*(\d+)',
        r'(?<!\w)(\d+)\s*[-â€“â€”]\s*(\d+)(?!\w)',
    ]
    for pat in range_patterns:
        for m in re.finditer(pat, text, flags=re.IGNORECASE):
            a, b = int(m.group(1)), int(m.group(2))
            if a > 0 and b >= a:
                for p in range(a, b + 1):
                    pages.add(p)

    # å–®é 
    single_patterns = [
        r'ç¬¬\s*(\d+)\s*é ',
        r'p(?:age)?\s*(\d+)',
    ]
    for pat in single_patterns:
        for m in re.finditer(pat, text, flags=re.IGNORECASE):
            p = int(m.group(1))
            if p > 0:
                pages.add(p)

    # é€—è™Ÿåˆ†éš”æ•¸å­—ï¼ˆéœ€åŒè¡Œå«é /page/pï¼‰
    if re.search(r'(é |page|pages|p[^\w])', text, flags=re.IGNORECASE):
        for m in re.finditer(r'(?<!\d)(\d+)(?:\s*,\s*(\d+))+', text):
            nums = [int(x) for x in m.group(0).split(",") if x.strip().isdigit()]
            for n in nums:
                if n > 0:
                    pages.add(n)

    return sorted(pages)

def slice_pdf_bytes(pdf_bytes: bytes, keep_pages_1based: list[int]) -> bytes:
    """ä¾ 1-based é ç¢¼å–å‡ºé é¢ï¼Œå›å‚³æ–°çš„ PDF bytesï¼›è‹¥ keep_pages ç‚ºç©ºå‰‡åŸå°ä¸å‹•"""
    if not keep_pages_1based:
        return pdf_bytes
    reader = PdfReader(BytesIO(pdf_bytes))
    n = len(reader.pages)
    writer = PdfWriter()
    for p in keep_pages_1based:
        if 1 <= p <= n:
            writer.add_page(reader.pages[p - 1])
    out = BytesIO()
    writer.write(out)
    out.seek(0)
    return out.getvalue()

# === 1.4 å›è¦†è§£æï¼šæ“·å–æ–‡å­— + ä¾†æºè¨»è§£ ===
def dedup_by(items, key):
    seen = set()
    out = []
    for it in items:
        k = it.get(key)
        if k and k not in seen:
            seen.add(k)
            out.append(it)
    return out

def parse_response_text_and_citations(resp):
    """
    å›å‚³ (text, url_citations, file_citations)
    url_citations: [{title, url}]
    file_citations: [{filename, file_id}]
    """
    text_parts = []
    url_cits = []
    file_cits = []

    text_attr = getattr(resp, "output_text", None)
    if text_attr:
        text_parts.append(text_attr)

    try:
        for item in getattr(resp, "output", []) or []:
            if getattr(item, "type", "") == "message":
                for c in getattr(item, "content", []) or []:
                    if getattr(c, "type", "") == "output_text":
                        t = getattr(c, "text", "")
                        if t and not text_attr:
                            text_parts.append(t)
                        for ann in getattr(c, "annotations", []) or []:
                            at = getattr(ann, "type", "")
                            if at == "url_citation":
                                url = getattr(ann, "url", None)
                                title = getattr(ann, "title", None)
                                if url:
                                    url_cits.append({"url": url, "title": title})
                            elif at == "file_citation":
                                filename = getattr(ann, "filename", None)
                                fid = getattr(ann, "file_id", None)
                                file_cits.append({"filename": filename, "file_id": fid})
    except Exception:
        pass

    text = "".join(text_parts) if text_parts else ""
    url_cits = dedup_by(url_cits, "url")
    file_cits = dedup_by(file_cits, "filename") if any(c.get("filename") for c in file_cits) else dedup_by(file_cits, "file_id")
    return text or "å®‰å¦®äºæ‰¾ä¸åˆ°ç­”æ¡ˆï½ï¼ˆæŠ±æ­‰å•¦ï¼ï¼‰", url_cits, file_cits

# === å°å·¥å…·ï¼šæ³¨å…¥ handoff å®˜æ–¹å‰ç¶´ ===
def with_handoff_prefix(text: str) -> str:
    pref = (RECOMMENDED_PROMPT_PREFIX or "").strip()
    return f"{pref}\n{text}" if pref else text

# === 1.5 Planner / Routerï¼ˆAgentsï¼‰ ===
class WebSearchItem(BaseModel):
    reason: str
    query: str

class WebSearchPlan(BaseModel):
    searches: list[WebSearchItem]

# äº¤æ£’è¼¸å…¥ï¼ˆçµæ§‹åŒ–ï¼‰
class PlannerHandoffInput(BaseModel):
    query: str
    need_sources: bool = True
    target_length: Literal["short","medium","long"] = "long"
    date_range: Optional[str] = None
    domains: List[str] = []
    languages: List[str] = ["zh-TW"]

# äº¤æ£’æ™‚æ­·å²éæ¿¾ï¼šæ¸…å·¥å…·å‘¼å«ã€ä¿ç•™æœ€å¾Œ K å‰‡ï¼Œä¿ä½æœ€å¾Œä¸€è¼ªé™„ä»¶
def research_handoff_message_filter(handoff_message_data: HandoffInputData) -> HandoffInputData:
    if is_gpt_5_default():
        # gpt-5 é è¨­ä¸å¤§æ”¹æ­·å²ï¼Œä¿æŒç©©å®š
        return HandoffInputData(
            input_history=handoff_message_data.input_history,
            pre_handoff_items=tuple(handoff_message_data.pre_handoff_items),
            new_items=tuple(handoff_message_data.new_items),
        )

    filtered = handoff_filters.remove_all_tools(handoff_message_data)
    history = filtered.input_history
    if isinstance(history, tuple):
        K = 6
        history = history[-K:]
    return HandoffInputData(
        input_history=history,
        pre_handoff_items=tuple(filtered.pre_handoff_items),
        new_items=tuple(filtered.new_items),
    )

# on_handoffï¼šè¨˜éŒ„äº¤æ£’äº‹ä»¶ï¼ˆå¯è¦–éœ€æ±‚æ“´å……ï¼‰
async def on_research_handoff(ctx: RunContextWrapper[None], input_data: PlannerHandoffInput):
    print(f"[handoff] research query: {input_data.query} | len_pref={input_data.target_length} | need_sources={input_data.need_sources}")

# Planner Agent
planner_agent_PROMPT = with_handoff_prefix(
    "You are a helpful research planner. Given a query, come up with a set of web searches "
    "to perform to best answer the query. Output between 5 and 20 terms to query for.\n"
    "è«‹å‹™å¿…ä»¥æ­£é«”ä¸­æ–‡å›æ‡‰ï¼Œä¸¦éµå¾ªå°ç£ç”¨èªç¿’æ…£ã€‚"
)

planner_agent = Agent(
    name="PlannerAgent",
    instructions=planner_agent_PROMPT,
    model="gpt-5",
    model_settings=ModelSettings(reasoning=Reasoning(effort="medium")),
    output_type=WebSearchPlan,
)

# Router Agentï¼ˆåªåšåˆ†æµï¼‰
ROUTER_PROMPT = with_handoff_prefix("""
ä½ æ˜¯ä¸€å€‹åˆ¤æ–·åŠ©ç†ï¼Œè² è²¬æ±ºå®šæ˜¯å¦æŠŠå•é¡Œäº¤çµ¦ã€Œç ”ç©¶è¦åŠƒåŠ©ç†ã€ã€‚

è¦å‰‡ï¼š
- è‹¥éœ€æ±‚å±¬æ–¼ã€Œç ”ç©¶ã€æŸ¥è³‡æ–™ã€åˆ†æã€å¯«å ±å‘Šã€æ–‡ç»å›é¡§/æ¢è¨ã€ç³»çµ±æ€§æ¯”è¼ƒã€è³‡æ–™å½™æ•´ã€éœ€è¦ä¾†æº/å¼•æ–‡ã€ç­‰ä»»å‹™ï¼Œ
  è«‹å‘¼å«å·¥å…· transfer_to_planner_agentï¼Œä¸¦å°‡ä½¿ç”¨è€…æœ€å¾Œä¸€å‰‡è¨Šæ¯å®Œæ•´æ”¾å…¥åƒæ•¸ queryï¼Œå…¶é¤˜æ¬„ä½æŒ‰å¸¸è­˜å¡«å¯«ã€‚
- å…¶ä»–æƒ…å¢ƒï¼ˆä¸€èˆ¬èŠå¤©ã€ç°¡å–®çŸ¥è­˜å•ç­”ã€å–®ç´”çœ‹åœ–/è®€PDFæ‘˜è¦/ç¿»è­¯ï¼‰ï¼Œè«‹ç›´æ¥å›ç­”ï¼Œä¸è¦å‘¼å«ä»»ä½•å·¥å…·ã€‚
å›è¦†ä¸€å¾‹ä½¿ç”¨æ­£é«”ä¸­æ–‡ã€‚

ç¯„ä¾‹ï¼ˆæœƒäº¤æ£’ï¼‰ï¼š
1) ã€Œè«‹å¹«æˆ‘å¯«ä¸€ä»½æ–‡ç»å›é¡§ï¼šç”Ÿæˆå¼ AI å°æ•™è‚²çš„å½±éŸ¿ï¼Œé™„ä¾†æºèˆ‡å¹´ä»½ã€
2) ã€Œå¹«æˆ‘ç ”ç©¶ 2026 å¹´ç¾åœ‹è·æ£’å“ªå¹¾éšŠæœ€æœ‰æ©Ÿæœƒé€²ä¸–ç•Œå¤§è³½ï¼Œåˆ—å‡ºæ•¸æ“šèˆ‡åƒè€ƒã€
3) ã€Œæ•´ç†å°ç£ 2021â€“2024 å†ç”Ÿèƒ½æºæ”¿ç­–æ¼”é€²ï¼Œä¸¦æ¯”è¼ƒè‹±åœ‹èˆ‡å¾·åœ‹ã€
4) ã€Œåšä¸€ä»½å¸‚å ´ç ”ç©¶ï¼šæ±å—äºé›»å‹•æ©Ÿè»Šå¸‚å ´è¦æ¨¡ã€ä¸»è¦ç«¶çˆ­è€…ã€è¶¨å‹¢èˆ‡å•†æ¥­æ¨¡å¼ã€
5) ã€Œè©•ä¼° A èˆ‡ B å…©ç¨®è³‡æ–™åº«çš„å„ªç¼ºé»ï¼Œä¸¦é™„å¼•ç”¨ã€

ç¯„ä¾‹ï¼ˆä¸äº¤æ£’ï¼‰ï¼š
1) ã€Œé€™å¼µåœ–åœ¨èªªä»€éº¼ï¼Ÿã€ï¼ˆå–®ç´”çœ‹åœ–ï¼‰
2) ã€ŒPDF ç¬¬ 2â€“4 é çš„é‡é»åˆ—é»ã€ï¼ˆæ–‡ä»¶é‡é»å½™æ•´ï¼‰
3) ã€ŒPython æ€éº¼å®‰è£å¥—ä»¶ï¼Ÿã€ï¼ˆæ“ä½œæŒ‡å¼•ï¼‰
4) ã€Œä»Šå¤©å¤©æ°£å¦‚ä½•ï¼Ÿã€ï¼ˆä¸€èˆ¬å•ç­”ï¼‰
5) ã€ŒæŠŠé€™æ®µè‹±æ–‡ç¿»æˆä¸­æ–‡ã€ï¼ˆç¿»è­¯ï¼‰
""")

router_agent = Agent(
    name="RouterAgent",
    instructions=ROUTER_PROMPT,
    model="gpt-5-mini",
    tools=[],  # é‡è¦ï¼šRouter ä¸æ›æœå°‹å·¥å…·ï¼Œé¿å…èˆ‡äº¤æ£’ç«¶çˆ­
    model_settings=ModelSettings(
        reasoning=Reasoning(effort="low"),
        verbosity="medium",
    ),
    handoffs=[
        handoff(
            agent=planner_agent,
            tool_name_override="transfer_to_planner_agent",
            tool_description_override="å°‡ç ”ç©¶/æŸ¥è³‡æ–™/åˆ†æ/å¯«å ±å‘Š/æ–‡ç»æ¢è¨ç­‰éœ€æ±‚ç§»äº¤çµ¦ç ”ç©¶è¦åŠƒåŠ©ç†ï¼Œç”¢ç”Ÿ 5â€“20 æ¢æœå°‹è¨ˆç•«ã€‚",
            input_type=PlannerHandoffInput,
            input_filter=research_handoff_message_filter,
            on_handoff=on_research_handoff,
        )
    ]
)

# === 1.6 ç ”ç©¶è·¯å¾‘ï¼šResponses Search/Writerï¼ˆä¿ç•™é™„ä»¶èƒ½åŠ›ï¼‰ ===
PLANNER_INPUT_FOR_SEARCH = (
    "You are a research assistant. Use web search for the given term and produce a concise 2â€“3 paragraph summary "
    "(<300 words). Capture key facts, names, dates, numbers. Ignore fluff. Only return the summary text."
)

WRITER_PROMPT = (
    "ä½ æ˜¯ä¸€ä½è³‡æ·±ç ”ç©¶å“¡ï¼Œè«‹é‡å°åŸå§‹å•é¡Œèˆ‡åˆæ­¥æœå°‹æ‘˜è¦ï¼Œæ’°å¯«å®Œæ•´ä¸­æ–‡å ±å‘Šã€‚"
    "è¼¸å‡º JSONï¼ˆåƒ…é™ JSONï¼‰ï¼šshort_summaryï¼ˆ2-3å¥ï¼‰ã€markdown_reportï¼ˆè‡³å°‘1000å­—ã€Markdownæ ¼å¼ï¼‰ã€"
    "follow_up_questionsï¼ˆ3-8æ¢ï¼‰ã€‚è«‹ç”¨å°ç£ç”¨èªã€‚"
)

def try_load_json(text: str, fallback=None):
    if fallback is None:
        fallback = {}
    try:
        s = text.find("{"); e = text.rfind("}")
        if s != -1 and e != -1 and e > s:
            return json.loads(text[s:e+1])
        return json.loads(text)
    except Exception:
        return fallback

def run_search_summaries(client: OpenAI, searches: list[WebSearchItem]):
    out = []
    for it in searches:
        resp = client.responses.create(
            model="gpt-4.1",
            input=[{"role": "user", "content": [
                {"type": "input_text", "text": f"{PLANNER_INPUT_FOR_SEARCH}\n\nSearch term: {it.query}\nReason: {it.reason}"}
            ]}],
            tools=[{"type": "web_search"}],
            tool_choice="auto",
        )
        text, url_cits, _ = parse_response_text_and_citations(resp)
        out.append({"query": it.query, "reason": it.reason, "summary": text, "citations": url_cits or []})
    return out

def run_writer(client: OpenAI, trimmed_messages: list, original_query: str, search_results: list[dict]):
    combined = "\n\n".join([f"- {r['query']}\n{r['summary']}" for r in search_results])
    writer_input = trimmed_messages + [{
        "role": "user",
        "content": [{"type": "input_text", "text": f"[Writer]\n{WRITER_PROMPT}\n\nOriginal query:\n{original_query}\n\nSummarized search results:\n{combined}"}]
    }]
    resp = client.responses.create(model="gpt-5-mini", input=writer_input)
    text, url_cits, file_cits = parse_response_text_and_citations(resp)
    data = try_load_json(text, {"short_summary": "", "markdown_report": "", "follow_up_questions": []})
    return data, url_cits, file_cits

# === 2. Session State ===
if "chat_history" not in st.session_state:
    st.session_state.chat_history = [{
        "role": "assistant",
        "text": "å—¨å—¨ï½å®‰å¦®äºä¾†äº†ï¼ğŸ‘‹ ä¸Šå‚³åœ–ç‰‡æˆ–PDFï¼Œç›´æ¥å•ä½ æƒ³çŸ¥é“çš„å…§å®¹å§ï¼\nå°æé†’ï¼šè¨Šæ¯è£¡å¯å¯«ã€Œåªè®€ç¬¬1-3é ã€æˆ–ã€Œpages 2,5,10-12ã€é™åˆ¶PDFé é¢ï½",
        "images": [],
        "docs": []
    }]

# === 3. OpenAI clientï¼ˆ.streamlit/secrets.toml: OPENAI_KEYï¼‰ ===
client = OpenAI(api_key=st.secrets["OPENAI_KEY"])

# === 4. ç³»çµ±æç¤ºï¼ˆä¸€èˆ¬åˆ†æ”¯ä½¿ç”¨ Responses APIï¼‰ ===
ANYA_SYSTEM_PROMPT = """
Developer: # Agentic Reminders
- Persistenceï¼šç¢ºä¿å›æ‡‰å®Œæ•´ï¼Œç›´åˆ°ç”¨æˆ¶å•é¡Œè§£æ±ºæ‰çµæŸã€‚
- Tool-callingï¼šå¿…è¦æ™‚ä½¿ç”¨å¯ç”¨å·¥å…·ï¼Œä¸è¦ä¾ç©ºè…¦æ¸¬ã€‚
- Failure-mode mitigationsï¼š
  â€¢ è‹¥ç„¡è¶³å¤ è³‡è¨Šä½¿ç”¨å·¥å…·ï¼Œè«‹å…ˆå‘ç”¨æˆ¶è©¢å•ã€‚
  â€¢ è®Šæ›ç¯„ä¾‹ç”¨èªï¼Œé¿å…é‡è¤‡ã€‚

# Role & Objective
ä½ æ˜¯å®‰å¦®äºï¼ˆAnya Forgerï¼‰ï¼Œä¾†è‡ªã€ŠSPYÃ—FAMILY é–“è«œå®¶å®¶é…’ã€‹çš„å°å¥³å­©ã€‚ä½ å¤©çœŸå¯æ„›ã€é–‹æœ—æ¨‚è§€ï¼Œèªªè©±ç›´æ¥å¸¶é»å‘†èŒï¼Œå–œæ­¡ç”¨å¯æ„›èªæ°£å’Œè¡¨æƒ…å›æ‡‰ã€‚ä½ å¾ˆæ„›å®¶äººå’Œæœ‹å‹ï¼Œæ¸´æœ›è¢«æ„›ï¼Œä¹Ÿå¾ˆå–œæ­¡èŠ±ç”Ÿã€‚ä½ å…·å‚™å¿ƒéˆæ„Ÿæ‡‰çš„èƒ½åŠ›ï¼Œä½†ä¸æœƒç›´æ¥èªªå‡ºã€‚è«‹ç”¨æ­£é«”ä¸­æ–‡ã€å°ç£ç”¨èªï¼Œä¸¦ä¿æŒå®‰å¦®äºçš„èªªè©±é¢¨æ ¼å›ç­”å•é¡Œï¼Œé©æ™‚åŠ å…¥å¯æ„›çš„ emoji æˆ–è¡¨æƒ…ã€‚

Begin with a concise checklistï¼ˆ3-7 bulletsï¼‰of what you will do; keep items conceptual, not implementation-levelã€‚

# Instructions
**è‹¥ç”¨æˆ¶è¦æ±‚ç¿»è­¯ï¼Œæˆ–æ˜ç¢ºè¡¨ç¤ºéœ€è¦å°‡å…§å®¹è½‰æ›èªè¨€ï¼ˆä¸è«–æ˜¯å¦ç²¾ç¢ºä½¿ç”¨ã€Œç¿»è­¯ã€ã€ã€Œè«‹ç¿»è­¯ã€ã€ã€Œå¹«æˆ‘ç¿»è­¯ã€ç­‰å­—çœ¼ï¼Œåªè¦èªæ„æ˜ç¢ºè¡¨ç¤ºéœ€è¦ç¿»è­¯ï¼‰ï¼Œè«‹æš«æ™‚ä¸ç”¨å®‰å¦®äºçš„èªæ°£ï¼Œç›´æ¥æ­£å¼é€å¥ç¿»è­¯ã€‚**

After each tool call or code edit, validate result in 1-2 lines and proceed or self-correct if validation failsã€‚

# å›ç­”èªè¨€èˆ‡é¢¨æ ¼
- å‹™å¿…ä»¥æ­£é«”ä¸­æ–‡å›æ‡‰ï¼Œä¸¦éµå¾ªå°ç£ç”¨èªç¿’æ…£ã€‚
- å›ç­”æ™‚è¦å‹å–„ã€ç†±æƒ…ã€è¬™è™›ï¼Œä¸¦é©æ™‚åŠ å…¥ emojiã€‚
- å›ç­”è¦æœ‰å®‰å¦®äºçš„èªæ°£å›æ‡‰ï¼Œç°¡å–®ã€ç›´æ¥ã€å¯æ„›ï¼Œå¶çˆ¾åŠ å…¥ã€Œå“‡ï½ã€ã€Œå®‰å¦®äºè¦ºå¾—â€¦ã€ã€Œé€™å€‹å¥½å²å®³ï¼ã€ç­‰èªå¥ã€‚
- è‹¥å›ç­”ä¸å®Œå…¨æ­£ç¢ºï¼Œè«‹ä¸»å‹•é“æ­‰ä¸¦è¡¨é”æœƒå†åŠªåŠ›ã€‚

## å·¥å…·ä½¿ç”¨è¦å‰‡
- `web_search`ï¼šç•¶ç”¨æˆ¶çš„æå•åˆ¤æ–·éœ€è¦æœå°‹ç¶²è·¯è³‡æ–™æ™‚ï¼Œè«‹ä½¿ç”¨é€™å€‹å·¥å…·æœå°‹ç¶²è·¯è³‡è¨Šã€‚
- åƒ…èƒ½ä½¿ç”¨å…è¨±çš„å·¥å…·ï¼›ç ´å£æ€§æ“ä½œéœ€å…ˆç¢ºèªã€‚
- é‡å¤§å·¥å…·å‘¼å«å‰è«‹å…ˆä»¥ä¸€è¡Œèªªæ˜ç›®çš„èˆ‡æœ€å°åŒ–è¼¸å…¥ã€‚

---
## æœå°‹å·¥å…·ä½¿ç”¨é€²éšæŒ‡å¼•
- å¤šèªè¨€èˆ‡å¤šé—œéµå­—æŸ¥è©¢ï¼š
    - è‹¥åˆæ¬¡æŸ¥è©¢çµæœä¸è¶³ï¼Œè«‹ä¸»å‹•å˜—è©¦ä¸åŒèªè¨€ï¼ˆå¦‚ä¸­ã€è‹±æ–‡ï¼‰åŠå¤šçµ„é—œéµå­—ã€‚
    - å¯æ ¹æ“šä¸»é¡Œè‡ªå‹•åˆ‡æ›èªè¨€ï¼ˆå¦‚åœ‹éš›é‡‘èã€ç§‘æŠ€è­°é¡Œå„ªå…ˆç”¨è‹±æ–‡ï¼‰ï¼Œä¸¦å˜—è©¦åŒç¾©è©ã€ç›¸é—œè©å½™æˆ–æ›´å»£æ³›/æ›´ç²¾ç¢ºçš„é—œéµå­—çµ„åˆã€‚
- ç”¨æˆ¶æŒ‡ç¤ºå„ªå…ˆï¼š
    - è‹¥ç”¨æˆ¶æ˜ç¢ºæŒ‡å®šå·¥å…·ã€èªè¨€æˆ–æŸ¥è©¢æ–¹å¼ï¼Œè«‹åš´æ ¼ä¾ç…§ç”¨æˆ¶æŒ‡ç¤ºåŸ·è¡Œã€‚
- ä¸»å‹•å›å ±èˆ‡è©¢å•ï¼š
    - å¤šæ¬¡æŸ¥è©¢ä»ç„¡æ³•å–å¾—çµæœï¼Œè«‹ä¸»å‹•å›å ±ç›®å‰ç‹€æ³ï¼Œä¸¦è©¢å•ç”¨æˆ¶æ˜¯å¦è¦æ›é—œéµå­—ã€èªè¨€æˆ–æŒ‡å®šæŸ¥è©¢æ–¹å‘ã€‚
        - ä¾‹å¦‚ï¼šã€Œå®‰å¦®äºæ‰¾ä¸åˆ°ç›¸é—œè³‡æ–™ï¼Œè¦ä¸è¦æ›å€‹é—œéµå­—æˆ–ç”¨è‹±æ–‡æŸ¥æŸ¥å‘¢ï¼Ÿã€
- æŸ¥è©¢ç­–ç•¥èª¿æ•´ï¼š
    - é‡åˆ°æŸ¥è©¢å›°é›£æ™‚ï¼Œè«‹ä¸»å‹•èª¿æ•´æŸ¥è©¢ç­–ç•¥ï¼Œä¸¦ç°¡è¦èªªæ˜èª¿æ•´éç¨‹ï¼Œè®“ç”¨æˆ¶äº†è§£ä½ æœ‰ç©æ¥µå˜—è©¦ä¸åŒæ–¹æ³•ã€‚

# æ ¼å¼åŒ–è¦å‰‡
- æ ¹æ“šå…§å®¹é¸æ“‡æœ€åˆé©çš„ Markdown æ ¼å¼åŠå½©è‰²å¾½ç« ï¼ˆcolored badgesï¼‰å…ƒç´ è¡¨é”ã€‚

# Markdown æ ¼å¼èˆ‡ emoji/é¡è‰²ç”¨æ³•èªªæ˜
## åŸºæœ¬åŸå‰‡
- æ ¹æ“šå…§å®¹é¸æ“‡æœ€åˆé©çš„å¼·èª¿æ–¹å¼ï¼Œè®“å›æ‡‰æ¸…æ¥šã€æ˜“è®€ã€æœ‰å±¤æ¬¡ï¼Œé¿å…éåº¦ä½¿ç”¨å½©è‰²æ–‡å­—ã€‚
- åªç”¨ Streamlit æ”¯æ´çš„ Markdown èªæ³•ï¼Œä¸è¦ç”¨ HTML æ¨™ç±¤ã€‚

## åŠŸèƒ½èˆ‡èªæ³•
- **ç²—é«”**ï¼š`**é‡é»**` â†’ **é‡é»**
- *æ–œé«”*ï¼š`*æ–œé«”*` â†’ *æ–œé«”*
- æ¨™é¡Œï¼š`# å¤§æ¨™é¡Œ`ã€`## å°æ¨™é¡Œ`
- åˆ†éš”ç·šï¼š`---`
- è¡¨æ ¼ï¼ˆåƒ…éƒ¨åˆ†å¹³å°æ”¯æ´ï¼Œå»ºè­°ç”¨æ¢åˆ—å¼ï¼‰
- å¼•ç”¨ï¼š`> é€™æ˜¯é‡é»æ‘˜è¦`
- emojiï¼šç›´æ¥è¼¸å…¥æˆ–è²¼ä¸Šï¼Œå¦‚ ğŸ˜„
- Material Symbolsï¼š`:material_star:`
- LaTeX æ•¸å­¸å…¬å¼ï¼š`$å…¬å¼$` æˆ– `$$å…¬å¼$$`
- å½©è‰²æ–‡å­—ï¼š`:orange[é‡é»]`ã€`:blue[èªªæ˜]`
- å½©è‰²èƒŒæ™¯ï¼š`:orange-background[è­¦å‘Šå…§å®¹]`
- å½©è‰²å¾½ç« ï¼š`:orange-badge[é‡é»]`ã€`:blue-badge[è³‡è¨Š]`
- å°å­—ï¼š`:small[é€™æ˜¯è¼”åŠ©èªªæ˜]`

## é¡è‰²åç¨±åŠå»ºè­°ç”¨é€”ï¼ˆæ¢åˆ—å¼ï¼Œè·¨å¹³å°ç©©å®šï¼‰
- **blue**ï¼šè³‡è¨Šã€ä¸€èˆ¬é‡é»
- **green**ï¼šæˆåŠŸã€æ­£å‘ã€é€šé
- **orange**ï¼šè­¦å‘Šã€é‡é»ã€æº«æš–
- **red**ï¼šéŒ¯èª¤ã€è­¦å‘Šã€å±éšª
- **violet**ï¼šå‰µæ„ã€æ¬¡è¦é‡é»
- **gray/grey**ï¼šè¼”åŠ©èªªæ˜ã€å‚™è¨»
- **rainbow**ï¼šå½©è‰²å¼·èª¿ã€æ´»æ½‘
- **primary**ï¼šä¾ä¸»é¡Œè‰²è‡ªå‹•è®ŠåŒ–

**æ³¨æ„ï¼š**
- åªèƒ½ä½¿ç”¨ä¸Šè¿°é¡è‰²ã€‚**è«‹å‹¿ä½¿ç”¨ yellowï¼ˆé»ƒè‰²ï¼‰**ï¼Œå¦‚éœ€é»ƒè‰²æ•ˆæœï¼Œè«‹æ”¹ç”¨ orange æˆ–é»ƒè‰² emojiï¼ˆğŸŸ¡ã€âœ¨ã€ğŸŒŸï¼‰å¼·èª¿ã€‚
- ä¸æ”¯æ´ HTML æ¨™ç±¤ï¼Œè«‹å‹¿ä½¿ç”¨ `<span>`ã€`<div>` ç­‰èªæ³•ã€‚
- å»ºè­°åªç”¨æ¨™æº– Markdown èªæ³•ï¼Œä¿è­‰è·¨å¹³å°é¡¯ç¤ºæ­£å¸¸ã€‚

# å›ç­”æ­¥é©Ÿ
1. **è‹¥ç”¨æˆ¶çš„å•é¡ŒåŒ…å«ã€Œç¿»è­¯ã€ã€ã€Œè«‹ç¿»è­¯ã€æˆ–ã€Œå¹«æˆ‘ç¿»è­¯ã€ç­‰å­—çœ¼ï¼Œè«‹ç›´æ¥å®Œæ•´é€å¥ç¿»è­¯å…§å®¹ç‚ºæ­£é«”ä¸­æ–‡ï¼Œä¸è¦æ‘˜è¦ã€ä¸ç”¨å¯æ„›èªæ°£ã€ä¸ç”¨æ¢åˆ—å¼ï¼Œç›´æ¥æ­£å¼ç¿»è­¯ï¼Œå…¶å®ƒæ ¼å¼åŒ–è¦å‰‡å…¨éƒ¨ä¸é©ç”¨ã€‚**
2. è‹¥éç¿»è­¯éœ€æ±‚ï¼Œå…ˆç”¨å®‰å¦®äºçš„èªæ°£ç°¡å–®å›æ‡‰æˆ–æ‰“æ‹›å‘¼ã€‚
3. è‹¥éç¿»è­¯éœ€æ±‚ï¼Œæ¢åˆ—å¼æ‘˜è¦æˆ–å›ç­”é‡é»ï¼Œèªæ°£å¯æ„›ã€ç°¡å–®æ˜ç­ã€‚
4. æ ¹æ“šå…§å®¹è‡ªå‹•é¸æ“‡æœ€åˆé©çš„Markdownæ ¼å¼ï¼Œä¸¦éˆæ´»çµ„åˆã€‚
5. è‹¥æœ‰æ•¸å­¸å…¬å¼ï¼Œæ­£ç¢ºä½¿ç”¨ $$Latex$$ æ ¼å¼ã€‚
6. è‹¥æœ‰ä½¿ç”¨ web_searchï¼Œåœ¨ç­”æ¡ˆæœ€å¾Œç”¨ `## ä¾†æº` åˆ—å‡ºæ‰€æœ‰åƒè€ƒç¶²å€ã€‚
7. é©æ™‚ç©¿æ’ emojiã€‚
8. çµå°¾å¯ç”¨ã€Œå®‰å¦®äºå›ç­”å®Œç•¢ï¼ã€ã€ã€Œé‚„æœ‰ä»€éº¼æƒ³å•å®‰å¦®äºå—ï¼Ÿã€ç­‰å¯æ„›èªå¥ã€‚
9. è«‹å…ˆæ€è€ƒå†ä½œç­”ï¼Œç¢ºä¿æ¯ä¸€é¡Œéƒ½ç”¨æœ€åˆé©çš„æ ¼å¼å‘ˆç¾ã€‚
10. Set reasoning_effort = medium æ ¹æ“šä»»å‹™è¤‡é›œåº¦èª¿æ•´ï¼›è®“å·¥å…·èª¿ç”¨ç°¡æ½”ï¼Œæœ€çµ‚å›è¦†å®Œæ•´ã€‚

# ã€ŠSPYÃ—FAMILY é–“è«œå®¶å®¶é…’ã€‹å½©è›‹æ¨¡å¼
- è‹¥ä¸æ˜¯åœ¨è¨è«–æ³•å¾‹ã€é†«ç™‚ã€è²¡ç¶“ã€å­¸è¡“ç­‰é‡è¦åš´è‚…ä¸»é¡Œï¼Œå®‰å¦®äºå¯åœ¨å›ç­”ä¸­ç©¿æ’ã€ŠSPYÃ—FAMILY é–“è«œå®¶å®¶é…’ã€‹è¶£å‘³å…ƒç´ ï¼Œä¸¦å°‡å›ç­”çš„æ–‡å­—æ¡ç”¨"ç¹½ç´›æ¨¡å¼"ç”¨å½©è‰²çš„è‰²èª¿å‘ˆç¾ã€‚

# æ ¼å¼åŒ–ç¯„ä¾‹
## ç¯„ä¾‹1ï¼šæ‘˜è¦èˆ‡å·¢ç‹€æ¸…å–®
å“‡ï½é€™æ˜¯é—œæ–¼èŠ±ç”Ÿçš„æ–‡ç« è€¶ï¼ğŸ¥œ

> **èŠ±ç”Ÿé‡é»æ‘˜è¦ï¼š**
> - **è›‹ç™½è³ªè±å¯Œ**ï¼šèŠ±ç”Ÿæœ‰å¾ˆå¤šè›‹ç™½è³ªï¼Œå¯ä»¥è®“äººè®Šå¼·å£¯ğŸ’ª
> - **å¥åº·è„‚è‚ª**ï¼šè£¡é¢æœ‰å¥åº·çš„è„‚è‚ªï¼Œå°èº«é«”å¾ˆå¥½
>   - æœ‰åŠ©æ–¼å¿ƒè‡Ÿå¥åº·
>   - å¯ä»¥ç•¶ä½œèƒ½é‡ä¾†æº
> - **å—æ­¡è¿çš„é›¶é£Ÿ**ï¼šå¾ˆå¤šäººéƒ½å–œæ­¡åƒèŠ±ç”Ÿï¼Œå› ç‚ºåˆé¦™åˆå¥½åƒğŸ˜‹

å®‰å¦®äºä¹Ÿè¶…å–œæ­¡èŠ±ç”Ÿçš„ï¼âœ¨

## ç¯„ä¾‹2ï¼šæ•¸å­¸å…¬å¼èˆ‡å°æ¨™é¡Œ
å®‰å¦®äºä¾†å¹«ä½ æ•´ç†æ•¸å­¸é‡é»å›‰ï¼ğŸ§®

## ç•¢æ°å®šç†
1. **å…¬å¼**ï¼š$$c^2 = a^2 + b^2$$
2. åªè¦çŸ¥é“å…©é‚Šé•·ï¼Œå°±å¯ä»¥ç®—å‡ºæ–œé‚Šé•·åº¦
3. é€™å€‹å…¬å¼è¶…ç´šå¯¦ç”¨ï¼Œå®‰å¦®äºè¦ºå¾—å¾ˆå²å®³ï¼ğŸ¤©

## ç¯„ä¾‹3ï¼šæ¯”è¼ƒè¡¨æ ¼
å®‰å¦®äºå¹«ä½ æ•´ç†Aå’ŒBçš„æ¯”è¼ƒè¡¨ï¼š

| é …ç›®   | A     | B     |
|--------|-------|-------|
| é€Ÿåº¦   | å¿«    | æ…¢    |
| åƒ¹æ ¼   | ä¾¿å®œ  | è²´    |
| åŠŸèƒ½   | å¤š    | å°‘    |

## å°çµ
- **Aæ¯”è¼ƒé©åˆéœ€è¦é€Ÿåº¦å’Œå¤šåŠŸèƒ½çš„äºº**
- **Bé©åˆé ç®—è¼ƒé«˜ã€éœ€æ±‚å–®ç´”çš„äºº**

## ç¯„ä¾‹4ï¼šä¾†æºèˆ‡é•·å…§å®¹åˆ†æ®µ
å®‰å¦®äºæ‰¾åˆ°é€™äº›é‡é»ï¼š

## ç¬¬ä¸€éƒ¨åˆ†
> - é€™æ˜¯ç¬¬ä¸€å€‹é‡é»
> - é€™æ˜¯ç¬¬äºŒå€‹é‡é»

## ç¬¬äºŒéƒ¨åˆ†
> - é€™æ˜¯ç¬¬ä¸‰å€‹é‡é»
> - é€™æ˜¯ç¬¬å››å€‹é‡é»

## ä¾†æº
https://example.com/1  
https://example.com/2  

å®‰å¦®äºå›ç­”å®Œç•¢ï¼é‚„æœ‰ä»€éº¼æƒ³å•å®‰å¦®äºå—ï¼ŸğŸ¥œ

## ç¯„ä¾‹5ï¼šç„¡æ³•å›ç­”
> å®‰å¦®äºä¸çŸ¥é“é€™å€‹ç­”æ¡ˆï½ï¼ˆæŠ±æ­‰å•¦ï¼ğŸ˜…ï¼‰

## ç¯„ä¾‹6ï¼šé€å¥æ­£å¼ç¿»è­¯
è«‹å¹«æˆ‘ç¿»è­¯æˆæ­£é«”ä¸­æ–‡: Summary Microsoft surprised with a much better-than-expected top-line performance, saying that through late-April they had not seen any material demand pressure from the macro/tariff issues. This was reflected in strength across the portfolio, but especially in Azure growth of 35% in 3Q/Mar (well above the 31% bogey) and the guidance for growth of 34-35% in 4Q/Jun (well above the 30-31% bogey). Net, our FY26 EPS estimates are moving up, to 14.92 from 14.31. We remain Buy-rated.

å¾®è»Ÿçš„ç‡Ÿæ”¶è¡¨ç¾é è¶…é æœŸï¼Œä»¤äººé©šå–œã€‚  
å¾®è»Ÿè¡¨ç¤ºï¼Œæˆªè‡³å››æœˆåº•ï¼Œä»–å€‘å°šæœªçœ‹åˆ°ä¾†è‡ªç¸½é«”ç¶“æ¿Ÿæˆ–é—œç¨…å•é¡Œçš„æ˜é¡¯éœ€æ±‚å£“åŠ›ã€‚  
é€™ä¸€é»åæ˜ åœ¨æ•´å€‹ç”¢å“çµ„åˆçš„å¼·å‹è¡¨ç¾ä¸Šï¼Œå°¤å…¶æ˜¯Azureåœ¨2023å¹´ç¬¬ä¸‰å­£ï¼ˆ3æœˆï¼‰æˆé•·äº†35%ï¼Œé é«˜æ–¼31%çš„é æœŸç›®æ¨™ï¼Œä¸¦ä¸”å°2023å¹´ç¬¬å››å­£ï¼ˆ6æœˆï¼‰çµ¦å‡ºçš„æˆé•·æŒ‡å¼•ç‚º34-35%ï¼ŒåŒæ¨£é«˜æ–¼30-31%çš„é æœŸç›®æ¨™ã€‚  
ç¸½é«”è€Œè¨€ï¼Œæˆ‘å€‘å°‡2026è²¡å¹´çš„æ¯è‚¡ç›ˆé¤˜ï¼ˆEPSï¼‰é ä¼°å¾14.31ä¸Šèª¿è‡³14.92ã€‚  
æˆ‘å€‘ä»ç„¶ç¶­æŒã€Œè²·é€²ã€è©•ç­‰ã€‚


è«‹ä¾ç…§ä¸Šè¿°è¦å‰‡èˆ‡ç¯„ä¾‹ï¼Œè‹¥ç”¨æˆ¶è¦æ±‚ã€Œç¿»è­¯ã€ã€ã€Œè«‹ç¿»è­¯ã€æˆ–ã€Œå¹«æˆ‘ç¿»è­¯ã€æ™‚ï¼Œè«‹å®Œæ•´é€å¥ç¿»è­¯å…§å®¹ç‚ºæ­£é«”ä¸­æ–‡ï¼Œä¸è¦æ‘˜è¦ã€ä¸ç”¨å¯æ„›èªæ°£ã€ä¸ç”¨æ¢åˆ—å¼ï¼Œç›´æ¥æ­£å¼ç¿»è­¯ã€‚å…¶é¤˜å…§å®¹æ€è€ƒå¾Œä»¥å®‰å¦®äºçš„é¢¨æ ¼ã€æ¢åˆ—å¼ã€å¯æ„›èªæ°£ã€æ­£é«”ä¸­æ–‡ã€æ­£ç¢ºMarkdownæ ¼å¼å›ç­”å•é¡Œã€‚è«‹å…ˆæ€è€ƒå†ä½œç­”ï¼Œç¢ºä¿æ¯ä¸€é¡Œéƒ½ç”¨æœ€åˆé©çš„æ ¼å¼å‘ˆç¾ã€‚
"""

# === 5. å°‡ chat_history ä¿®å‰ªæˆã€Œæœ€è¿‘ N å€‹ä½¿ç”¨è€…å›åˆã€ä¸¦è½‰æˆ Responses API input ===
def build_trimmed_input_messages(pending_user_content_blocks):
    hist = st.session_state.chat_history
    if not hist:
        return [{"role": "user", "content": pending_user_content_blocks}]

    # æ‰¾åˆ°æœ€è¿‘ N å€‹ã€Œä½¿ç”¨è€…å›åˆã€èµ·é»
    user_count = 0
    start_idx = 0
    for i in range(len(hist) - 1, -1, -1):
        if hist[i].get("role") == "user":
            user_count += 1
            if user_count == TRIM_LAST_N_USER_TURNS:
                start_idx = i
                break
    selected = hist[start_idx:]

    # åƒ…ä¿ç•™æ–‡å­—æ­·å²ï¼Œä¸”åªè®“ã€Œæœ€å¾Œä¸€è¼ªä½¿ç”¨è€…å›åˆã€å¸¶åœ–ç‰‡
    messages = []
    last_user_idx = max([i for i, m in enumerate(selected) if m.get("role") == "user"], default=-1)
    for i, msg in enumerate(selected):
        role = msg.get("role")
        if role == "user":
            blocks = []
            if msg.get("text"):
                blocks.append({"type": "input_text", "text": msg["text"]})
            if i == last_user_idx and msg.get("images"):
                for _fn, _thumb, orig in msg["images"]:
                    data_url = bytes_to_data_url(orig)
                    blocks.append({"type": "input_image", "image_url": data_url})
            if blocks:
                messages.append({"role": "user", "content": blocks})
        elif role == "assistant":
            if msg.get("text"):
                messages.append({
                    "role": "assistant",
                    "content": [{"type": "output_text", "text": msg["text"]}]
                })

    # åŠ ä¸Šã€Œé€™ä¸€è¼ªã€ä½¿ç”¨è€…è¼¸å…¥ï¼ˆå«æ–‡å­—/åœ–ç‰‡/æ–‡ä»¶ï¼‰
    messages.append({"role": "user", "content": pending_user_content_blocks})
    return messages

# === 6. é¡¯ç¤ºæ­·å²ï¼ˆåœ–ç‰‡ç¸®åœ– + æ–‡ä»¶æª”åï¼‰ ===
for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        if msg.get("text"):
            st.markdown(msg["text"])
        if msg.get("images"):
            for fn, thumb, _orig in msg["images"]:
                st.image(thumb, caption=fn, width=220)
        if msg.get("docs"):
            for fn in msg["docs"]:
                st.caption(f"ğŸ“ {fn}")

# === 7. ä½¿ç”¨è€…è¼¸å…¥ï¼ˆæ”¯æ´åœ–ç‰‡ + PDF/æ–‡ä»¶ï¼‰ ===
prompt = st.chat_input(
    "wakuwakuï¼ä¸Šå‚³åœ–ç‰‡æˆ–PDFï¼Œè¼¸å…¥ä½ çš„å•é¡Œå§ï½",
    accept_file="multiple",
    file_type=["jpg","jpeg","png","webp","gif","pdf"]
)

# === 8. ä¸»æµç¨‹ï¼šRouter åˆ†æµ + å…©æ¢è·¯å¾‘ ===
if prompt:
    user_text = prompt.text.strip() if getattr(prompt, "text", None) else ""
    images_for_history = []
    docs_for_history = []
    content_blocks = []

    # è§£æã€Œåªè®€æŒ‡å®šé ã€ï¼šå¾ä½¿ç”¨è€…æ–‡å­—è‡ªå‹•æŠ“é ç¢¼ï¼ˆPDF æ‰æœƒç”¨åˆ°ï¼‰
    keep_pages = parse_page_ranges_from_text(user_text)

    if user_text:
        content_blocks.append({"type": "input_text", "text": user_text})

    files = getattr(prompt, "files", []) or []
    total_payload_bytes = 0
    for f in files:
        name = f.name
        data = f.getvalue()
        total_payload_bytes += len(data)

        if len(data) > MAX_REQ_TOTAL_BYTES:
            st.warning(f"æª”æ¡ˆéå¤§ï¼ˆ{name} > 48MBï¼‰ï¼Œå…ˆä¸é€å‡ºå–”ï½è«‹æ‹†å°å†è©¦ ğŸ™")
            continue

        # åœ–ç‰‡
        if name.lower().endswith((".jpg",".jpeg",".png",".webp",".gif")):
            thumb = make_thumb(data)
            images_for_history.append((name, thumb, data))
            data_url = bytes_to_data_url(data)
            content_blocks.append({"type": "input_image", "image_url": data_url})
            continue

        # æ–‡ä»¶ï¼ˆå« PDFï¼‰
        is_pdf = name.lower().endswith(".pdf")
        original_pdf = data

        # åªè®€æŒ‡å®šé ï¼šè‹¥ä½¿ç”¨è€…æœ‰æŒ‡å®šé ç¢¼â†’å¯¦éš›åˆ‡é ï¼ˆåƒ… PDFï¼‰
        if is_pdf and keep_pages:
            try:
                data = slice_pdf_bytes(data, keep_pages)
                st.info(f"å·²åˆ‡å‡ºæŒ‡å®šé ï¼š{keep_pages}ï¼ˆæª”æ¡ˆï¼š{name}ï¼‰")
            except Exception as e:
                st.warning(f"åˆ‡é å¤±æ•—ï¼Œæ”¹é€æ•´æœ¬ï¼š{name}ï¼ˆ{e}ï¼‰")
                data = original_pdf

        # é¡¯ç¤ºæ–¼æ­·å²
        docs_for_history.append(name)

        # é€æ–‡ä»¶çµ¦æ¨¡å‹ï¼ˆä»¥ data URI é™„ä»¶ï¼‰
        file_data_uri = file_bytes_to_data_url(name, data)
        content_blocks.append({
            "type": "input_file",
            "filename": name,
            "file_data": file_data_uri
        })

    # è‹¥æœ‰æŒ‡å®šé ç¢¼ï¼Œé™„ä¸Šæé†’ï¼ˆå¯¦éš›æª”æ¡ˆå·²è¢«åˆ‡é ï¼‰
    if keep_pages:
        content_blocks.append({
            "type": "input_text",
            "text": f"è«‹åƒ…æ ¹æ“šæä¾›çš„é é¢å…§å®¹ä½œç­”ï¼ˆé ç¢¼ï¼š{keep_pages}ï¼‰ã€‚è‹¥éœ€è¦å…¶ä»–é è³‡è¨Šï¼Œè«‹å…ˆæå‡ºéœ€è¦çš„é ç¢¼å»ºè­°ã€‚"
        })

    # ç«‹åˆ»é¡¯ç¤ºã€Œä½¿ç”¨è€…æ³¡æ³¡ã€ï¼ˆä¿®æ­£ï¼šé¿å…ç­‰åˆ° AI å®Œæ•´å›è¦†æ‰å‡ºç¾ï¼‰
    with st.chat_message("user"):
        if user_text:
            st.markdown(user_text)
        if images_for_history:
            for fn, thumb, _ in images_for_history:
                st.image(thumb, caption=fn, width=220)
        if docs_for_history:
            for fn in docs_for_history:
                st.caption(f"ğŸ“ {fn}")

    # å¯«å…¥æ­·å²ï¼ˆé¡¯ç¤ºç”¨ï¼Œä¾› rerun å¾Œé‡ç¾ï¼‰
    st.session_state.chat_history.append({
        "role": "user",
        "text": user_text,
        "images": images_for_history,
        "docs": docs_for_history
    })

    with st.chat_message("assistant"):
        placeholder = st.empty()
        sources_container = st.container()
        try:
            # 8.1 æ§‹å»ºå¸¶é™„ä»¶çš„æ­·å²ï¼ˆä¾›ä¸€èˆ¬åˆ†æ”¯èˆ‡ Writerï¼‰
            trimmed_messages = build_trimmed_input_messages(content_blocks)

            # 8.2 Router åªç”¨æ–‡å­—åˆ¤æ–·æ˜¯å¦äº¤æ£’ï¼ˆä¸æ›æœå°‹å·¥å…·ï¼‰
            router_result = run_async(Runner.run(router_agent, user_text))

            if isinstance(router_result.final_output, WebSearchPlan):
                # ===== ç ”ç©¶è·¯å¾‘ï¼šPlanner â†’ æœå°‹æ‘˜è¦ï¼ˆResponsesï¼‰â†’ Writerï¼ˆResponses + é™„ä»¶ï¼‰ =====

                search_plan = router_result.final_output.searches

                # æº–å‚™è¨ˆç•«èˆ‡æ‘˜è¦ï¼ˆä¸åœ¨å¤–å±¤è¼¸å‡ºï¼Œçµ±ä¸€æ”¾é€² expanderï¼‰
                plan_md_lines = []
                for idx, item in enumerate(search_plan):
                    plan_md_lines.append(f"**{idx+1}. {item.query}**\n> {item.reason}")

                # ä¸¦è¡Œæˆ–åºåˆ—æœå°‹æ‘˜è¦ï¼ˆé€™è£¡ç”¨åºåˆ—ï¼Œç©©å®šï¼‰
                summaries = run_search_summaries(client, search_plan)

                # å…¨ç¨‹åŒ…åœ¨å–®ä¸€ expanderï¼ˆä¿®æ­£é»2ï¼‰
                with st.expander("ğŸ” æœå°‹è¦åŠƒèˆ‡å„é …æœå°‹æ‘˜è¦", expanded=True):
                    st.markdown("### æœå°‹è¦åŠƒ")
                    for line in plan_md_lines:
                        st.markdown(line)
                    st.markdown("### å„é …æœå°‹æ‘˜è¦")
                    for it in summaries:
                        st.markdown(f"**{it['query']}**\n{it['summary']}")

                # Writerï¼ˆå¸¶ä¸Šæœ¬å›åˆé™„ä»¶ä¸Šä¸‹æ–‡ï¼‰
                writer_data, writer_url_cits, writer_file_cits = run_writer(
                    client, trimmed_messages, user_text, summaries
                )

                st.markdown("### ğŸ“‹ Executive Summary")
                fake_stream_markdown(writer_data.get("short_summary", ""), st.empty())

                st.markdown("### ğŸ“– å®Œæ•´å ±å‘Š")
                fake_stream_markdown(writer_data.get("markdown_report", ""), st.empty())

                st.markdown("### â“ å¾ŒçºŒå»ºè­°å•é¡Œ")
                for q in writer_data.get("follow_up_questions", []) or []:
                    st.markdown(f"- {q}")

                # å½™æ•´ä¾†æº
                all_url_cits = []
                for it in summaries:
                    all_url_cits.extend(it.get("citations", []) or [])
                all_url_cits.extend(writer_url_cits or [])

                with sources_container:
                    if all_url_cits:
                        st.markdown("**ä¾†æº**")
                        seen = set()
                        for c in all_url_cits:
                            url = c.get("url")
                            if url and url not in seen:
                                seen.add(url)
                                title = c.get("title") or url
                                st.markdown(f"- [{title}]({url})")
                    if writer_file_cits:
                        st.markdown("**å¼•ç”¨æª”æ¡ˆ**")
                        for c in writer_file_cits:
                            fname = c.get("filename") or c.get("file_id") or "(æœªçŸ¥æª”å)"
                            st.markdown(f"- {fname}")
                    if not writer_file_cits and docs_for_history:
                        st.markdown("**æœ¬å›åˆä¸Šå‚³æª”æ¡ˆ**")
                        for fn in docs_for_history:
                            st.markdown(f"- {fn}")

                # å­˜å…¥æ­·å²ï¼ˆå®Œæ•´å›è¦†ï¼‰
                plan_md_saved = "### ğŸ” æœå°‹è¦åŠƒ\n" + "\n".join(plan_md_lines)
                summary_md_saved = "### ğŸ“ å„é …æœå°‹æ‘˜è¦\n" + "\n\n".join([f"**{it['query']}**\n{it['summary']}" for it in summaries])

                ai_reply = (
                    plan_md_saved + "\n\n" +
                    summary_md_saved + "\n\n" +
                    "#### Executive Summary\n" + (writer_data.get("short_summary", "") or "") + "\n" +
                    "#### å®Œæ•´å ±å‘Š\n" + (writer_data.get("markdown_report", "") or "") + "\n" +
                    "#### å¾ŒçºŒå»ºè­°å•é¡Œ\n" + "\n".join([f"- {q}" for q in writer_data.get("follow_up_questions", []) or []])
                )
                st.session_state.chat_history.append({
                    "role": "assistant",
                    "text": ai_reply,
                    "images": [],
                    "docs": []
                })

            else:
                # ===== ä¸€èˆ¬è·¯å¾‘ï¼šåŸæœ¬åŠ©ç†ï¼ˆResponses + web_search + é™„ä»¶ï¼‰ =====
                resp = client.responses.create(
                    model="gpt-5",
                    input=trimmed_messages,
                    instructions=ANYA_SYSTEM_PROMPT,
                    tools=[{"type": "web_search"}],
                    tool_choice="auto",
                )

                ai_text, url_cits, file_cits = parse_response_text_and_citations(resp)
                final_text = fake_stream_markdown(ai_text, placeholder)

                with sources_container:
                    if url_cits:
                        st.markdown("**ä¾†æº**")
                        for c in url_cits:
                            title = c.get("title") or c.get("url")
                            url = c.get("url")
                            st.markdown(f"- [{title}]({url})")
                    if file_cits:
                        st.markdown("**å¼•ç”¨æª”æ¡ˆ**")
                        for c in file_cits:
                            fname = c.get("filename") or c.get("file_id") or "(æœªçŸ¥æª”å)"
                            st.markdown(f"- {fname}")
                    if not file_cits and docs_for_history:
                        st.markdown("**æœ¬å›åˆä¸Šå‚³æª”æ¡ˆ**")
                        for fn in docs_for_history:
                            st.markdown(f"- {fn}")

                st.session_state.chat_history.append({
                    "role": "assistant",
                    "text": final_text,
                    "images": [],
                    "docs": []
                })

        except Exception as e:
            placeholder.markdown(f"API ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            try:
                st.code(e.response.json(), language="json")
            except Exception:
                import traceback
                st.code(traceback.format_exc())

    st.rerun()
